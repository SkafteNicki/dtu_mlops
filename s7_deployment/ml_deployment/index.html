
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../testing_apis/">
      
      
        <link rel="next" href="../frontend/">
      
      
      <link rel="icon" href="../../figures/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>M25 - ML deployment - DTU-MLOps</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_markdown_exec_pyodide.css">
    
      <link rel="stylesheet" href="../../assets/_markdown_exec_ansi.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deployment-of-machine-learning-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="DTU-MLOps" class="md-header__button md-logo" aria-label="DTU-MLOps" data-md-component="logo">
      
  <img src="../../figures/mlops_cycle.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DTU-MLOps
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              M25 - ML deployment
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/SkafteNicki/dtu_mlops" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="DTU-MLOps" class="md-nav__button md-logo" aria-label="DTU-MLOps" data-md-component="logo">
      
  <img src="../../figures/mlops_cycle.png" alt="logo">

    </a>
    DTU-MLOps
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/SkafteNicki/dtu_mlops" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pages/timeplan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Time plan
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s1_development_environment/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S1 - Development Environment 💻
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            S1 - Development Environment 💻
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s1_development_environment/command_line/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M1 - The command line
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s1_development_environment/package_manager/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M2 - Package Manager
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s1_development_environment/editor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M3 - Editor
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s1_development_environment/deep_learning_software/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M4 - Deep Learning Software
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s2_organisation_and_version_control/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S2 - Organisation and Version Control 📁
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            S2 - Organisation and Version Control 📁
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s2_organisation_and_version_control/git/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M5 - Git
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s2_organisation_and_version_control/code_structure/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M6 - Code structure
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s2_organisation_and_version_control/good_coding_practice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M7 - Good coding practice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s2_organisation_and_version_control/dvc/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M8 - Data version control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s2_organisation_and_version_control/cli/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M9 - Command Line Interfaces
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s3_reproducibility/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S3 - Reproducibility ♻️
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            S3 - Reproducibility ♻️
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s3_reproducibility/docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M10 - Docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s3_reproducibility/config_files/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M11 - Config Files
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s4_debugging_and_logging/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S4 - Debugging, Profiling and Logging ⏱️
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_6" id="__nav_6_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            S4 - Debugging, Profiling and Logging ⏱️
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s4_debugging_and_logging/debugging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M12 - Debugging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s4_debugging_and_logging/profiling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M13 - Profiling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s4_debugging_and_logging/logging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M14 - Logging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s4_debugging_and_logging/boilerplate/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M15 - Boilerplate
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s5_continuous_integration/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S5 - Continuous Integration ✔️
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7" id="__nav_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            S5 - Continuous Integration ✔️
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s5_continuous_integration/unittesting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M16 - Unit testing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s5_continuous_integration/github_actions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M17 - GitHub Actions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s5_continuous_integration/pre_commit/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M18 - Pre-commit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s5_continuous_integration/cml/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M19 - Continuous Machine Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s6_the_cloud/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S6 - The cloud 🌐
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_8" id="__nav_8_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            S6 - The cloud 🌐
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s6_the_cloud/cloud_setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M20 - Cloud Setup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s6_the_cloud/using_the_cloud/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M21 - Using the Cloud
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S7 - Deployment 📦
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_9" id="__nav_9_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            S7 - Deployment 📦
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../apis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M22 - Requests and APIs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cloud_deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M23 - Cloud Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../testing_apis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M24 - API Testing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    M25 - ML deployment
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    M25 - ML deployment
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-packaging" class="md-nav__link">
    <span class="md-ellipsis">
      Model Packaging
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exercises" class="md-nav__link">
    <span class="md-ellipsis">
      ❔ Exercises
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bentoml" class="md-nav__link">
    <span class="md-ellipsis">
      BentoML
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BentoML">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercises_1" class="md-nav__link">
    <span class="md-ellipsis">
      ❔ Exercises
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#knowledge-check" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Knowledge check
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../frontend/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M26 - Frontend
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s8_monitoring/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S8 - Monitoring 📊
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_10" id="__nav_10_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            S8 - Monitoring 📊
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s8_monitoring/data_drifting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M27 - Data Drifting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s8_monitoring/monitoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M28 - System Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s9_scalable_applications/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S9 - Scalable applications ⚖️
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11" id="__nav_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            S9 - Scalable applications ⚖️
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s9_scalable_applications/data_loading/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M29 - Distributed Data Loading
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s9_scalable_applications/distributed_training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M30 - Distributed Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s9_scalable_applications/inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M31 - Scalable Inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_12" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../s10_extra/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    S10 - Extra 🔥
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_12" id="__nav_12_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12">
            <span class="md-nav__icon md-icon"></span>
            S10 - Extra 🔥
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s10_extra/documentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M32 - Documentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s10_extra/hyperparameters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M33 - Hyperparameter optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../s10_extra/high_performance_clusters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    M34 - High Performance Clusters
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pages/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Summary
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pages/projects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Projects
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pages/faq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAQ
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-packaging" class="md-nav__link">
    <span class="md-ellipsis">
      Model Packaging
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exercises" class="md-nav__link">
    <span class="md-ellipsis">
      ❔ Exercises
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bentoml" class="md-nav__link">
    <span class="md-ellipsis">
      BentoML
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BentoML">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exercises_1" class="md-nav__link">
    <span class="md-ellipsis">
      ❔ Exercises
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#knowledge-check" class="md-nav__link">
    <span class="md-ellipsis">
      🧠 Knowledge check
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/SkafteNicki/dtu_mlops/edit/main/s7_deployment/ml_deployment.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/SkafteNicki/dtu_mlops/raw/main/s7_deployment/ml_deployment.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5"/></svg>
    </a>
  


<p><a class="glightbox" href="../../figures/icons/onnx.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img align="right" alt="Logo" src="../../figures/icons/onnx.png" width="130" /></a>
<a class="glightbox" href="../../figures/icons/bentoml.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img align="right" alt="Logo" src="../../figures/icons/bentoml.png" width="130" /></a></p>
<h1 id="deployment-of-machine-learning-models">Deployment of Machine Learning Models</h1>
<hr />
<p>In one of the <a href="../apis/">previous modules</a> you learned about how to use
<a href="https://fastapi.tiangolo.com/">FastAPI</a> to create an API to interact with your machine learning models. FastAPI is a
great framework, but it is a general framework meaning that it was not developed with machine learning applications in
mind. This means that there are features which you may consider to be missing when considering running large scale
machine learning models:</p>
<ul>
<li>
<p>Dynamic-batching: if you have a large number of requests coming in, you may want to process them in batches to
    reduce the overhead of loading the model and running the inference. This is especially true if you are running your
    model on a GPU, where the overhead of loading the model is significant.</p>
</li>
<li>
<p>Async inference: FastAPI does support async requests but not a way to call the model asynchronously. This means that
    if you have a large number of requests coming in, you will have to wait for the model to finish processing (because
    the model is not async) before you can start processing the next request.</p>
</li>
<li>
<p>Native GPU support: you can definitely run part of your application in FastAPI if you want to. But again it was not
    built with machine learning in mind, so you will have to do some extra work to get it to work.</p>
</li>
</ul>
<p>It should come as no surprise that multiple frameworks have therefore sprung up that better support deployment of
machine learning algorithms (just listing a few here):</p>
<table>
<thead>
<tr>
<th>🌟 Framework</th>
<th>🧩 Backend Agnostic</th>
<th>🧠 Model Agnostic</th>
<th>📂 Repository</th>
<th>⭐ GitHub Stars</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cortex</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/cortexlabs/cortex">🔗 Link</a></td>
<td>8.0k</td>
</tr>
<tr>
<td>BentoML</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/bentoml/bentoml">🔗 Link</a></td>
<td>8.1k</td>
</tr>
<tr>
<td>Ray Serve</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/ray-project/ray">🔗 Link</a></td>
<td>39.1k</td>
</tr>
<tr>
<td>Triton Inference Server</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/NVIDIA/triton-inference-server">🔗 Link</a></td>
<td>9.8k</td>
</tr>
<tr>
<td>OpenVINO</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/openvinotoolkit/openvino">🔗 Link</a></td>
<td>8.9k</td>
</tr>
<tr>
<td>Seldon-core</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/seldonio/seldon-core">🔗 Link</a></td>
<td>4.6k</td>
</tr>
<tr>
<td>Litserve</td>
<td>✅</td>
<td>✅</td>
<td><a href="https://github.com/Lightning-AI/LitServe">🔗 Link</a></td>
<td>3.6k</td>
</tr>
<tr>
<td>Torchserve</td>
<td>❌</td>
<td>✅</td>
<td><a href="https://github.com/pytorch/serve">🔗 Link</a></td>
<td>4.3k</td>
</tr>
<tr>
<td>TensorFlow serve</td>
<td>❌</td>
<td>✅</td>
<td><a href="https://github.com/tensorflow/serving">🔗 Link</a></td>
<td>6.3k</td>
</tr>
<tr>
<td>vLLM</td>
<td>❌</td>
<td>❌</td>
<td><a href="https://github.com/vllm-project/vllm">🔗 Link</a></td>
<td>59.1k</td>
</tr>
</tbody>
</table>
<p>The first 7 frameworks are backend agnostic, meaning that they are intended to work with whatever computational backend
your model is implemented in (TensorFlow, PyTorch, Jax, Sklearn, etc.), whereas the last 3 are backend specific (PyTorch,
TensorFlow and a custom framework). The first 9 frameworks are model agnostic, meaning that they are intended to work
with whatever model you have implemented, whereas the last one is model specific in this case to LLM's. When choosing a
framework to deploy your model, you should consider the following:</p>
<ul>
<li>
<p><strong>Ease of use</strong>. Some frameworks are easier to use and get started with than others, but may have fewer features. As
    an example from the list above, <code>Litserve</code> is very easy to get started with but is a relatively new framework and
    may not have all the features you need.</p>
</li>
<li>
<p><strong>Performance</strong>. Some frameworks are optimized for performance, but may be harder to use. As an example from the list
    above, <code>vLLM</code> is a very high performance framework for serving large language models but it cannot be used for other
    types of models.</p>
</li>
<li>
<p><strong>Community</strong>. Some frameworks have a large community, which can be helpful if you run into problems. As an example
    from the list above, <code>Triton Inference Server</code> was developed by Nvidia and has a large community of users. As a good
    rule of thumb, the more stars a repository has on GitHub, the larger the community.</p>
</li>
</ul>
<p>In this module we are going to be looking at the <code>BentoML</code> framework because it strikes a good balance between ease of
use and having a lot of features that can improve the performance of serving your models. However, before we dive into
this serving framework, we are going to look at a general way to package our machine learning models that should work
with most of the above frameworks.</p>
<h2 id="model-packaging">Model Packaging</h2>
<p>Whenever we want to serve a machine learning model, we in general need three things:</p>
<ul>
<li>The computational graph of the model, e.g. how to pass data through the model to get a prediction</li>
<li>The weights of the model, e.g. the parameters that the model has learned during training</li>
<li>A computational backend that can run the model</li>
</ul>
<p>In the past module on <a href="../../s3_reproducibility/docker/">Docker</a> we learned how to package all of these things into
a container. This is a great way to package a model, but it is not the only way. The core assumption we have currently
made is that the computational backend is the same as the one we trained the model on. However, this does not need to
be the case. As long as we can export our model and weights to a common format, we can run the model on any backend
that supports this format.</p>
<p>This is exactly what the <a href="https://onnx.ai/">Open Neural Network Exchange (ONNX)</a> is designed to do. ONNX is a
standardized format for creating and sharing machine learning models. It defines an extensible computation graph model,
as well as definitions of built-in operators and standard data types. The idea behind ONNX is that a model trained with
a specific framework on a specific device, let's say PyTorch on your local computer, can be exported and run with an
entirely different framework and hardware easily. Learning how to export your models to ONNX is therefore a great way
to increase the longevity of your models and not be locked into a specific framework for serving your models.</p>
<figure>
<p><a class="glightbox" href="../../figures/onnx.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Image" src="../../figures/onnx.png" width="1000" /></a></p>
<figcaption>
The ONNX format is designed to bridge the gap between development and deployment of machine learning models by making
it easy to export models between different frameworks and hardware. For example, PyTorch is in general considered to be
a developer-friendly framework, though it has historically been slow to run inference with.
<a href="https://medium.com/trueface-ai/two-benefits-of-the-onnx-library-for-ml-models-4b3e417df52e"> Image credit </a>
</figcaption>
</figure>
<h2 id="exercises">❔ Exercises</h2>
<ol>
<li>
<p>Start by installing ONNX, ONNX runtime and ONNX script. This can be done by running the following command:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>pip<span class="w"> </span>install<span class="w"> </span>onnx<span class="w"> </span>onnxruntime<span class="w"> </span>onnxscript
</span></code></pre></div>
<p>The first package contains the core ONNX framework, the second package contains the runtime for running ONNX models
and the third package contains a new experimental package that is designed to make it easier to export models to
ONNX.</p>
</li>
<li>
<p>Let's start out by converting a model to ONNX. The following code snippets show how to export a PyTorch model to
    ONNX.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:3"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><input id="__tabbed_1_3" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">PyTorch =&gt; 2.0</label><label for="__tabbed_1_2">PyTorch &lt; 2.0 or Windows</label><label for="__tabbed_1_3">PyTorch-lightning</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="n">onnx_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">dynamo_export</span><span class="p">(</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    <span class="n">model_args</span><span class="o">=</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">,),</span>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>    <span class="n">export_options</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">ExportOptions</span><span class="p">(</span><span class="n">dynamic_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span class="p">)</span>
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a><span class="n">onnx_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;resnet18.onnx&quot;</span><span class="p">)</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
</span><span id="__span-2-9"><a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
</span><span id="__span-2-10"><a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">,),</span>
</span><span id="__span-2-11"><a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>    <span class="n">f</span><span class="o">=</span><span class="s2">&quot;resnet18.onnx&quot;</span><span class="p">,</span>
</span><span id="__span-2-12"><a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>
</span><span id="__span-2-13"><a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">],</span>
</span><span id="__span-2-14"><a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">},</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}}</span>
</span><span id="__span-2-15"><a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a><span class="p">)</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">pytorch_lightning</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnx</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="k">class</span><span class="w"> </span><span class="nc">LitModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-3-9"><a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span id="__span-3-10"><a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-3-11"><a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="__span-3-12"><a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>
</span><span id="__span-3-13"><a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span id="__span-3-14"><a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span id="__span-3-15"><a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>
</span><span id="__span-3-16"><a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a><span class="n">model</span> <span class="o">=</span> <span class="n">LitModel</span><span class="p">()</span>
</span><span id="__span-3-17"><a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="__span-3-18"><a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a>
</span><span id="__span-3-19"><a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span><span id="__span-3-20"><a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a><span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span>
</span><span id="__span-3-21"><a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a>    <span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;resnet18.onnx&quot;</span><span class="p">,</span>
</span><span id="__span-3-22"><a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a>    <span class="n">input_sample</span><span class="o">=</span><span class="n">dummy_input</span><span class="p">,</span>
</span><span id="__span-3-23"><a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a>    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">],</span>
</span><span id="__span-3-24"><a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a>    <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">],</span>
</span><span id="__span-3-25"><a id="__codelineno-3-25" name="__codelineno-3-25" href="#__codelineno-3-25"></a>    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">},</span> <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">}}</span>
</span><span id="__span-3-26"><a id="__codelineno-3-26" name="__codelineno-3-26" href="#__codelineno-3-26"></a><span class="p">)</span>
</span></code></pre></div>
</div>
</div>
</div>
<p>Export a model of your own choice to ONNX or just try to export the <code>resnet18</code> model as shown in the examples above,
and confirm that the model was exported by checking that the file exists. Can you figure out what is meant by
<code>dynamic_axes</code>?</p>
<details class="success">
<summary>Solution</summary>
<p>The <code>dynamic_axes</code> argument is used to specify which axes of the input tensor that should be considered dynamic.
This is useful when the model can accept inputs of different sizes, e.g. when the model is used in a dynamic
batching scenario. In the example above we have specified that the first axis of the input tensor should be
considered dynamic, meaning that the model can accept inputs of different batch sizes. While it may be tempting
to specify all axes as dynamic, this can lead to slower inference times because the ONNX runtime will
not be able to optimize the computational graph as well.</p>
</details>
</li>
<li>
<p>Check that the model was correctly exported by loading it using the <code>onnx</code> package and afterwards check the graph
    of the model using the following code:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnx</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;resnet18.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">onnx</span><span class="o">.</span><span class="n">checker</span><span class="o">.</span><span class="n">check_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span><span id="__span-4-4"><a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="nb">print</span><span class="p">(</span><span class="n">onnx</span><span class="o">.</span><span class="n">helper</span><span class="o">.</span><span class="n">printable_graph</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="p">))</span>
</span></code></pre></div>
</li>
<li>
<p>To get a better understanding of what is actually exported, let's try to visualize the computational graph of the
    model. This can be done using the open-source tool <a href="https://github.com/lutzroeder/netron">netron</a>. You can either
    try it out directly in <a href="https://netron.app/">webbrowser</a> or you can install it locally using <code>pip install netron</code>
    and then run it using <code>netron resnet18.onnx</code>. Can you figure out what method of the model is exported to ONNX?</p>
<details class="success">
<summary>Solution</summary>
<p>When a PyTorch model is exported to ONNX, it is only the <code>forward</code> method of the model that is exported. This
means that that is the only method we have access to when we load the model later. Therefore, make sure that the
<code>forward</code> method of your model is implemented in a way that it can be used for inference.</p>
</details>
</li>
<li>
<p>After converting a model to ONNX format we can use <a href="https://onnxruntime.ai/docs/">ONNX Runtime</a> to run it.
    The benefit of this is that ONNX Runtime is able to optimize the computational graph of the model, which can lead
    to faster inference times. Let's try to look into that.</p>
<ol>
<li>
<p>Figure out how to run a model using the ONNX Runtime. Relevant
    <a href="https://onnxruntime.ai/docs/get-started/with-python.html">documentation</a>.</p>
<details class="success">
<summary>Solution</summary>
<p>To use the ONNX runtime to run a model, we first need to start an inference session, then extract the input and
output names of our model and finally run the model. The following code snippet shows how to do this.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">rt</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="n">ort_session</span> <span class="o">=</span> <span class="n">rt</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;&lt;path-to-model&gt;&quot;</span><span class="p">)</span>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()]</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a><span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()]</span>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)}</span>
</span><span id="__span-5-6"><a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a><span class="n">out</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">output_names</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
</span></code></pre></div>
</details>
</li>
<li>
<p>Let's experiment with the performance of ONNX vs. PyTorch. Implement a benchmark that measures the time it takes to
    run a model using PyTorch and ONNX. Bonus points if you test for multiple input sizes. To get you started we
    have implemented a timing decorator that you can use to measure the time it takes to run a function.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">statistics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
</span><span id="__span-6-3"><a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="k">def</span><span class="w"> </span><span class="nf">timing_decorator</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">function_repeat</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">timing_repeat</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
</span><span id="__span-6-4"><a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot; Decorator that times the execution of a function. &quot;&quot;&quot;</span>
</span><span id="__span-6-5"><a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="__span-6-6"><a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>        <span class="n">timing_results</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-6-7"><a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">timing_repeat</span><span class="p">):</span>
</span><span id="__span-6-8"><a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="__span-6-9"><a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">function_repeat</span><span class="p">):</span>
</span><span id="__span-6-10"><a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>                <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="__span-6-11"><a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="__span-6-12"><a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>            <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
</span><span id="__span-6-13"><a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a>            <span class="n">timing_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elapsed_time</span><span class="p">)</span>
</span><span id="__span-6-14"><a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Avg +- Stddev: </span><span class="si">{</span><span class="n">mean</span><span class="p">(</span><span class="n">timing_results</span><span class="p">)</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2"> +- </span><span class="si">{</span><span class="n">stdev</span><span class="p">(</span><span class="n">timing_results</span><span class="p">)</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</span><span id="__span-6-15"><a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a>        <span class="k">return</span> <span class="n">result</span>
</span><span id="__span-6-16"><a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a>    <span class="k">return</span> <span class="n">wrapper</span>
</span></code></pre></div>
<details class="success">
<summary>Solution</summary>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">onnx_benchmark.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-7-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-7-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-7-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-7-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-7-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-7-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-7-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-7-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-7-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-7-10">10</a></span>
<span class="normal"><a href="#__codelineno-7-11">11</a></span>
<span class="normal"><a href="#__codelineno-7-12">12</a></span>
<span class="normal"><a href="#__codelineno-7-13">13</a></span>
<span class="normal"><a href="#__codelineno-7-14">14</a></span>
<span class="normal"><a href="#__codelineno-7-15">15</a></span>
<span class="normal"><a href="#__codelineno-7-16">16</a></span>
<span class="normal"><a href="#__codelineno-7-17">17</a></span>
<span class="normal"><a href="#__codelineno-7-18">18</a></span>
<span class="normal"><a href="#__codelineno-7-19">19</a></span>
<span class="normal"><a href="#__codelineno-7-20">20</a></span>
<span class="normal"><a href="#__codelineno-7-21">21</a></span>
<span class="normal"><a href="#__codelineno-7-22">22</a></span>
<span class="normal"><a href="#__codelineno-7-23">23</a></span>
<span class="normal"><a href="#__codelineno-7-24">24</a></span>
<span class="normal"><a href="#__codelineno-7-25">25</a></span>
<span class="normal"><a href="#__codelineno-7-26">26</a></span>
<span class="normal"><a href="#__codelineno-7-27">27</a></span>
<span class="normal"><a href="#__codelineno-7-28">28</a></span>
<span class="normal"><a href="#__codelineno-7-29">29</a></span>
<span class="normal"><a href="#__codelineno-7-30">30</a></span>
<span class="normal"><a href="#__codelineno-7-31">31</a></span>
<span class="normal"><a href="#__codelineno-7-32">32</a></span>
<span class="normal"><a href="#__codelineno-7-33">33</a></span>
<span class="normal"><a href="#__codelineno-7-34">34</a></span>
<span class="normal"><a href="#__codelineno-7-35">35</a></span>
<span class="normal"><a href="#__codelineno-7-36">36</a></span>
<span class="normal"><a href="#__codelineno-7-37">37</a></span>
<span class="normal"><a href="#__codelineno-7-38">38</a></span>
<span class="normal"><a href="#__codelineno-7-39">39</a></span>
<span class="normal"><a href="#__codelineno-7-40">40</a></span>
<span class="normal"><a href="#__codelineno-7-41">41</a></span>
<span class="normal"><a href="#__codelineno-7-42">42</a></span>
<span class="normal"><a href="#__codelineno-7-43">43</a></span>
<span class="normal"><a href="#__codelineno-7-44">44</a></span>
<span class="normal"><a href="#__codelineno-7-45">45</a></span>
<span class="normal"><a href="#__codelineno-7-46">46</a></span>
<span class="normal"><a href="#__codelineno-7-47">47</a></span>
<span class="normal"><a href="#__codelineno-7-48">48</a></span>
<span class="normal"><a href="#__codelineno-7-49">49</a></span>
<span class="normal"><a href="#__codelineno-7-50">50</a></span>
<span class="normal"><a href="#__codelineno-7-51">51</a></span>
<span class="normal"><a href="#__codelineno-7-52">52</a></span>
<span class="normal"><a href="#__codelineno-7-53">53</a></span>
<span class="normal"><a href="#__codelineno-7-54">54</a></span>
<span class="normal"><a href="#__codelineno-7-55">55</a></span>
<span class="normal"><a href="#__codelineno-7-56">56</a></span>
<span class="normal"><a href="#__codelineno-7-57">57</a></span>
<span class="normal"><a href="#__codelineno-7-58">58</a></span>
<span class="normal"><a href="#__codelineno-7-59">59</a></span>
<span class="normal"><a href="#__codelineno-7-60">60</a></span>
<span class="normal"><a href="#__codelineno-7-61">61</a></span>
<span class="normal"><a href="#__codelineno-7-62">62</a></span>
<span class="normal"><a href="#__codelineno-7-63">63</a></span>
<span class="normal"><a href="#__codelineno-7-64">64</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
</span><span id="__span-7-3"><a id="__codelineno-7-3" name="__codelineno-7-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">statistics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span>
</span><span id="__span-7-4"><a id="__codelineno-7-4" name="__codelineno-7-4"></a>
</span><span id="__span-7-5"><a id="__codelineno-7-5" name="__codelineno-7-5"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ort</span>
</span><span id="__span-7-6"><a id="__codelineno-7-6" name="__codelineno-7-6"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-7-7"><a id="__codelineno-7-7" name="__codelineno-7-7"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
</span><span id="__span-7-8"><a id="__codelineno-7-8" name="__codelineno-7-8"></a>
</span><span id="__span-7-9"><a id="__codelineno-7-9" name="__codelineno-7-9"></a>
</span><span id="__span-7-10"><a id="__codelineno-7-10" name="__codelineno-7-10"></a><span class="k">def</span><span class="w"> </span><span class="nf">timing_decorator</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">function_repeat</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">timing_repeat</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
</span><span id="__span-7-11"><a id="__codelineno-7-11" name="__codelineno-7-11"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Decorator that times the execution of a function.&quot;&quot;&quot;</span>
</span><span id="__span-7-12"><a id="__codelineno-7-12" name="__codelineno-7-12"></a>
</span><span id="__span-7-13"><a id="__codelineno-7-13" name="__codelineno-7-13"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="__span-7-14"><a id="__codelineno-7-14" name="__codelineno-7-14"></a>        <span class="n">timing_results</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-7-15"><a id="__codelineno-7-15" name="__codelineno-7-15"></a>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">timing_repeat</span><span class="p">):</span>
</span><span id="__span-7-16"><a id="__codelineno-7-16" name="__codelineno-7-16"></a>            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="__span-7-17"><a id="__codelineno-7-17" name="__codelineno-7-17"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">function_repeat</span><span class="p">):</span>
</span><span id="__span-7-18"><a id="__codelineno-7-18" name="__codelineno-7-18"></a>                <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="__span-7-19"><a id="__codelineno-7-19" name="__codelineno-7-19"></a>            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="__span-7-20"><a id="__codelineno-7-20" name="__codelineno-7-20"></a>            <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span>
</span><span id="__span-7-21"><a id="__codelineno-7-21" name="__codelineno-7-21"></a>            <span class="n">timing_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">elapsed_time</span><span class="p">)</span>
</span><span id="__span-7-22"><a id="__codelineno-7-22" name="__codelineno-7-22"></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Avg +- Stddev: </span><span class="si">{</span><span class="n">mean</span><span class="p">(</span><span class="n">timing_results</span><span class="p">)</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2"> +- </span><span class="si">{</span><span class="n">stdev</span><span class="p">(</span><span class="n">timing_results</span><span class="p">)</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</span><span id="__span-7-23"><a id="__codelineno-7-23" name="__codelineno-7-23"></a>        <span class="k">return</span> <span class="n">result</span>
</span><span id="__span-7-24"><a id="__codelineno-7-24" name="__codelineno-7-24"></a>
</span><span id="__span-7-25"><a id="__codelineno-7-25" name="__codelineno-7-25"></a>    <span class="k">return</span> <span class="n">wrapper</span>
</span><span id="__span-7-26"><a id="__codelineno-7-26" name="__codelineno-7-26"></a>
</span><span id="__span-7-27"><a id="__codelineno-7-27" name="__codelineno-7-27"></a>
</span><span id="__span-7-28"><a id="__codelineno-7-28" name="__codelineno-7-28"></a><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">()</span>
</span><span id="__span-7-29"><a id="__codelineno-7-29" name="__codelineno-7-29"></a><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span><span id="__span-7-30"><a id="__codelineno-7-30" name="__codelineno-7-30"></a>
</span><span id="__span-7-31"><a id="__codelineno-7-31" name="__codelineno-7-31"></a><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
</span><span id="__span-7-32"><a id="__codelineno-7-32" name="__codelineno-7-32"></a><span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s2">&quot;win32&quot;</span><span class="p">:</span>
</span><span id="__span-7-33"><a id="__codelineno-7-33" name="__codelineno-7-33"></a>    <span class="c1"># Windows doesn&#39;t support the new TorchDynamo-based ONNX Exporter</span>
</span><span id="__span-7-34"><a id="__codelineno-7-34" name="__codelineno-7-34"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
</span><span id="__span-7-35"><a id="__codelineno-7-35" name="__codelineno-7-35"></a>        <span class="n">model</span><span class="p">,</span>
</span><span id="__span-7-36"><a id="__codelineno-7-36" name="__codelineno-7-36"></a>        <span class="n">dummy_input</span><span class="p">,</span>
</span><span id="__span-7-37"><a id="__codelineno-7-37" name="__codelineno-7-37"></a>        <span class="s2">&quot;resnet18.onnx&quot;</span><span class="p">,</span>
</span><span id="__span-7-38"><a id="__codelineno-7-38" name="__codelineno-7-38"></a>        <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input.1&quot;</span><span class="p">],</span>
</span><span id="__span-7-39"><a id="__codelineno-7-39" name="__codelineno-7-39"></a>        <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input.1&quot;</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;width&quot;</span><span class="p">}},</span>
</span><span id="__span-7-40"><a id="__codelineno-7-40" name="__codelineno-7-40"></a>    <span class="p">)</span>
</span><span id="__span-7-41"><a id="__codelineno-7-41" name="__codelineno-7-41"></a><span class="k">else</span><span class="p">:</span>
</span><span id="__span-7-42"><a id="__codelineno-7-42" name="__codelineno-7-42"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">dynamo_export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;resnet18.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-7-43"><a id="__codelineno-7-43" name="__codelineno-7-43"></a>
</span><span id="__span-7-44"><a id="__codelineno-7-44" name="__codelineno-7-44"></a><span class="n">ort_session</span> <span class="o">=</span> <span class="n">ort</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;resnet18.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-7-45"><a id="__codelineno-7-45" name="__codelineno-7-45"></a>
</span><span id="__span-7-46"><a id="__codelineno-7-46" name="__codelineno-7-46"></a>
</span><span id="__span-7-47"><a id="__codelineno-7-47" name="__codelineno-7-47"></a><span class="nd">@timing_decorator</span>
</span><span id="__span-7-48"><a id="__codelineno-7-48" name="__codelineno-7-48"></a><span class="k">def</span><span class="w"> </span><span class="nf">torch_predict</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-7-49"><a id="__codelineno-7-49" name="__codelineno-7-49"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Predict using PyTorch model.&quot;&quot;&quot;</span>
</span><span id="__span-7-50"><a id="__codelineno-7-50" name="__codelineno-7-50"></a>    <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span><span id="__span-7-51"><a id="__codelineno-7-51" name="__codelineno-7-51"></a>
</span><span id="__span-7-52"><a id="__codelineno-7-52" name="__codelineno-7-52"></a>
</span><span id="__span-7-53"><a id="__codelineno-7-53" name="__codelineno-7-53"></a><span class="nd">@timing_decorator</span>
</span><span id="__span-7-54"><a id="__codelineno-7-54" name="__codelineno-7-54"></a><span class="k">def</span><span class="w"> </span><span class="nf">onnx_predict</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-7-55"><a id="__codelineno-7-55" name="__codelineno-7-55"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Predict using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-7-56"><a id="__codelineno-7-56" name="__codelineno-7-56"></a>    <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input.1&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">numpy</span><span class="p">()})</span>
</span><span id="__span-7-57"><a id="__codelineno-7-57" name="__codelineno-7-57"></a>
</span><span id="__span-7-58"><a id="__codelineno-7-58" name="__codelineno-7-58"></a>
</span><span id="__span-7-59"><a id="__codelineno-7-59" name="__codelineno-7-59"></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span id="__span-7-60"><a id="__codelineno-7-60" name="__codelineno-7-60"></a>    <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">448</span><span class="p">,</span> <span class="mi">896</span><span class="p">]:</span>
</span><span id="__span-7-61"><a id="__codelineno-7-61" name="__codelineno-7-61"></a>        <span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span><span id="__span-7-62"><a id="__codelineno-7-62" name="__codelineno-7-62"></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image size: </span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span id="__span-7-63"><a id="__codelineno-7-63" name="__codelineno-7-63"></a>        <span class="n">torch_predict</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
</span><span id="__span-7-64"><a id="__codelineno-7-64" name="__codelineno-7-64"></a>        <span class="n">onnx_predict</span><span class="p">(</span><span class="n">dummy_input</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
<li>
<p>To get a better understanding of why running the model using the ONNX runtime is usually faster let's try to see
    what happens to the computational graph. By default the ONNX Runtime will apply this optimization in <em>online</em>
    mode, meaning that the optimizations are applied when the model is loaded. However, it is also possible to apply
    the optimizations in <em>offline</em> mode, such that the optimized model is saved to disk. Below is an example of how
    to do this.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">rt</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">sess_options</span> <span class="o">=</span> <span class="n">rt</span><span class="o">.</span><span class="n">SessionOptions</span><span class="p">()</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="c1"># Set graph optimization level</span>
</span><span id="__span-8-5"><a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="n">sess_options</span><span class="o">.</span><span class="n">graph_optimization_level</span> <span class="o">=</span> <span class="n">rt</span><span class="o">.</span><span class="n">GraphOptimizationLevel</span><span class="o">.</span><span class="n">ORT_ENABLE_EXTENDED</span>
</span><span id="__span-8-6"><a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>
</span><span id="__span-8-7"><a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="c1"># To enable model serialization after graph optimization set this</span>
</span><span id="__span-8-8"><a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a><span class="n">sess_options</span><span class="o">.</span><span class="n">optimized_model_filepath</span> <span class="o">=</span> <span class="s2">&quot;optimized_model.onnx&gt;&quot;</span>
</span><span id="__span-8-9"><a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>
</span><span id="__span-8-10"><a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a><span class="n">session</span> <span class="o">=</span> <span class="n">rt</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;&lt;model_path&gt;&quot;</span><span class="p">,</span> <span class="n">sess_options</span><span class="p">)</span>
</span></code></pre></div>
<p>Try to apply the optimizations in offline mode and use <code>netron</code> to visualize both the original and optimized
models side by side. Can you see any differences?</p>
<details class="success">
<summary>Solution</summary>
<p>You should hopefully see that the optimized model consists of fewer nodes and edges than the original model.
These nodes are often called fused nodes, because they are the result of multiple nodes being fused
together. In the image below we have visualized the first part of the computational graph of a resnet18
model, before and after optimization.</p>
<p><figure markdown>
<a class="glightbox" href="../../figures/onnx_optimization.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Image" src="../../figures/onnx_optimization.png" width="600" /></a>
</figure></p>
</details>
</li>
<li>
<p>Exporting a model to ONNX is not always perfect out of the box. To the conversion of your PyTorch models there
    need to be a one-to-one correspondence between PyTorch and ONNX operators. Especially, the
    <a href="http://onnx.ai/sklearn-onnx/auto_tutorial/plot_cbegin_opset.html">opset number</a> is important to set correctly
    in ONNX to get the correct operators. If this is not the case, the exported model can lead to a difference in
    results. To check the model, it is therefore also a good idea to check if the difference between the PyTorch
    and ONNX model is within a certain threshold. Implement a simple function that loads the model using PyTorch
    and ONNX and checks if the difference between the two models is within a certain threshold.</p>
<details class="success">
<summary>Solution</summary>
<p>The function below should work for a neural network which takes in a single input tensor and returns a
single output tensor. If this is not the case, you will need to modify the function to fit your model.</p>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">onnx_check.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-9-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-9-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-9-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-9-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-9-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-9-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-9-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-9-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-9-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-9-10">10</a></span>
<span class="normal"><a href="#__codelineno-9-11">11</a></span>
<span class="normal"><a href="#__codelineno-9-12">12</a></span>
<span class="normal"><a href="#__codelineno-9-13">13</a></span>
<span class="normal"><a href="#__codelineno-9-14">14</a></span>
<span class="normal"><a href="#__codelineno-9-15">15</a></span>
<span class="normal"><a href="#__codelineno-9-16">16</a></span>
<span class="normal"><a href="#__codelineno-9-17">17</a></span>
<span class="normal"><a href="#__codelineno-9-18">18</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2"></a>
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3"></a><span class="k">def</span><span class="w"> </span><span class="nf">check_onnx_model</span><span class="p">(</span>
</span><span id="__span-9-4"><a id="__codelineno-9-4" name="__codelineno-9-4"></a>    <span class="n">onnx_model_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span><span id="__span-9-5"><a id="__codelineno-9-5" name="__codelineno-9-5"></a>    <span class="n">pytorch_model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span><span id="__span-9-6"><a id="__codelineno-9-6" name="__codelineno-9-6"></a>    <span class="n">random_input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="__span-9-7"><a id="__codelineno-9-7" name="__codelineno-9-7"></a>    <span class="n">rtol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-03</span><span class="p">,</span>
</span><span id="__span-9-8"><a id="__codelineno-9-8" name="__codelineno-9-8"></a>    <span class="n">atol</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-05</span><span class="p">,</span>
</span><span id="__span-9-9"><a id="__codelineno-9-9" name="__codelineno-9-9"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-9-10"><a id="__codelineno-9-10" name="__codelineno-9-10"></a>    <span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">rt</span>
</span><span id="__span-9-11"><a id="__codelineno-9-11" name="__codelineno-9-11"></a>    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-9-12"><a id="__codelineno-9-12" name="__codelineno-9-12"></a>
</span><span id="__span-9-13"><a id="__codelineno-9-13" name="__codelineno-9-13"></a>    <span class="n">ort_session</span> <span class="o">=</span> <span class="n">rt</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="n">onnx_model_file</span><span class="p">)</span>
</span><span id="__span-9-14"><a id="__codelineno-9-14" name="__codelineno-9-14"></a>    <span class="n">ort_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">ort_session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">random_input</span><span class="o">.</span><span class="n">numpy</span><span class="p">()}</span>
</span><span id="__span-9-15"><a id="__codelineno-9-15" name="__codelineno-9-15"></a>    <span class="n">ort_outs</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">ort_inputs</span><span class="p">)</span>
</span><span id="__span-9-16"><a id="__codelineno-9-16" name="__codelineno-9-16"></a>    <span class="n">pytorch_outs</span> <span class="o">=</span> <span class="n">pytorch_model</span><span class="p">(</span><span class="n">random_input</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span><span id="__span-9-17"><a id="__codelineno-9-17" name="__codelineno-9-17"></a>
</span><span id="__span-9-18"><a id="__codelineno-9-18" name="__codelineno-9-18"></a>    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ort_outs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pytorch_outs</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
</ol>
</li>
<li>
<p>As mentioned in the introduction, ONNX is able to run on many different types of hardware and execution engines.
    You can check all the providers and all the available providers by running the following code:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="nb">print</span><span class="p">(</span><span class="n">onnxruntime</span><span class="o">.</span><span class="n">get_all_providers</span><span class="p">())</span>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a><span class="nb">print</span><span class="p">(</span><span class="n">onnxruntime</span><span class="o">.</span><span class="n">get_available_providers</span><span class="p">())</span>
</span></code></pre></div>
<p>Can you figure out how to set which provider the ONNX runtime should use?</p>
<details class="success">
<summary>Solution</summary>
<p>The provider that the ONNX runtime should use can be set by passing the <code>providers</code> argument to the
<code>InferenceSession</code> class. A list should be provided, which prioritizes the providers in the order they are
listed.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">rt</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="n">provider_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;CUDAExecutionProvider&#39;</span><span class="p">,</span> <span class="s1">&#39;CPUExecutionProvider&#39;</span><span class="p">]</span>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="n">ort_session</span> <span class="o">=</span> <span class="n">rt</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;&lt;path-to-model&gt;&quot;</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="n">provider_list</span><span class="p">)</span>
</span></code></pre></div>
<p>In this case we will prefer CUDA Execution Provider over CPU Execution Provider if both are available.</p>
</details>
</li>
<li>
<p>As you have probably realized in the exercises <a href="../../s3_reproducibility/docker/">on docker</a>, it can take a long time
    to build the kind of containers we are working with and they can be quite large. There is a reason for this and that
    is that PyTorch is a very large framework with a lot of dependencies. ONNX on the other hand is a much smaller
    framework. This kind of makes sense, because PyTorch is a framework that primarily was designed for developing, e.g.
    training models, while ONNX is a framework that is designed for serving models. Let's try to quantify this.</p>
<ol>
<li>
<p>Construct a dockerfile that builds a docker image with PyTorch as a dependency. The dockerfile does not actually
    need to run anything. Repeat the same process for the ONNX runtime. Bonus point for developing a docker
    image that takes a <a href="https://docs.docker.com/build/guide/build-args/">build arg</a> at build time that specifies
    if the image should be built with CUDA support or not.</p>
<details class="success">
<summary>Solution</summary>
<p>The dockerfile for the PyTorch image could look something like this</p>
<div class="language-dockerfile highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">inference_pytorch.dockerfile</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-12-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-12-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-12-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-12-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-12-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-12-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-12-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-12-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-12-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-12-10">10</a></span>
<span class="normal"><a href="#__codelineno-12-11">11</a></span>
<span class="normal"><a href="#__codelineno-12-12">12</a></span>
<span class="normal"><a href="#__codelineno-12-13">13</a></span>
<span class="normal"><a href="#__codelineno-12-14">14</a></span>
<span class="normal"><a href="#__codelineno-12-15">15</a></span>
<span class="normal"><a href="#__codelineno-12-16">16</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1"></a><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.11-slim</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2"></a>
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3"></a><span class="k">RUN</span><span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4"></a><span class="w">    </span>apt<span class="w"> </span>install<span class="w"> </span>--no-install-recommends<span class="w"> </span>-y<span class="w"> </span>build-essential<span class="w"> </span>gcc<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5"></a><span class="w">    </span>apt<span class="w"> </span>clean<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>rm<span class="w"> </span>-rf<span class="w"> </span>/var/lib/apt/lists/*
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6"></a>
</span><span id="__span-12-7"><a id="__codelineno-12-7" name="__codelineno-12-7"></a><span class="k">ARG</span><span class="w"> </span>CUDA
</span><span id="__span-12-8"><a id="__codelineno-12-8" name="__codelineno-12-8"></a><span class="k">ENV</span><span class="w"> </span><span class="nv">CUDA</span><span class="o">=</span><span class="si">${</span><span class="nv">CUDA</span><span class="si">}</span>
</span><span id="__span-12-9"><a id="__codelineno-12-9" name="__codelineno-12-9"></a><span class="k">RUN</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;CUDA is set to: </span><span class="si">${</span><span class="nv">CUDA</span><span class="si">}</span><span class="s2">&quot;</span>
</span><span id="__span-12-10"><a id="__codelineno-12-10" name="__codelineno-12-10"></a>
</span><span id="__span-12-11"><a id="__codelineno-12-11" name="__codelineno-12-11"></a><span class="k">RUN</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;CUDA is set to: </span><span class="si">${</span><span class="nv">CUDA</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-12"><a id="__codelineno-12-12" name="__codelineno-12-12"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>-n<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$CUDA</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-13"><a id="__codelineno-12-13" name="__codelineno-12-13"></a><span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>torch<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cu121<span class="p">;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-14"><a id="__codelineno-12-14" name="__codelineno-12-14"></a><span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-15"><a id="__codelineno-12-15" name="__codelineno-12-15"></a><span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>torch<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cpu<span class="p">;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-12-16"><a id="__codelineno-12-16" name="__codelineno-12-16"></a><span class="w">    </span><span class="k">fi</span>
</span></code></pre></div></td></tr></table></div>
<p>and the dockerfile for the ONNX image could look something like this</p>
<div class="language-dockerfile highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">inference_onnx.dockerfile</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-13-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-13-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-13-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-13-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-13-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-13-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-13-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-13-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-13-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-13-10">10</a></span>
<span class="normal"><a href="#__codelineno-13-11">11</a></span>
<span class="normal"><a href="#__codelineno-13-12">12</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1"></a><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.11-slim</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2"></a>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3"></a><span class="k">RUN</span><span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4"></a><span class="w">    </span>apt<span class="w"> </span>install<span class="w"> </span>--no-install-recommends<span class="w"> </span>-y<span class="w"> </span>build-essential<span class="w"> </span>gcc<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-13-5"><a id="__codelineno-13-5" name="__codelineno-13-5"></a><span class="w">    </span>apt<span class="w"> </span>clean<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>rm<span class="w"> </span>-rf<span class="w"> </span>/var/lib/apt/lists/*
</span><span id="__span-13-6"><a id="__codelineno-13-6" name="__codelineno-13-6"></a>
</span><span id="__span-13-7"><a id="__codelineno-13-7" name="__codelineno-13-7"></a><span class="k">RUN</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;CUDA is set to: </span><span class="si">${</span><span class="nv">CUDA</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-13-8"><a id="__codelineno-13-8" name="__codelineno-13-8"></a><span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span>-n<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$CUDA</span><span class="s2">&quot;</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-13-9"><a id="__codelineno-13-9" name="__codelineno-13-9"></a><span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>onnxruntime-gpu<span class="p">;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-13-10"><a id="__codelineno-13-10" name="__codelineno-13-10"></a><span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-13-11"><a id="__codelineno-13-11" name="__codelineno-13-11"></a><span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>onnxruntime<span class="p">;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-13-12"><a id="__codelineno-13-12" name="__codelineno-13-12"></a><span class="w">    </span><span class="k">fi</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
<li>
<p>Build both containers and measure the time it takes to build them. How much faster is it to build the ONNX
    container compared to the PyTorch container?</p>
<details class="success">
<summary>Solution</summary>
<p>On unix/linux you can use the <a href="https://linuxize.com/post/linux-time-command/">time</a> command to measure
the time it takes to build the containers. Building both images, with and without CUDA support, can be done
with the following commands:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="nb">time</span><span class="w"> </span>docker<span class="w"> </span>build<span class="w"> </span>.<span class="w"> </span>-t<span class="w"> </span>pytorch_inference_cuda:latest<span class="w"> </span>-f<span class="w"> </span>inference_pytorch.dockerfile<span class="w"> </span><span class="se">\</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="w">    </span>--no-cache<span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">CUDA</span><span class="o">=</span><span class="nb">true</span>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="nb">time</span><span class="w"> </span>docker<span class="w"> </span>build<span class="w"> </span>.<span class="w"> </span>-t<span class="w"> </span>pytorch_inference:latest<span class="w"> </span>-f<span class="w"> </span>inference_pytorch.dockerfile<span class="w"> </span><span class="se">\</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="w">    </span>--no-cache<span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">CUDA</span><span class="o">=</span>
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="nb">time</span><span class="w"> </span>docker<span class="w"> </span>build<span class="w"> </span>.<span class="w"> </span>-t<span class="w"> </span>onnx_inference_cuda:latest<span class="w"> </span>-f<span class="w"> </span>inference_onnx.dockerfile<span class="w"> </span><span class="se">\</span>
</span><span id="__span-14-6"><a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a><span class="w">    </span>--no-cache<span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">CUDA</span><span class="o">=</span><span class="nb">true</span>
</span><span id="__span-14-7"><a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a><span class="nb">time</span><span class="w"> </span>docker<span class="w"> </span>build<span class="w"> </span>.<span class="w"> </span>-t<span class="w"> </span>onnx_inference:latest<span class="w"> </span>-f<span class="w"> </span>inference_onnx.dockerfile<span class="w"> </span><span class="se">\</span>
</span><span id="__span-14-8"><a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a><span class="w">    </span>--no-cache<span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">CUDA</span><span class="o">=</span>
</span></code></pre></div>
<p>The <code>--no-cache</code> flag is used to ensure that the build process is not cached and ensures a fair comparison.
On my laptop this respectively took <code>5m1s</code>, <code>1m4s</code>, <code>0m4s</code>, <code>0m50s</code> meaning that the ONNX
container was respectively 7x (with CUDA) and 1.28x (no CUDA) faster to build than the PyTorch container.</p>
</details>
</li>
<li>
<p>Figure out the sizes of the two docker images. This can be done in the terminal by running the <code>docker images</code>
    command. How much smaller is the ONNX model compared to the PyTorch model?</p>
<details class="success">
<summary>Solution</summary>
<p>As of writing, the docker image containing the PyTorch framework was 5.54GB (with CUDA) and 1.25GB (no CUDA).
In comparison the ONNX image was 647MB (with CUDA) and 647MB (no CUDA). This means that the ONNX image is
respectively 8.5x (with CUDA) and 1.94x (no CUDA) smaller than the PyTorch image.</p>
</details>
</li>
</ol>
</li>
<li>
<p>(Optional) Assuming you have completed the module on <a href="../apis/">FastAPI</a>, try creating a small
    FastAPI application that serves a model using the ONNX runtime.</p>
<details class="success">
<summary>Solution</summary>
<p>Here is a simple example of how to create a FastAPI application that serves a model using the ONNX runtime.</p>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">onnx_fastapi.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-15-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-15-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-15-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-15-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-15-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-15-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-15-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-15-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-15-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-15-10">10</a></span>
<span class="normal"><a href="#__codelineno-15-11">11</a></span>
<span class="normal"><a href="#__codelineno-15-12">12</a></span>
<span class="normal"><a href="#__codelineno-15-13">13</a></span>
<span class="normal"><a href="#__codelineno-15-14">14</a></span>
<span class="normal"><a href="#__codelineno-15-15">15</a></span>
<span class="normal"><a href="#__codelineno-15-16">16</a></span>
<span class="normal"><a href="#__codelineno-15-17">17</a></span>
<span class="normal"><a href="#__codelineno-15-18">18</a></span>
<span class="normal"><a href="#__codelineno-15-19">19</a></span>
<span class="normal"><a href="#__codelineno-15-20">20</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">fastapi</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastAPI</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4"></a>
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5"></a><span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>
</span><span id="__span-15-6"><a id="__codelineno-15-6" name="__codelineno-15-6"></a>
</span><span id="__span-15-7"><a id="__codelineno-15-7" name="__codelineno-15-7"></a>
</span><span id="__span-15-8"><a id="__codelineno-15-8" name="__codelineno-15-8"></a><span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;/predict&quot;</span><span class="p">)</span>
</span><span id="__span-15-9"><a id="__codelineno-15-9" name="__codelineno-15-9"></a><span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">():</span>
</span><span id="__span-15-10"><a id="__codelineno-15-10" name="__codelineno-15-10"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Predict using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-15-11"><a id="__codelineno-15-11" name="__codelineno-15-11"></a>    <span class="c1"># Load the ONNX model</span>
</span><span id="__span-15-12"><a id="__codelineno-15-12" name="__codelineno-15-12"></a>    <span class="n">model</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-15-13"><a id="__codelineno-15-13" name="__codelineno-15-13"></a>
</span><span id="__span-15-14"><a id="__codelineno-15-14" name="__codelineno-15-14"></a>    <span class="c1"># Prepare the input data</span>
</span><span id="__span-15-15"><a id="__codelineno-15-15" name="__codelineno-15-15"></a>    <span class="n">input_data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)}</span>
</span><span id="__span-15-16"><a id="__codelineno-15-16" name="__codelineno-15-16"></a>
</span><span id="__span-15-17"><a id="__codelineno-15-17" name="__codelineno-15-17"></a>    <span class="c1"># Run the model</span>
</span><span id="__span-15-18"><a id="__codelineno-15-18" name="__codelineno-15-18"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
</span><span id="__span-15-19"><a id="__codelineno-15-19" name="__codelineno-15-19"></a>
</span><span id="__span-15-20"><a id="__codelineno-15-20" name="__codelineno-15-20"></a>    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()}</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
</ol>
<p>This completes the exercises on the ONNX format. Do note that one limitation of the ONNX format is that it is based on
<a href="https://protobuf.dev/">ProtoBuf</a>, which is a binary format. A protobuf file can have a maximum size of 2GB, which means
that the <code>.onnx</code> format is not enough for very large models. However, through the use of
<a href="https://onnxruntime.ai/docs/tutorials/web/large-models.html">external data</a> it is possible to circumvent this
limitation.</p>
<h2 id="bentoml">BentoML</h2>
<div class="admonition note">
<p class="admonition-title">BentoML cloud vs BentoML OSS</p>
<p>We are only going to be looking at the open-source version of BentoML in this module. However, BentoML also has a
cloud version that makes it very easy to deploy models that are coded in BentoML to the cloud. If you are interested
in this, you can check out the
<a href="https://docs.bentoml.com/en/latest/guides/cloud/index.html">BentoML cloud documentation</a>. This business strategy
of having an open-source product and a cloud product is very common in the machine learning space (HuggingFace,
LightningAI, Weights and Biases, etc.), because it allows companies to make money from the cloud product while still
providing a free product to the community.</p>
</div>
<p>BentoML is a framework that is designed to make it easy to serve machine learning models. It is designed to be backend
agnostic, meaning that it can be used with any computational backend. It is also model agnostic, meaning that it can be
used with any machine learning model.</p>
<p>Let's consider a simple example of how to serve a model using BentoML. The following
<a href="https://docs.bentoml.com/en/latest/get-started/quickstart.html">code snippet</a> shows how to serve a model that uses
the <code>transformers</code> library to summarize text.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">bentoml</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="n">EXAMPLE_INPUT</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>    <span class="s2">&quot;Breaking News: In an astonishing turn of events, the small town of Willow Creek has been taken by storm as &quot;</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a>    <span class="s2">&quot;local resident Jerry Thompson&#39;s cat, Whiskers, performed what witnesses are calling a &#39;miraculous and gravity-&quot;</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>    <span class="s2">&quot;defying leap.&#39; Eyewitnesses report that Whiskers, an otherwise unremarkable tabby cat, jumped a record-breaking &quot;</span>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a>    <span class="s2">&quot;20 feet into the air to catch a fly. The event, which took place in Thompson&#39;s backyard, is now being investigated &quot;</span>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a>    <span class="s2">&quot;by scientists for potential breaches in the laws of physics. Local authorities are considering a town festival to &quot;</span>
</span><span id="__span-16-10"><a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a>    <span class="s2">&quot;celebrate what is being hailed as &#39;The Leap of the Century.&#39;&quot;</span>
</span><span id="__span-16-11"><a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="p">)</span>
</span><span id="__span-16-12"><a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>
</span><span id="__span-16-13"><a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span><span class="p">(</span><span class="n">resources</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;cpu&quot;</span><span class="p">:</span> <span class="s2">&quot;2&quot;</span><span class="p">},</span> <span class="n">traffic</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;timeout&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>
</span><span id="__span-16-14"><a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a><span class="k">class</span><span class="w"> </span><span class="nc">Summarization</span><span class="p">:</span>
</span><span id="__span-16-15"><a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-16-16"><a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;summarization&#39;</span><span class="p">)</span>
</span><span id="__span-16-17"><a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a>
</span><span id="__span-16-18"><a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span>
</span><span id="__span-16-19"><a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">summarize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">EXAMPLE_INPUT</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span><span id="__span-16-20"><a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a>        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipeline</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</span><span id="__span-16-21"><a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a>        <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;summary_text&#39;</span><span class="p">]</span>
</span></code></pre></div>
<p>In <code>BentoML</code> we organize our services in classes, where each class is a service that we want to serve. The two important
parts of the code snippet are the <code>@bentoml.service</code> and <code>@bentoml.api</code> decorators.</p>
<ul>
<li>
<p>The <code>@bentoml.service</code> decorator is used to specify the resources that the service should use and in general how the
    service should be run. In this case we are specifying that the service should use 2 CPU cores and that the timeout
    for the service should be 10 seconds.</p>
</li>
<li>
<p>The <code>@bentoml.api</code> decorator is used to specify the API that the service should expose. In this case we are specifying
    that the service should have an API called <code>summarize</code> that takes a string as input and returns a string as output.</p>
</li>
</ul>
<p>To serve the model using <code>BentoML</code> we can execute the following command, which is very similar to the command we used
to serve the model using FastAPI.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>bentoml<span class="w"> </span>serve<span class="w"> </span>service:Summarization
</span></code></pre></div>
<h3 id="exercises_1">❔ Exercises</h3>
<p>In general, we recommend looking through the <a href="https://docs.bentoml.com/en/latest/index.html">docs</a> for Bento ML if you
need help with any of the exercises. We are going to assume that you have done the exercises on ONNX and we are
therefore going to be using <code>BentoML</code> to serve ONNX models. If you have not done that part, you can still follow along
but you will need to use a PyTorch model instead of an ONNX model.</p>
<ol>
<li>
<p>Install BentoML.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>pip<span class="w"> </span>install<span class="w"> </span>bentoml
</span></code></pre></div>
<p>Remember to add the dependency to your <code>requirements.txt</code> file.</p>
</li>
<li>
<p>You are in principal free to serve any model you like, but we recommend just using a
    <a href="https://pytorch.org/vision/stable/index.html">torchvision</a> model as in the ONNX exercises. Write your first service
    in <code>BentoML</code> that serves a model of your choice. We recommend experimenting with providing
    <a href="https://docs.bentoml.com/en/latest/guides/iotypes.html">input/output as tensors</a> because bentoml supports this
    natively. Secondly, write a client that can send a request to the service and print the result. Here we recommend
    using the built-in <a href="https://docs.bentoml.com/en/latest/reference/client.html">bentoml.SyncHTTPClient</a>.</p>
<details class="success">
<summary>Solution</summary>
<p>The following implements a simple BentoML service that serves an ONNX resnet18 model. The service expects
both the input and output to be numpy arrays.</p>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">bentoml_service.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-19-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-19-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-19-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-19-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-19-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-19-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-19-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-19-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-19-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-19-10">10</a></span>
<span class="normal"><a href="#__codelineno-19-11">11</a></span>
<span class="normal"><a href="#__codelineno-19-12">12</a></span>
<span class="normal"><a href="#__codelineno-19-13">13</a></span>
<span class="normal"><a href="#__codelineno-19-14">14</a></span>
<span class="normal"><a href="#__codelineno-19-15">15</a></span>
<span class="normal"><a href="#__codelineno-19-16">16</a></span>
<span class="normal"><a href="#__codelineno-19-17">17</a></span>
<span class="normal"><a href="#__codelineno-19-18">18</a></span>
<span class="normal"><a href="#__codelineno-19-19">19</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2"></a>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">bentoml</span>
</span><span id="__span-19-4"><a id="__codelineno-19-4" name="__codelineno-19-4"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-19-5"><a id="__codelineno-19-5" name="__codelineno-19-5"></a><span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceSession</span>
</span><span id="__span-19-6"><a id="__codelineno-19-6" name="__codelineno-19-6"></a>
</span><span id="__span-19-7"><a id="__codelineno-19-7" name="__codelineno-19-7"></a>
</span><span id="__span-19-8"><a id="__codelineno-19-8" name="__codelineno-19-8"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span>
</span><span id="__span-19-9"><a id="__codelineno-19-9" name="__codelineno-19-9"></a><span class="k">class</span><span class="w"> </span><span class="nc">ImageClassifierService</span><span class="p">:</span>
</span><span id="__span-19-10"><a id="__codelineno-19-10" name="__codelineno-19-10"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Image classifier service using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-19-11"><a id="__codelineno-19-11" name="__codelineno-19-11"></a>
</span><span id="__span-19-12"><a id="__codelineno-19-12" name="__codelineno-19-12"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-19-13"><a id="__codelineno-19-13" name="__codelineno-19-13"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-19-14"><a id="__codelineno-19-14" name="__codelineno-19-14"></a>
</span><span id="__span-19-15"><a id="__codelineno-19-15" name="__codelineno-19-15"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span>
</span><span id="__span-19-16"><a id="__codelineno-19-16" name="__codelineno-19-16"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-19-17"><a id="__codelineno-19-17" name="__codelineno-19-17"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict the class of the input image.&quot;&quot;&quot;</span>
</span><span id="__span-19-18"><a id="__codelineno-19-18" name="__codelineno-19-18"></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)})</span>
</span><span id="__span-19-19"><a id="__codelineno-19-19" name="__codelineno-19-19"></a>        <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></code></pre></div></td></tr></table></div>
<p>The service can be served using the following command:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>bentoml<span class="w"> </span>serve<span class="w"> </span>bentoml_service:ImageClassifierService
</span></code></pre></div>
<p>To test that the service works the following client can be used:</p>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">bentoml_client.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-21-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-21-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-21-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-21-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-21-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-21-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-21-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-21-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-21-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-21-10">10</a></span>
<span class="normal"><a href="#__codelineno-21-11">11</a></span>
<span class="normal"><a href="#__codelineno-21-12">12</a></span>
<span class="normal"><a href="#__codelineno-21-13">13</a></span>
<span class="normal"><a href="#__codelineno-21-14">14</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">bentoml</span>
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-21-3"><a id="__codelineno-21-3" name="__codelineno-21-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
</span><span id="__span-21-4"><a id="__codelineno-21-4" name="__codelineno-21-4"></a>
</span><span id="__span-21-5"><a id="__codelineno-21-5" name="__codelineno-21-5"></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
</span><span id="__span-21-6"><a id="__codelineno-21-6" name="__codelineno-21-6"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;my_cat.jpg&quot;</span><span class="p">)</span>
</span><span id="__span-21-7"><a id="__codelineno-21-7" name="__codelineno-21-7"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>  <span class="c1"># Resize to match the minimum input size of the model</span>
</span><span id="__span-21-8"><a id="__codelineno-21-8" name="__codelineno-21-8"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span><span id="__span-21-9"><a id="__codelineno-21-9" name="__codelineno-21-9"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Change to CHW format</span>
</span><span id="__span-21-10"><a id="__codelineno-21-10" name="__codelineno-21-10"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch dimension</span>
</span><span id="__span-21-11"><a id="__codelineno-21-11" name="__codelineno-21-11"></a>
</span><span id="__span-21-12"><a id="__codelineno-21-12" name="__codelineno-21-12"></a>    <span class="k">with</span> <span class="n">bentoml</span><span class="o">.</span><span class="n">SyncHTTPClient</span><span class="p">(</span><span class="s2">&quot;http://localhost:4040&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
</span><span id="__span-21-13"><a id="__codelineno-21-13" name="__codelineno-21-13"></a>        <span class="n">resp</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
</span><span id="__span-21-14"><a id="__codelineno-21-14" name="__codelineno-21-14"></a>        <span class="nb">print</span><span class="p">(</span><span class="n">resp</span><span class="p">)</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
<li>
<p>We are now going to look at features where <code>BentoML</code> really sets itself apart from <code>FastAPI</code>. The first is
    <em>adaptive batching</em>. As you are hopefully aware, modern machine learning models can process multiple samples at the
    same time and in doing so increase the throughput of the model. When we train a model we often set a fixed
    batch size, however we cannot do that when serving the model because that would mean that we would have to wait for
    the batch to be full before we can process it. <em>Adaptive batching</em> simply refers to the process where we specify a
    <em>maximum batch size</em> and also a <em>timeout</em>. When either the batch is full or the timeout is reached, however many
    samples we have collected are sent to the model for processing. This can be a very powerful feature because it
    allows us to process samples as soon as they arrive, while still taking advantage of the increased throughput of
    batching.</p>
<p><figure markdown>
<a class="glightbox" href="../../figures/bentoml_adaptive_batching.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Image" src="../../figures/bentoml_adaptive_batching.png" width="700" /></a>
<figcaption>
The overall architecture of the adaptive batching feature in BentoML. The feature is implemented on the server side
and mainly consists of a dispatcher that is in charge of collecting requests and sending them to the model server when
either the batch is full or a timeout is reached.
<a href="https://docs.bentoml.com/en/latest/guides/adaptive-batching.html"> Image credit </a>
</figcaption>
</figure></p>
<ol>
<li>
<p>Look through the
    <a href="https://docs.bentoml.com/en/latest/guides/adaptive-batching.html">documentation on adaptive batching</a> and
    add adaptive batching to your service from the previous exercise. Make sure your service works as expected by
    testing it with the client from the previous exercise.</p>
<details class="success">
<summary>Solution</summary>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">bentoml_service_adaptive_batching.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-22-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-22-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-22-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-22-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-22-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-22-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-22-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-22-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-22-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-22-10">10</a></span>
<span class="normal"><a href="#__codelineno-22-11">11</a></span>
<span class="normal"><a href="#__codelineno-22-12">12</a></span>
<span class="normal"><a href="#__codelineno-22-13">13</a></span>
<span class="normal"><a href="#__codelineno-22-14">14</a></span>
<span class="normal"><a href="#__codelineno-22-15">15</a></span>
<span class="normal"><a href="#__codelineno-22-16">16</a></span>
<span class="normal"><a href="#__codelineno-22-17">17</a></span>
<span class="normal"><a href="#__codelineno-22-18">18</a></span>
<span class="normal"><a href="#__codelineno-22-19">19</a></span>
<span class="normal"><a href="#__codelineno-22-20">20</a></span>
<span class="normal"><a href="#__codelineno-22-21">21</a></span>
<span class="normal"><a href="#__codelineno-22-22">22</a></span>
<span class="normal"><a href="#__codelineno-22-23">23</a></span>
<span class="normal"><a href="#__codelineno-22-24">24</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2"></a>
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">bentoml</span>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5"></a><span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceSession</span>
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6"></a>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7"></a>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span>
</span><span id="__span-22-9"><a id="__codelineno-22-9" name="__codelineno-22-9"></a><span class="k">class</span><span class="w"> </span><span class="nc">ImageClassifierService</span><span class="p">:</span>
</span><span id="__span-22-10"><a id="__codelineno-22-10" name="__codelineno-22-10"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Image classifier service using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-22-11"><a id="__codelineno-22-11" name="__codelineno-22-11"></a>
</span><span id="__span-22-12"><a id="__codelineno-22-12" name="__codelineno-22-12"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-22-13"><a id="__codelineno-22-13" name="__codelineno-22-13"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-22-14"><a id="__codelineno-22-14" name="__codelineno-22-14"></a>
</span><span id="__span-22-15"><a id="__codelineno-22-15" name="__codelineno-22-15"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span><span class="p">(</span>
</span><span id="__span-22-16"><a id="__codelineno-22-16" name="__codelineno-22-16"></a>        <span class="n">batchable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-22-17"><a id="__codelineno-22-17" name="__codelineno-22-17"></a>        <span class="n">batch_dim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
</span><span id="__span-22-18"><a id="__codelineno-22-18" name="__codelineno-22-18"></a>        <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
</span><span id="__span-22-19"><a id="__codelineno-22-19" name="__codelineno-22-19"></a>        <span class="n">max_latency_ms</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
</span><span id="__span-22-20"><a id="__codelineno-22-20" name="__codelineno-22-20"></a>    <span class="p">)</span>
</span><span id="__span-22-21"><a id="__codelineno-22-21" name="__codelineno-22-21"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-22-22"><a id="__codelineno-22-22" name="__codelineno-22-22"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict the class of the input image.&quot;&quot;&quot;</span>
</span><span id="__span-22-23"><a id="__codelineno-22-23" name="__codelineno-22-23"></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)})</span>
</span><span id="__span-22-24"><a id="__codelineno-22-24" name="__codelineno-22-24"></a>        <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
<li>
<p>Try to measure the throughput of your model with and without adaptive batching. Assuming that you have completed
    the <a href="../testing_apis/">module on testing APIs</a> and therefore are familiar with the <code>locust</code> framework, we
    recommend that you write a simple locustfile and use the <code>locust</code> command to measure the throughput of your
    model.</p>
<details class="success">
<summary>Solution</summary>
<p>The following locust file can be used to measure the throughput of the model with and without adaptive batching</p>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">locustfile.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-23-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-23-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-23-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-23-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-23-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-23-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-23-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-23-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-23-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-23-10">10</a></span>
<span class="normal"><a href="#__codelineno-23-11">11</a></span>
<span class="normal"><a href="#__codelineno-23-12">12</a></span>
<span class="normal"><a href="#__codelineno-23-13">13</a></span>
<span class="normal"><a href="#__codelineno-23-14">14</a></span>
<span class="normal"><a href="#__codelineno-23-15">15</a></span>
<span class="normal"><a href="#__codelineno-23-16">16</a></span>
<span class="normal"><a href="#__codelineno-23-17">17</a></span>
<span class="normal"><a href="#__codelineno-23-18">18</a></span>
<span class="normal"><a href="#__codelineno-23-19">19</a></span>
<span class="normal"><a href="#__codelineno-23-20">20</a></span>
<span class="normal"><a href="#__codelineno-23-21">21</a></span>
<span class="normal"><a href="#__codelineno-23-22">22</a></span>
<span class="normal"><a href="#__codelineno-23-23">23</a></span>
<span class="normal"><a href="#__codelineno-23-24">24</a></span>
<span class="normal"><a href="#__codelineno-23-25">25</a></span>
<span class="normal"><a href="#__codelineno-23-26">26</a></span>
<span class="normal"><a href="#__codelineno-23-27">27</a></span>
<span class="normal"><a href="#__codelineno-23-28">28</a></span>
<span class="normal"><a href="#__codelineno-23-29">29</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-23-2"><a id="__codelineno-23-2" name="__codelineno-23-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">locust</span><span class="w"> </span><span class="kn">import</span> <span class="n">HttpUser</span><span class="p">,</span> <span class="n">between</span><span class="p">,</span> <span class="n">task</span>
</span><span id="__span-23-3"><a id="__codelineno-23-3" name="__codelineno-23-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
</span><span id="__span-23-4"><a id="__codelineno-23-4" name="__codelineno-23-4"></a>
</span><span id="__span-23-5"><a id="__codelineno-23-5" name="__codelineno-23-5"></a>
</span><span id="__span-23-6"><a id="__codelineno-23-6" name="__codelineno-23-6"></a><span class="k">def</span><span class="w"> </span><span class="nf">prepare_image</span><span class="p">():</span>
</span><span id="__span-23-7"><a id="__codelineno-23-7" name="__codelineno-23-7"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Load and preprocess the image as required.&quot;&quot;&quot;</span>
</span><span id="__span-23-8"><a id="__codelineno-23-8" name="__codelineno-23-8"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;my_cat.jpg&quot;</span><span class="p">)</span>
</span><span id="__span-23-9"><a id="__codelineno-23-9" name="__codelineno-23-9"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</span><span id="__span-23-10"><a id="__codelineno-23-10" name="__codelineno-23-10"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span><span id="__span-23-11"><a id="__codelineno-23-11" name="__codelineno-23-11"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># Convert to CHW format</span>
</span><span id="__span-23-12"><a id="__codelineno-23-12" name="__codelineno-23-12"></a>    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch dimension</span>
</span><span id="__span-23-13"><a id="__codelineno-23-13" name="__codelineno-23-13"></a>    <span class="c1"># Convert to list format for JSON serialization</span>
</span><span id="__span-23-14"><a id="__codelineno-23-14" name="__codelineno-23-14"></a>    <span class="k">return</span> <span class="n">image</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</span><span id="__span-23-15"><a id="__codelineno-23-15" name="__codelineno-23-15"></a>
</span><span id="__span-23-16"><a id="__codelineno-23-16" name="__codelineno-23-16"></a>
</span><span id="__span-23-17"><a id="__codelineno-23-17" name="__codelineno-23-17"></a><span class="n">image</span> <span class="o">=</span> <span class="n">prepare_image</span><span class="p">()</span>
</span><span id="__span-23-18"><a id="__codelineno-23-18" name="__codelineno-23-18"></a>
</span><span id="__span-23-19"><a id="__codelineno-23-19" name="__codelineno-23-19"></a>
</span><span id="__span-23-20"><a id="__codelineno-23-20" name="__codelineno-23-20"></a><span class="k">class</span><span class="w"> </span><span class="nc">BentoMLUser</span><span class="p">(</span><span class="n">HttpUser</span><span class="p">):</span>
</span><span id="__span-23-21"><a id="__codelineno-23-21" name="__codelineno-23-21"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Locust user class for sending prediction requests to the server.&quot;&quot;&quot;</span>
</span><span id="__span-23-22"><a id="__codelineno-23-22" name="__codelineno-23-22"></a>
</span><span id="__span-23-23"><a id="__codelineno-23-23" name="__codelineno-23-23"></a>    <span class="n">wait_time</span> <span class="o">=</span> <span class="n">between</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span id="__span-23-24"><a id="__codelineno-23-24" name="__codelineno-23-24"></a>
</span><span id="__span-23-25"><a id="__codelineno-23-25" name="__codelineno-23-25"></a>    <span class="nd">@task</span>
</span><span id="__span-23-26"><a id="__codelineno-23-26" name="__codelineno-23-26"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">send_prediction_request</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span id="__span-23-27"><a id="__codelineno-23-27" name="__codelineno-23-27"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Send a prediction request to the server.&quot;&quot;&quot;</span>
</span><span id="__span-23-28"><a id="__codelineno-23-28" name="__codelineno-23-28"></a>        <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="n">image</span><span class="p">}</span>  <span class="c1"># Package the image as JSON</span>
</span><span id="__span-23-29"><a id="__codelineno-23-29" name="__codelineno-23-29"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">&quot;/predict&quot;</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Content-Type&quot;</span><span class="p">:</span> <span class="s2">&quot;application/json&quot;</span><span class="p">})</span>
</span></code></pre></div></td></tr></table></div>
<p>and then the following command can be used to measure the throughput of the model</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a>locust<span class="w"> </span>-f<span class="w"> </span>locustfile_bentoml.py<span class="w"> </span>--host<span class="w"> </span>http://localhost:4040<span class="w"> </span>--headless<span class="w"> </span>-u<span class="w"> </span><span class="m">50</span><span class="w"> </span>-t<span class="w"> </span>60s
</span></code></pre></div>
<p>You should hopefully see that the throughput of the model is higher when adaptive batching is enabled, but
the speedup is largely dependent on the model you are running, the configuration of the adaptive batching
and the hardware you are running on.</p>
<p>On my laptop I saw about a 1.5 - 2x speedup when adaptive batching was enabled.</p>
</details>
</li>
</ol>
</li>
<li>
<p>(Optional, requires GPU) Look through the
    <a href="https://docs.bentoml.org/en/latest/guides/gpu-inference.html">documentation for inference on GPU</a> and add this to
    your service. Check that your service works as expected by testing it with the client from the previous exercise and
    make sure you are seeing a speedup when running on the GPU.</p>
<details class="success">
<summary>Solution</summary>
<p>A simple change to the <code>bento.service</code> decorator is all that is needed to run the model on the GPU.</p>
<p>```python
@bentoml.service(resources={"gpu": 1})
class MyService:
    def <strong>init</strong>(self):
        self.model = torch.load('model.pth').to('cuda:0')</p>
</details>
</li>
<li>
<p>Another way to speed up the inference is to just use multiple workers. This duplicates the server over multiple
    processes taking advantage of modern multi-core CPUs. This is similar to running the <code>uvicorn</code> command with the
    <code>--workers</code> flag for FastAPI applications. Implement multiple workers in your service and test that it works as
    expected by testing it with the client from the previous exercise. Also test that you are seeing a speedup when
    running with multiple workers.</p>
<details class="success">
<summary>Solution</summary>
<p>Multiple workers can be added to the <code>bento.service</code> decorator as shown below.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span><span class="p">(</span><span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span><span id="__span-25-2"><a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a><span class="k">class</span><span class="w"> </span><span class="nc">MyService</span><span class="p">:</span>
</span><span id="__span-25-3"><a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>    <span class="c1"># Service implementation</span>
</span></code></pre></div>
<p>Alternatively, you can set <code>workers="cpu_count"</code> to use all available CPU cores. The speedup depends on the
model you are serving, the hardware you are running on and the number of workers you are using, but it should be
higher than using a single worker.</p>
</details>
</li>
<li>
<p>In addition to increasing the throughput of your deployments <code>BentoML</code> can also help with ML applications that
    require some kind of composition of multiple models. It is very normal in production setups to have multiple models
    that either</p>
<ul>
<li>Run in a sequence, e.g., the output of one model is the input of another model. You may have a preprocessing
    service that preprocesses the data before it is sent to a model that makes a prediction.</li>
<li>Run concurrently, e.g., you have multiple models that are run at the same time and the outputs of all the models
    are combined to make a prediction. Ensemble models are a good example of this.</li>
</ul>
<p><code>BentoML</code> makes it easy to
<a href="https://docs.bentoml.org/en/latest/guides/model-composition.html">compose multiple models together</a>.</p>
<ol>
<li>
<p>Implement two services that run in a sequence, e.g., the output of one service is used as the input to another
    service. As an example you can implement either some pre- or post-processing service that is used in conjunction
    with the model you have implemented in the previous exercises.</p>
<details class="success">
<summary>Solution</summary>
<p>The following code snippet shows how to implement two services that run in sequence.</p>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">bentoml_service_composition.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-26-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-26-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-26-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-26-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-26-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-26-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-26-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-26-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-26-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-26-10">10</a></span>
<span class="normal"><a href="#__codelineno-26-11">11</a></span>
<span class="normal"><a href="#__codelineno-26-12">12</a></span>
<span class="normal"><a href="#__codelineno-26-13">13</a></span>
<span class="normal"><a href="#__codelineno-26-14">14</a></span>
<span class="normal"><a href="#__codelineno-26-15">15</a></span>
<span class="normal"><a href="#__codelineno-26-16">16</a></span>
<span class="normal"><a href="#__codelineno-26-17">17</a></span>
<span class="normal"><a href="#__codelineno-26-18">18</a></span>
<span class="normal"><a href="#__codelineno-26-19">19</a></span>
<span class="normal"><a href="#__codelineno-26-20">20</a></span>
<span class="normal"><a href="#__codelineno-26-21">21</a></span>
<span class="normal"><a href="#__codelineno-26-22">22</a></span>
<span class="normal"><a href="#__codelineno-26-23">23</a></span>
<span class="normal"><a href="#__codelineno-26-24">24</a></span>
<span class="normal"><a href="#__codelineno-26-25">25</a></span>
<span class="normal"><a href="#__codelineno-26-26">26</a></span>
<span class="normal"><a href="#__codelineno-26-27">27</a></span>
<span class="normal"><a href="#__codelineno-26-28">28</a></span>
<span class="normal"><a href="#__codelineno-26-29">29</a></span>
<span class="normal"><a href="#__codelineno-26-30">30</a></span>
<span class="normal"><a href="#__codelineno-26-31">31</a></span>
<span class="normal"><a href="#__codelineno-26-32">32</a></span>
<span class="normal"><a href="#__codelineno-26-33">33</a></span>
<span class="normal"><a href="#__codelineno-26-34">34</a></span>
<span class="normal"><a href="#__codelineno-26-35">35</a></span>
<span class="normal"><a href="#__codelineno-26-36">36</a></span>
<span class="normal"><a href="#__codelineno-26-37">37</a></span>
<span class="normal"><a href="#__codelineno-26-38">38</a></span>
<span class="normal"><a href="#__codelineno-26-39">39</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>
</span><span id="__span-26-2"><a id="__codelineno-26-2" name="__codelineno-26-2"></a>
</span><span id="__span-26-3"><a id="__codelineno-26-3" name="__codelineno-26-3"></a><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
</span><span id="__span-26-4"><a id="__codelineno-26-4" name="__codelineno-26-4"></a>
</span><span id="__span-26-5"><a id="__codelineno-26-5" name="__codelineno-26-5"></a><span class="kn">import</span><span class="w"> </span><span class="nn">bentoml</span>
</span><span id="__span-26-6"><a id="__codelineno-26-6" name="__codelineno-26-6"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-26-7"><a id="__codelineno-26-7" name="__codelineno-26-7"></a><span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceSession</span>
</span><span id="__span-26-8"><a id="__codelineno-26-8" name="__codelineno-26-8"></a><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
</span><span id="__span-26-9"><a id="__codelineno-26-9" name="__codelineno-26-9"></a>
</span><span id="__span-26-10"><a id="__codelineno-26-10" name="__codelineno-26-10"></a>
</span><span id="__span-26-11"><a id="__codelineno-26-11" name="__codelineno-26-11"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span>
</span><span id="__span-26-12"><a id="__codelineno-26-12" name="__codelineno-26-12"></a><span class="k">class</span><span class="w"> </span><span class="nc">ImagePreprocessorService</span><span class="p">:</span>
</span><span id="__span-26-13"><a id="__codelineno-26-13" name="__codelineno-26-13"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Image preprocessor service.&quot;&quot;&quot;</span>
</span><span id="__span-26-14"><a id="__codelineno-26-14" name="__codelineno-26-14"></a>
</span><span id="__span-26-15"><a id="__codelineno-26-15" name="__codelineno-26-15"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span>
</span><span id="__span-26-16"><a id="__codelineno-26-16" name="__codelineno-26-16"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_file</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-26-17"><a id="__codelineno-26-17" name="__codelineno-26-17"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Preprocess the input image.&quot;&quot;&quot;</span>
</span><span id="__span-26-18"><a id="__codelineno-26-18" name="__codelineno-26-18"></a>        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image_file</span><span class="p">)</span>
</span><span id="__span-26-19"><a id="__codelineno-26-19" name="__codelineno-26-19"></a>        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
</span><span id="__span-26-20"><a id="__codelineno-26-20" name="__codelineno-26-20"></a>        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span><span id="__span-26-21"><a id="__codelineno-26-21" name="__codelineno-26-21"></a>        <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span><span id="__span-26-22"><a id="__codelineno-26-22" name="__codelineno-26-22"></a>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-26-23"><a id="__codelineno-26-23" name="__codelineno-26-23"></a>
</span><span id="__span-26-24"><a id="__codelineno-26-24" name="__codelineno-26-24"></a>
</span><span id="__span-26-25"><a id="__codelineno-26-25" name="__codelineno-26-25"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span>
</span><span id="__span-26-26"><a id="__codelineno-26-26" name="__codelineno-26-26"></a><span class="k">class</span><span class="w"> </span><span class="nc">ImageClassifierService</span><span class="p">:</span>
</span><span id="__span-26-27"><a id="__codelineno-26-27" name="__codelineno-26-27"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Image classifier service using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-26-28"><a id="__codelineno-26-28" name="__codelineno-26-28"></a>
</span><span id="__span-26-29"><a id="__codelineno-26-29" name="__codelineno-26-29"></a>    <span class="n">preprocessing_service</span> <span class="o">=</span> <span class="n">bentoml</span><span class="o">.</span><span class="n">depends</span><span class="p">(</span><span class="n">ImagePreprocessorService</span><span class="p">)</span>
</span><span id="__span-26-30"><a id="__codelineno-26-30" name="__codelineno-26-30"></a>
</span><span id="__span-26-31"><a id="__codelineno-26-31" name="__codelineno-26-31"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-26-32"><a id="__codelineno-26-32" name="__codelineno-26-32"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-26-33"><a id="__codelineno-26-33" name="__codelineno-26-33"></a>
</span><span id="__span-26-34"><a id="__codelineno-26-34" name="__codelineno-26-34"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span>
</span><span id="__span-26-35"><a id="__codelineno-26-35" name="__codelineno-26-35"></a>    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_file</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-26-36"><a id="__codelineno-26-36" name="__codelineno-26-36"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict the class of the input image.&quot;&quot;&quot;</span>
</span><span id="__span-26-37"><a id="__codelineno-26-37" name="__codelineno-26-37"></a>        <span class="n">image</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocessing_service</span><span class="o">.</span><span class="n">to_async</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">image_file</span><span class="p">)</span>
</span><span id="__span-26-38"><a id="__codelineno-26-38" name="__codelineno-26-38"></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)})</span>
</span><span id="__span-26-39"><a id="__codelineno-26-39" name="__codelineno-26-39"></a>        <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
<li>
<p>Implement three services, where two of them run concurrently and the outputs of both services are combined in the
    third service to make a prediction. As an example you can expand your previous service to serve two different
    models and then implement a third service that combines the outputs of both models to make a prediction.</p>
<details class="success">
<summary>Solution</summary>
<p>The following code snippet shows how to implement a service that consists of two concurrent services. The
example assumes that two models called <code>model_a.onnx</code> and <code>model_b.onnx</code> are available.</p>
<div class="language-python highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">bentoml_service_composition.py</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-27-1"> 1</a></span>
<span class="normal"><a href="#__codelineno-27-2"> 2</a></span>
<span class="normal"><a href="#__codelineno-27-3"> 3</a></span>
<span class="normal"><a href="#__codelineno-27-4"> 4</a></span>
<span class="normal"><a href="#__codelineno-27-5"> 5</a></span>
<span class="normal"><a href="#__codelineno-27-6"> 6</a></span>
<span class="normal"><a href="#__codelineno-27-7"> 7</a></span>
<span class="normal"><a href="#__codelineno-27-8"> 8</a></span>
<span class="normal"><a href="#__codelineno-27-9"> 9</a></span>
<span class="normal"><a href="#__codelineno-27-10">10</a></span>
<span class="normal"><a href="#__codelineno-27-11">11</a></span>
<span class="normal"><a href="#__codelineno-27-12">12</a></span>
<span class="normal"><a href="#__codelineno-27-13">13</a></span>
<span class="normal"><a href="#__codelineno-27-14">14</a></span>
<span class="normal"><a href="#__codelineno-27-15">15</a></span>
<span class="normal"><a href="#__codelineno-27-16">16</a></span>
<span class="normal"><a href="#__codelineno-27-17">17</a></span>
<span class="normal"><a href="#__codelineno-27-18">18</a></span>
<span class="normal"><a href="#__codelineno-27-19">19</a></span>
<span class="normal"><a href="#__codelineno-27-20">20</a></span>
<span class="normal"><a href="#__codelineno-27-21">21</a></span>
<span class="normal"><a href="#__codelineno-27-22">22</a></span>
<span class="normal"><a href="#__codelineno-27-23">23</a></span>
<span class="normal"><a href="#__codelineno-27-24">24</a></span>
<span class="normal"><a href="#__codelineno-27-25">25</a></span>
<span class="normal"><a href="#__codelineno-27-26">26</a></span>
<span class="normal"><a href="#__codelineno-27-27">27</a></span>
<span class="normal"><a href="#__codelineno-27-28">28</a></span>
<span class="normal"><a href="#__codelineno-27-29">29</a></span>
<span class="normal"><a href="#__codelineno-27-30">30</a></span>
<span class="normal"><a href="#__codelineno-27-31">31</a></span>
<span class="normal"><a href="#__codelineno-27-32">32</a></span>
<span class="normal"><a href="#__codelineno-27-33">33</a></span>
<span class="normal"><a href="#__codelineno-27-34">34</a></span>
<span class="normal"><a href="#__codelineno-27-35">35</a></span>
<span class="normal"><a href="#__codelineno-27-36">36</a></span>
<span class="normal"><a href="#__codelineno-27-37">37</a></span>
<span class="normal"><a href="#__codelineno-27-38">38</a></span>
<span class="normal"><a href="#__codelineno-27-39">39</a></span>
<span class="normal"><a href="#__codelineno-27-40">40</a></span>
<span class="normal"><a href="#__codelineno-27-41">41</a></span>
<span class="normal"><a href="#__codelineno-27-42">42</a></span>
<span class="normal"><a href="#__codelineno-27-43">43</a></span>
<span class="normal"><a href="#__codelineno-27-44">44</a></span>
<span class="normal"><a href="#__codelineno-27-45">45</a></span>
<span class="normal"><a href="#__codelineno-27-46">46</a></span>
<span class="normal"><a href="#__codelineno-27-47">47</a></span>
<span class="normal"><a href="#__codelineno-27-48">48</a></span>
<span class="normal"><a href="#__codelineno-27-49">49</a></span>
<span class="normal"><a href="#__codelineno-27-50">50</a></span>
<span class="normal"><a href="#__codelineno-27-51">51</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>
</span><span id="__span-27-2"><a id="__codelineno-27-2" name="__codelineno-27-2"></a>
</span><span id="__span-27-3"><a id="__codelineno-27-3" name="__codelineno-27-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
</span><span id="__span-27-4"><a id="__codelineno-27-4" name="__codelineno-27-4"></a>
</span><span id="__span-27-5"><a id="__codelineno-27-5" name="__codelineno-27-5"></a><span class="kn">import</span><span class="w"> </span><span class="nn">bentoml</span>
</span><span id="__span-27-6"><a id="__codelineno-27-6" name="__codelineno-27-6"></a><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</span><span id="__span-27-7"><a id="__codelineno-27-7" name="__codelineno-27-7"></a><span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceSession</span>
</span><span id="__span-27-8"><a id="__codelineno-27-8" name="__codelineno-27-8"></a>
</span><span id="__span-27-9"><a id="__codelineno-27-9" name="__codelineno-27-9"></a>
</span><span id="__span-27-10"><a id="__codelineno-27-10" name="__codelineno-27-10"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span>
</span><span id="__span-27-11"><a id="__codelineno-27-11" name="__codelineno-27-11"></a><span class="k">class</span><span class="w"> </span><span class="nc">ImageClassifierServiceModelA</span><span class="p">:</span>
</span><span id="__span-27-12"><a id="__codelineno-27-12" name="__codelineno-27-12"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Image classifier service using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-27-13"><a id="__codelineno-27-13" name="__codelineno-27-13"></a>
</span><span id="__span-27-14"><a id="__codelineno-27-14" name="__codelineno-27-14"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-27-15"><a id="__codelineno-27-15" name="__codelineno-27-15"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model_a.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-27-16"><a id="__codelineno-27-16" name="__codelineno-27-16"></a>
</span><span id="__span-27-17"><a id="__codelineno-27-17" name="__codelineno-27-17"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span>
</span><span id="__span-27-18"><a id="__codelineno-27-18" name="__codelineno-27-18"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-27-19"><a id="__codelineno-27-19" name="__codelineno-27-19"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict the class of the input image.&quot;&quot;&quot;</span>
</span><span id="__span-27-20"><a id="__codelineno-27-20" name="__codelineno-27-20"></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)})</span>
</span><span id="__span-27-21"><a id="__codelineno-27-21" name="__codelineno-27-21"></a>        <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-27-22"><a id="__codelineno-27-22" name="__codelineno-27-22"></a>
</span><span id="__span-27-23"><a id="__codelineno-27-23" name="__codelineno-27-23"></a>
</span><span id="__span-27-24"><a id="__codelineno-27-24" name="__codelineno-27-24"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span>
</span><span id="__span-27-25"><a id="__codelineno-27-25" name="__codelineno-27-25"></a><span class="k">class</span><span class="w"> </span><span class="nc">ImageClassifierServiceModelB</span><span class="p">:</span>
</span><span id="__span-27-26"><a id="__codelineno-27-26" name="__codelineno-27-26"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Image classifier service using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-27-27"><a id="__codelineno-27-27" name="__codelineno-27-27"></a>
</span><span id="__span-27-28"><a id="__codelineno-27-28" name="__codelineno-27-28"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="__span-27-29"><a id="__codelineno-27-29" name="__codelineno-27-29"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;model_b.onnx&quot;</span><span class="p">)</span>
</span><span id="__span-27-30"><a id="__codelineno-27-30" name="__codelineno-27-30"></a>
</span><span id="__span-27-31"><a id="__codelineno-27-31" name="__codelineno-27-31"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span>
</span><span id="__span-27-32"><a id="__codelineno-27-32" name="__codelineno-27-32"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-27-33"><a id="__codelineno-27-33" name="__codelineno-27-33"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict the class of the input image.&quot;&quot;&quot;</span>
</span><span id="__span-27-34"><a id="__codelineno-27-34" name="__codelineno-27-34"></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)})</span>
</span><span id="__span-27-35"><a id="__codelineno-27-35" name="__codelineno-27-35"></a>        <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-27-36"><a id="__codelineno-27-36" name="__codelineno-27-36"></a>
</span><span id="__span-27-37"><a id="__codelineno-27-37" name="__codelineno-27-37"></a>
</span><span id="__span-27-38"><a id="__codelineno-27-38" name="__codelineno-27-38"></a><span class="nd">@bentoml</span><span class="o">.</span><span class="n">service</span>
</span><span id="__span-27-39"><a id="__codelineno-27-39" name="__codelineno-27-39"></a><span class="k">class</span><span class="w"> </span><span class="nc">ImageClassifierService</span><span class="p">:</span>
</span><span id="__span-27-40"><a id="__codelineno-27-40" name="__codelineno-27-40"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Image classifier service using ONNX model.&quot;&quot;&quot;</span>
</span><span id="__span-27-41"><a id="__codelineno-27-41" name="__codelineno-27-41"></a>
</span><span id="__span-27-42"><a id="__codelineno-27-42" name="__codelineno-27-42"></a>    <span class="n">model_a</span> <span class="o">=</span> <span class="n">bentoml</span><span class="o">.</span><span class="n">depends</span><span class="p">(</span><span class="n">ImageClassifierServiceModelA</span><span class="p">)</span>
</span><span id="__span-27-43"><a id="__codelineno-27-43" name="__codelineno-27-43"></a>    <span class="n">model_b</span> <span class="o">=</span> <span class="n">bentoml</span><span class="o">.</span><span class="n">depends</span><span class="p">(</span><span class="n">ImageClassifierServiceModelB</span><span class="p">)</span>
</span><span id="__span-27-44"><a id="__codelineno-27-44" name="__codelineno-27-44"></a>
</span><span id="__span-27-45"><a id="__codelineno-27-45" name="__codelineno-27-45"></a>    <span class="nd">@bentoml</span><span class="o">.</span><span class="n">api</span>
</span><span id="__span-27-46"><a id="__codelineno-27-46" name="__codelineno-27-46"></a>    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</span><span id="__span-27-47"><a id="__codelineno-27-47" name="__codelineno-27-47"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Predict the class of the input image.&quot;&quot;&quot;</span>
</span><span id="__span-27-48"><a id="__codelineno-27-48" name="__codelineno-27-48"></a>        <span class="n">result_a</span><span class="p">,</span> <span class="n">result_b</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
</span><span id="__span-27-49"><a id="__codelineno-27-49" name="__codelineno-27-49"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">model_a</span><span class="o">.</span><span class="n">to_async</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_b</span><span class="o">.</span><span class="n">to_async</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span><span id="__span-27-50"><a id="__codelineno-27-50" name="__codelineno-27-50"></a>        <span class="p">)</span>
</span><span id="__span-27-51"><a id="__codelineno-27-51" name="__codelineno-27-51"></a>        <span class="k">return</span> <span class="p">(</span><span class="n">result_a</span> <span class="o">+</span> <span class="n">result_b</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</span></code></pre></div></td></tr></table></div>
</details>
</li>
<li>
<p>(Optional) Implement a server that consists of both sequential and concurrent services.</p>
</li>
</ol>
</li>
<li>
<p>Similar to deploying a FastAPI application to the cloud, deploying a <code>BentoML</code> framework to the cloud
    often requires you to first containerize the application. Because <code>BentoML</code> is designed to be easy to use for even
    users not that familiar with Docker, it introduces the concept of a <code>bentofile</code>. A <code>bentofile</code> is a file that
    specifies how the container should be built. Below is an example of how a <code>bentofile</code> could look.</p>
<div class="language-yaml highlight"><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="nt">service</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;service:Summarization&#39;</span>
</span><span id="__span-28-2"><a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a><span class="nt">labels</span><span class="p">:</span>
</span><span id="__span-28-3"><a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a><span class="w">  </span><span class="nt">owner</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bentoml-team</span>
</span><span id="__span-28-4"><a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a><span class="w">  </span><span class="nt">project</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gallery</span>
</span><span id="__span-28-5"><a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a><span class="nt">include</span><span class="p">:</span>
</span><span id="__span-28-6"><a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;*.py&#39;</span>
</span><span id="__span-28-7"><a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a><span class="nt">python</span><span class="p">:</span>
</span><span id="__span-28-8"><a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a><span class="w">  </span><span class="nt">packages</span><span class="p">:</span>
</span><span id="__span-28-9"><a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch</span>
</span><span id="__span-28-10"><a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">transformers</span>
</span></code></pre></div>
<p>This can then be used to build a <code>bento</code> using the following command:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>bentoml<span class="w"> </span>build
</span></code></pre></div>
<p>A <code>bento</code> is not a docker image, but it can be used to build a docker image with the following command:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-30-1"><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a>bentoml<span class="w"> </span>containerize<span class="w"> </span>summarization:latest
</span></code></pre></div>
<ol>
<li>
<p>Can you figure out how the different parts of the <code>bentofile</code> are used to build the docker image? Additionally,
    can you figure out from the <a href="https://github.com/bentoml/BentoML">source repository</a> how the <code>bentofile</code> is
    used to build the docker image?</p>
<details class="success">
<summary>Solution</summary>
<p>The <code>service</code> part specifies both what the container should be called and also what service it should
serve, e.g., the last statement in the corresponding dockerfile is
<code>CMD ["bentoml", "serve", "service:Summarization"]</code>. The <code>labels</code> part is used to specify labels about the
container, see this <a href="https://docs.docker.com/reference/dockerfile/#label">link</a> for more info. The <code>include</code>
part corresponds to <code>COPY</code> statements in the dockerfile and finally the <code>python</code> part is used to specify
what python packages should be installed in the container which corresponds to <code>RUN pip install ...</code> in the
dockerfile.</p>
<p>Regarding how the <code>bentofile</code> is used to build the docker image, the <code>bentoml</code> package contains a number
of templates (written using the <a href="https://jinja.palletsprojects.com/en/stable/">jinja2</a> templating language)
that are used to generate the dockerfiles. The templates can be found
<a href="https://github.com/bentoml/BentoML/tree/main/src/bentoml/_internal/container/frontend/dockerfile">here</a>.</p>
</details>
</li>
<li>
<p>Take any service from the previous exercises and try to containerize it. You are free to either write a
    <code>bentofile</code> or a <code>dockerfile</code> to do this.</p>
<details class="success">
<summary>Solution</summary>
<p>The following <code>bentofile</code> can be used to containerize the very first service we implemented in this set of
exercises.</p>
<div class="language-yaml highlight"><pre><span></span><code><span id="__span-31-1"><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a><span class="nt">service</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;bentoml_service:ImageClassifierService&#39;</span>
</span><span id="__span-31-2"><a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a><span class="nt">labels</span><span class="p">:</span>
</span><span id="__span-31-3"><a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a><span class="w">  </span><span class="nt">owner</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">bentoml-team</span>
</span><span id="__span-31-4"><a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a><span class="w">  </span><span class="nt">project</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gallery</span>
</span><span id="__span-31-5"><a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a><span class="nt">include</span><span class="p">:</span>
</span><span id="__span-31-6"><a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;bentoml_service.py&#39;</span>
</span><span id="__span-31-7"><a id="__codelineno-31-7" name="__codelineno-31-7" href="#__codelineno-31-7"></a><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&#39;model.onnx&#39;</span>
</span><span id="__span-31-8"><a id="__codelineno-31-8" name="__codelineno-31-8" href="#__codelineno-31-8"></a><span class="nt">python</span><span class="p">:</span>
</span><span id="__span-31-9"><a id="__codelineno-31-9" name="__codelineno-31-9" href="#__codelineno-31-9"></a><span class="w">  </span><span class="nt">packages</span><span class="p">:</span>
</span><span id="__span-31-10"><a id="__codelineno-31-10" name="__codelineno-31-10" href="#__codelineno-31-10"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">onnxruntime</span>
</span><span id="__span-31-11"><a id="__codelineno-31-11" name="__codelineno-31-11" href="#__codelineno-31-11"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">numpy</span>
</span></code></pre></div>
<p>The corresponding dockerfile would look something like this:</p>
<div class="language-dockerfile highlight"><pre><span></span><code><span id="__span-32-1"><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.11-slim</span>
</span><span id="__span-32-2"><a id="__codelineno-32-2" name="__codelineno-32-2" href="#__codelineno-32-2"></a><span class="k">WORKDIR</span><span class="w"> </span><span class="s">/bento</span>
</span><span id="__span-32-3"><a id="__codelineno-32-3" name="__codelineno-32-3" href="#__codelineno-32-3"></a><span class="k">COPY</span><span class="w"> </span>bentoml_service.py<span class="w"> </span>.
</span><span id="__span-32-4"><a id="__codelineno-32-4" name="__codelineno-32-4" href="#__codelineno-32-4"></a><span class="k">COPY</span><span class="w"> </span>model.onnx<span class="w"> </span>.
</span><span id="__span-32-5"><a id="__codelineno-32-5" name="__codelineno-32-5" href="#__codelineno-32-5"></a><span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>onnxruntime<span class="w"> </span>numpy<span class="w"> </span>bentoml
</span><span id="__span-32-6"><a id="__codelineno-32-6" name="__codelineno-32-6" href="#__codelineno-32-6"></a><span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;bentoml&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;serve&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;bentoml_service:ImageClassifierService&quot;</span><span class="p">]</span>
</span></code></pre></div>
</details>
</li>
<li>
<p>Deploy the container to GCP Run and test that it works.</p>
<details class="success">
<summary>Solution</summary>
<p>The following command can be used to deploy the container to GCP Run. We assume that you have already built
the container and called it <code>bentoml_service:latest</code></p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-33-1"><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a>docker<span class="w"> </span>tag<span class="w"> </span>bentoml_service:latest<span class="w"> </span><span class="se">\</span>
</span><span id="__span-33-2"><a id="__codelineno-33-2" name="__codelineno-33-2" href="#__codelineno-33-2"></a><span class="w">    </span>&lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository-name&gt;/bentoml_service:latest
</span><span id="__span-33-3"><a id="__codelineno-33-3" name="__codelineno-33-3" href="#__codelineno-33-3"></a>docker<span class="w"> </span>push<span class="w"> </span>&lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository-name&gt;/bentoml_service:latest
</span><span id="__span-33-4"><a id="__codelineno-33-4" name="__codelineno-33-4" href="#__codelineno-33-4"></a>gcloud<span class="w"> </span>run<span class="w"> </span>deploy<span class="w"> </span>bentoml-service<span class="w"> </span><span class="se">\</span>
</span><span id="__span-33-5"><a id="__codelineno-33-5" name="__codelineno-33-5" href="#__codelineno-33-5"></a><span class="w">    </span>--image<span class="o">=</span>&lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository-name&gt;/bentoml_service:latest<span class="w"> </span><span class="se">\</span>
</span><span id="__span-33-6"><a id="__codelineno-33-6" name="__codelineno-33-6" href="#__codelineno-33-6"></a><span class="w">    </span>--platform<span class="w"> </span>managed<span class="w"> </span><span class="se">\</span>
</span><span id="__span-33-7"><a id="__codelineno-33-7" name="__codelineno-33-7" href="#__codelineno-33-7"></a><span class="w">    </span>--port<span class="w"> </span><span class="m">3000</span><span class="w">  </span><span class="c1"># default used by BentoML</span>
</span></code></pre></div>
<p>where <code>&lt;project-id&gt;</code> should be replaced with the id of the project you are deploying to. The service should
now be available at the URL that is printed in the terminal.</p>
</details>
</li>
</ol>
</li>
</ol>
<p>This completes the exercises on the <code>BentoML</code> framework. If you want to deep dive more into this we recommend
looking into their <a href="https://docs.bentoml.org/en/latest/guides/tasks.html">tasks feature</a> for use cases that have a
very long running time and built-in
<a href="https://docs.bentoml.org/en/latest/guides/model-loading-and-management.html">model management feature</a> to unify the
way models are loaded, managed and served.</p>
<h2 id="knowledge-check">🧠 Knowledge check</h2>
<ol>
<li>
<p>How would you export a <code>scikit-learn</code> model to ONNX? What method is exported when you export a <code>scikit-learn</code> model to
    ONNX?</p>
<details class="success">
<summary>Solution</summary>
<p>It is possible to export a <code>scikit-learn</code> model to ONNX using the <code>sklearn-onnx</code> package. The following code
snippet shows how to export a <code>scikit-learn</code> model to ONNX.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-34-1"><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</span><span id="__span-34-2"><a id="__codelineno-34-2" name="__codelineno-34-2" href="#__codelineno-34-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">skl2onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">to_onnx</span>
</span><span id="__span-34-3"><a id="__codelineno-34-3" name="__codelineno-34-3" href="#__codelineno-34-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span><span id="__span-34-4"><a id="__codelineno-34-4" name="__codelineno-34-4" href="#__codelineno-34-4"></a><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span><span id="__span-34-5"><a id="__codelineno-34-5" name="__codelineno-34-5" href="#__codelineno-34-5"></a><span class="n">onx</span> <span class="o">=</span> <span class="n">to_onnx</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">)</span>
</span><span id="__span-34-6"><a id="__codelineno-34-6" name="__codelineno-34-6" href="#__codelineno-34-6"></a><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span><span id="__span-34-7"><a id="__codelineno-34-7" name="__codelineno-34-7" href="#__codelineno-34-7"></a>    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">onx</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
</span></code></pre></div>
<p>The method that is exported when you export a <code>scikit-learn</code> model to ONNX is the <code>predict</code> method.</p>
</details>
</li>
<li>
<p>In your own words, describe what the concept of <em>computational graph</em> means.</p>
<details class="success">
<summary>Solution</summary>
<p>A computational graph is a way to represent the mathematical operations that are performed in a model. It is
essentially a graph where the nodes are the operations and the edges are the data that is passed between them.
The computational graph normally represents the forward pass of the model and is the reason that we can easily
backpropagate through the model to train it, because the graph contains all the necessary information to
calculate the gradients of the model.</p>
</details>
</li>
<li>
<p>In your own words, explain why fusing operations together in the computational graph often leads to better
    performance.</p>
<details class="success">
<summary>Solution</summary>
<p>Each time we want to do a computation, the data needs to be loaded from memory into the CPU/GPU. This is a
slow process and the more operations we have, the more times we need to load the data. By fusing operations
together, we can reduce the number of times we need to load the data, because we can do multiple operations
on the same data before we need to load new data.</p>
</details>
</li>
</ol>
<p>This ends the module on tools specifically designed for serving machine learning models. As stated in the beginning of
the module, there are a lot of different tools that can be used to serve machine learning models and the choice of tool
often depends on the specific use case. In general, we recommend that whenever you want to serve a machine learning
model, you try out a few different frameworks and see which one fits your use case best.</p>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="February 6, 2025 10:25:44">February 6, 2025</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="November 9, 2024 16:44:03">November 9, 2024</span>
  </span>

    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../testing_apis/" class="md-footer__link md-footer__link--prev" aria-label="Previous: M24 - API Testing">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                M24 - API Testing
              </div>
            </div>
          </a>
        
        
          
          <a href="../frontend/" class="md-footer__link md-footer__link--next" aria-label="Next: M26 - Frontend">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                M26 - Frontend
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021 - 2024 Nicki Skafte Detlefsen
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:skaftenicki@gmail.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/SkafteNicki" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/nicki-skafte-detlefsen/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.indexes", "navigation.footer", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../assets/_markdown_exec_pyodide.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>