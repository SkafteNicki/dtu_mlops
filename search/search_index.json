{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Machine Learning Operations <p>Repository for course 02476 at DTU.</p> <p>Checkout the homepage!</p> </p> <p> </p>"},{"location":"#i-course-information","title":"\u2139\ufe0f Course information","text":"<ul> <li>Course responsible<ul> <li>Postdoc Nicki Skafte Detlefsen, nsde@dtu.dk</li> <li>Professor S\u00f8ren Hauberg, sohau@dtu.dk</li> </ul> </li> <li>5 ECTS (European Credit Transfer System), corresponding to 140 hours of work</li> <li>3 week period in January</li> <li>Master level course</li> <li>Grade: Pass/not passed</li> <li>Type of assessment: project report</li> <li> <p>Recommended prerequisites: DTU course 02456 (Deep Learning) or     experience with the following topics:</p> <ul> <li>General understanding of machine learning (datasets, probability, classifiers, overfitting, underfitting, etc.)</li> <li>Basic knowledge of deep learning (backpropagation, convolutional neural networks, auto-encoders etc.)</li> <li>Coding in PyTorch. On the first day, we provide some exercises in PyTorch to     get everyone's skills up-to-date as fast as possible.</li> </ul> </li> </ul>"},{"location":"#course-setup","title":"\ud83d\udcbb Course setup :)","text":"<p>Start by cloning or downloading this repository</p> <pre><code>git clone https://github.com/SkafteNicki/dtu_mlops\n</code></pre> <p>If you do not have git installed (yet) we will touch upon it in the course. The folder will contain all the exercise material for this course and lectures. Additionally, you should join our Slack channel which we use for communication. The link may be expired, write to me.</p>"},{"location":"#course-organization","title":"\ud83d\udcc2 Course organization","text":"<p>We highly recommend that when going through the material you use the homepage which is the corresponding GitHub Pages version of this repository that is more nicely rendered, and also includes some special HTML magic provided by Material for MkDocs.</p> <p>The course is divided into sessions, denoted by capital S, and modules, denoted by capital M. A session corresponds to a full day of work if you are following the course, meaning approximately 9 hours of work. Each session (S) corresponds to a topic within MLOps and consists of multiple modules (M) that each cover a specific topic.</p> <p>Importantly we differ between core modules and optional modules. Core modules will be marked by</p> <p>Core Module</p> <p>at the top of their corresponding page. Core modules are important to go through to be able to pass the course. You are highly recommended to still do the optional modules.</p> <p>Additionally, be aware of the following icons throughout the course material:</p> <ul> <li> <p>This icon can be expanded to show code belonging to a given exercise</p> Example <p>I will contain some code for an exercise.</p> </li> <li> <p>This icon can be expanded to show a solution for a given exercise</p> Solution <p>I will present a solution to the exercise.</p> </li> <li> <p>This icon (1) can be expanded to show a hint or a note for a given exercise</p> <ol> <li> I am a hint or note</li> </ol> </li> </ul>"},{"location":"#mlops-what-is-it","title":"\ud83c\udd92 MLOps: What is it?","text":"<p>Machine Learning Operations (MLOps) is a rather new field that has seen its uprise as machine learning and particularly deep learning has become a widely available technology. The term itself is a compound of \"machine learning\" and \"operations\" and covers everything that has to do with the management of the production ML lifecycle.</p> <p>The lifecycle of production ML can largely be divided into three phases:</p> <ol> <li> <p>Design: The initial phase starts with an investigation of the problem. Based on this analysis, several requirements     can be prioritized for what we want our future model to do. Since machine learning requires     data to be trained, we also investigate in this step what data we have and if we need to source it in some other way.</p> </li> <li> <p>Model development: Based on the design phase we can begin to conjure some machine learning algorithms to solve our     problems. As always, the initial step often involves doing some data analysis to make sure that our model is     learning the signal that we want it to learn. Secondly, is the machine learning engineering phase, where the     particular model architecture is chosen. Finally, we also need to do validation and testing to make sure that     our model is generalizing well.</p> </li> <li> <p>Operations: Based on the model development phase, we now have a model that we want to use. The operations are where     we create an automatic pipeline that makes sure that whenever we make changes to our codebase they get automatically     incorporated into our model, such that we do not slow down production. Equally important is the ongoing monitoring     of already deployed models to make sure that they behave exactly as we specified them.</p> </li> </ol> <p>It is important to note that the three steps are a cycle, meaning that when you have successfully deployed a machine learning model that is not the end of it. Your initial requirements may change, forcing you to revisit the design phase. Some new algorithms may show promising results, so you revisit the model development phase to implement this. Finally, you may try to cut the cost of running your model in production, making you revisit the operations phase, and trying to optimize some steps.</p> <p>The focus in this course is particularly on the Operations part of MLOps as this is what many data scientists are missing in their toolbox to implement all the knowledge they have about data processing and model development into a production setting.</p>"},{"location":"#learning-objectives","title":"\u2754 Learning objectives","text":"<p>General course objective</p> <p>Introduce the student to a number of coding practices that will help them organize, scale, monitor and deploy machine learning models either in a research or production setting. To provide hands-on experience with a number of frameworks, both local and in the cloud, for doing large scale machine learning models.</p> <p>This includes:</p> <ul> <li>Organize code in an efficient way for easy maintainability and shareability</li> <li>Understand the importance of reproducibility and how to create reproducible containerized applications and experiments</li> <li>Capable of using version control to efficiently collaborate on code development</li> <li>Knowledge of continuous integration (CI) and continuous machine learning (CML) for automating code development</li> <li>Being able to debug, profile, visualize and monitor multiple experiments to assess model performance</li> <li>Capable of using online cloud-based computing services to scale experiments</li> <li>Demonstrate knowledge about different distributed training paradigms within  machine learning and how to apply them</li> <li>Deploy machine learning models, both locally and in the cloud</li> <li>Conduct a research project in collaboration with fellow students using the frameworks taught in the course</li> <li>Have lots of fun and share memes! :)</li> </ul>"},{"location":"#references","title":"\ud83d\udcd3 References","text":"<p>Additional reading resources (in no particular order):</p> <ul> <li> <p>Ref 1     Introduction blog post for those who have never heard about MLOps and want to get an overview.</p> </li> <li> <p>Ref 2     Great document from Google about the different levels of MLOps.</p> </li> <li> <p>Ref 3     Another introduction to the principles of MLOps and the different stages of MLOps.</p> </li> <li> <p>Ref 4     Great paper about the technical debt in machine learning.</p> </li> <li> <p>Ref 5     Interview study that uncovers many of the pain points that ML engineers go through when doing MLOps.</p> </li> </ul> <p>Other courses with content similar to this:</p> <ul> <li> <p>Made with ML. Great online MLOps course that also covers additional topics on the     foundations of working with ML.</p> </li> <li> <p>Full stack deep learning. Another MLOps online course going through the whole     developer pipeline.</p> </li> <li> <p>MLOps Zoomcamp. MLOps online course that includes many of the same     topics.</p> </li> </ul>"},{"location":"#contributing","title":"\ud83d\udc68\u200d\ud83c\udfeb Contributing","text":"<p>If you want to contribute to the course, we are happy to have you! Anything from fixing typos to adding new content is welcome. For building the course material locally, it is a simple two-step process:</p> <p>Either use <code>pip</code> to install the requirements:</p> <pre><code>pip install -r requirements.txt\nmkdocs serve\n</code></pre> <p>Or use <code>uv</code></p> <pre><code>uv sync\nuv run mkdocs serve\n</code></pre> <p>Which will start a local server that you can access at <code>http://127.0.0.1:8000</code> and will automatically update when you make changes to the course material. When you have something that you want to contribute, please make a pull request.</p>"},{"location":"#license","title":"\u2755 License","text":"<p>I highly value open source, and the content of this course is therefore free to use under the Apache 2.0 license. If you use parts of this course in your work, please cite using:</p> <pre><code>@misc{skafte_mlops,\n    author       = {Nicki Skafte Detlefsen},\n    title        = {Machine Learning Operations},\n    howpublished = {\\url{https://github.com/SkafteNicki/dtu_mlops}},\n    year         = {2024}\n}\n</code></pre>"},{"location":"pages/faq/","title":"Frequently asked questions","text":"<p>For further questions, please contact Nicki.</p>"},{"location":"pages/faq/#when-is-the-next-time-the-course-is-running","title":"When is the next time the course is running \u2754","text":"<p>The course always runs in January, during the 3-week period at DTU. The exact dates can be found in the academic calendar.</p>"},{"location":"pages/faq/#is-it-possible-to-attend-the-course-fully-online","title":"Is it possible to attend the course fully online \u2754","text":"<p>Mostly yes. All exercises are provided online and lectures will be recorded and streamed. However, do note that</p> <ul> <li>For project days (see which days in the time plan) you will need to agree with your project group that     you are working from home.</li> <li>We have limited TA resources and will be prioritizing students coming to campus for help. If you are attending online,     feel free to ask questions on our Slack channel, and we will help to the best of our ability.</li> </ul> <p>Overall we try to support flexible learning as much as possible with some limitations.</p>"},{"location":"pages/faq/#what-are-the-prerequisites-for-taking-this-course","title":"What are the prerequisites for taking this course \u2754","text":"<p>We recommend that you have a basic understanding of machine learning concepts such as what a dataset is, what probabilities are, what a classifier is, what overfitting means etc. This corresponds to the curriculum covered in course 02450. The actual focus of the course is not on machine learning models, but we will be using these basic concepts throughout the exercises.</p> <p>Additionally, we recommend basic knowledge about deep learning and how to code in PyTorch, corresponding to the curriculum covered in 02456. From prior experience, we know that not all students have gained knowledge about deep learning models before this course, and we will be covering the basics of how to code in PyTorch in one of the first modules of the course to get everyone up to speed.</p>"},{"location":"pages/faq/#i-will-be-missing-x-days-of-the-course-will-that-be-a-problem","title":"I will be missing X days of the course, will that be a problem \u2754","text":"<p>Depends. The course is fairly intensive, with most students working from 9-17 every day. If you already know that you will be missing X days of the course, then I highly recommend that you go through some of the first sessions before the course starts to give yourself a bit of breathing room. If you are not able to do so, please be aware that an additional effort may be needed from you to keep up with your fellow students.</p>"},{"location":"pages/faq/#how-many-should-we-be-in-a-group-for-the-projects","title":"How many should we be in a group for the projects \u2754","text":"<p>Between 3 and 5. The projects are designed to be done in groups meaning that we intentionally make them too big for one person to do alone. Luckily, a lot of the work that you need to do can be done in parallel, so it is not as bad as it sounds.</p>"},{"location":"pages/faq/#when-will-the-exam-take-place","title":"When will the exam take place \u2754","text":"<p>From 2025 and onwards, the exam only consists of a project report. The report should be handed in at midnight on the final day of the course. For January 2025, this means the 24th.</p>"},{"location":"pages/faq/#where-can-i-find-information-regarding-the-exam","title":"Where can I find information regarding the exam \u2754","text":"<p>Look at the bottom of this page. Details will be updated as we get closer to the exam date.</p>"},{"location":"pages/faq/#can-i-use-chatgpt-or-similar-tools-for-the-exercises-project-exam-report-coding-writing","title":"Can I use ChatGPT or similar tools for the exercises, project, exam report (coding + writing) \u2754","text":"<p>Yes, yes, and yes, but remember that it's a tool, and you need to validate the output before using it. We would prefer for the exam report that you formulate the answers in your own words because it is intended for you do describe what you have been doing in your project. The I in LLM stands for intelligence.</p>"},{"location":"pages/faq/#i-am-a-phd-student-not-enrolled-at-dtu-can-i-take-the-course","title":"I am a PhD student not enrolled at DTU, can I take the course \u2754","text":"<p>Yes, PhD students from other universities can attend the course. You can check out this page for more information or in general you can contact phdcourses@dtu.dk for more information. Do note that the registration deadline is usually in beginning of December.</p>"},{"location":"pages/faq/#i-am-a-foreign-student-and-my-home-university-doesnt-accept-passnot-pass-what-can-i-do","title":"I am a foreign student and my home university doesn't accept pass/not pass, what can I do \u2754","text":"<p>We can give a grade on the Danish 7-point grading scale for foreign students who need it, where their home university does not accept pass/no-pass. You need to contact the course responsible Nicki within the first week of the course to request this. Secondly, we may need to further validate your work, so please be prepared for doing a short oral exam on one of the last days of the course.</p>"},{"location":"pages/faq/#i-am-a-euroteq-student-any-special-rules-for-me","title":"I am a EuroTeQ student, any special rules for me \u2754","text":"<p>Not really, you will attend the course as any other student. However, we will provide a special Slack channel for you, trying to make sure that you can get the same help as students from DTU who can attend the course on campus.</p>"},{"location":"pages/overview/","title":"Summary of course content","text":"<p>There are a lot of moving parts in this course, so it may be hard to understand how it all fits together. This page provides a summary of the frameworks in this course e.g. the stack of tools used. In the figure below we have provided an overview on how the different tools of the course interacts with each other. The table after the figure provides a short description of each of the parts.</p> <p></p>  The MLOps stack in the course. This is just an example of one stack, and depending on your use case you may want to use a different stack of tools that better fits your needs. Regardless of the stack, the principles of MLOps are the same.  Framework Description PyTorch is the backbone of our code, it provides the computational engine and the data structures that we need to define our data structures. PyTorch lightning is a framework that provides a high-level interface to PyTorch. It provides a lot of functionality that we need to train our models, such as logging, checkpointing, early stopping, etc. such that we do not have to implement it ourselves. It also allows us to scale our models to multiple GPUs and multiple nodes. We control the dependencies and Python interpreter using Conda that enables us to construct reproducible virtual environments For configuring our experiments we use Hydra that allows us to define a hierarchical configuration structure config files For creating command line interfaces we can use Typer that provides a high-level interface for creating CLIs Using Weights and Bias allows us to track and log any values and hyperparameters for our experiments Whenever we run into performance bottlenecks with our code we can use the Profiler to find the cause of the bottleneck When we run into bugs in our code we can use the Debugger to find the cause of the bug For organizing our code and creating templates we can use Cookiecutter Docker is a tool that allows us to create a container that contains all the dependencies and code that we need to run our code For controlling the versions of our data and synchronization between local and remote data storage, we can use DVC that makes this process easy For version control of our code we use Git (in complement with GitHub) that allows multiple developers to work together on a shared codebase We can use Pytest to write unit tests for our code, to make sure that new changes to the code does break the code base For linting our code and keeping a consistent coding style we can use tools such as Pylint and Flake8 that checks our code for common mistakes and style issues For running our unit tests and other checks on our code in a continuous manner e.g. after we commit and push our code we can use GitHub actions that automate this process Using Cloud build we can automate the process of building our docker images and pushing them to our artifact registry Artifact registry is a service that allows us to store our docker images for later use by other services For storing our data and trained models we can use Cloud storage that provides a scalable and secure storage solution For general compute tasks we can use Compute engine that provides a scalable and secure compute solution For training our experiments in a easy and scalable manner we can use Vertex AI For creating a REST API for our model we can use FastAPI that provides a high-level interface for creating APIs For converting our PyTorch model to a format that can be used in production we can use ONNX For creating a frontend for our model we can use Streamlit that provides a high-level interface for creating web applications For simple deployments of our code we can use Cloud functions that allows us to run our code in response to events through simple Python functions For more complex deployments of our code we can use Cloud run that allows us to run our code in response to events through docker containers For load testing our deployed model we can use Locust Cloud monitoring gives us the tools to keep track of important logs and errors from the other cloud services For monitoring our deployed model is experiencing any drift we can use Evidently AI that provides a framework and dashboard for monitoring drift For monitoring the telemetry of our deployed model we can use OpenTelemetry that provides a standard for collecting and exporting telemetry data"},{"location":"pages/projects/","title":"Project work","text":"<p>Slides</p> <p>Project learderboard</p> <p>Approximately 1/3 of the course time is dedicated to doing project work. The projects will serve as the basis of your exam. In the project, you will essentially re-apply everything that you learn throughout the course to a self chosen project. The overall goals with the project are:</p> <ul> <li>Being able to work in a group on a larger project</li> <li>To formulate a project within the provided guidelines</li> <li>Apply the material taught in the course to the problem</li> <li>Present your findings</li> </ul> <p>In the projects you are free to work on whatever problem that you want. If you want inspiration for projects, here are some examples</p> <ol> <li> <p>Classification of tweets</p> </li> <li> <p>Translating from English to German</p> </li> <li> <p>Classification of scientific papers</p> </li> <li> <p>Classification of rice types from images</p> </li> </ol> <p>We hope most students will be able to form groups by themselves. Expected group size is between 3 and 5. If you are not able to form a group, please make sure to post in the <code>#looking-for-group</code> channel on Slack or make sure to be present on the 4th day of the course (the day before the project work starts) where we will help students that have not found a group yet.</p>"},{"location":"pages/projects/#open-source-tools","title":"Open-source tools","text":"<p>We strive to keep the tools taught in this course as open-source as possible. The great thing about the open-source community is that whatever problem you are working on, there is probably some package out there that can get you at least 10% of the way. For the project, we want to enforce this point, and you are required to include some third-party package, that is neither PyTorch nor one of the tools already covered in the course, into your project.</p> <p>If you have no idea what framework to include, the PyTorch ecosystem is a great place for finding open-source frameworks that can help you accelerate your own projects where PyTorch is the back engine. All tools in the ecosystem should work greatly together with PyTorch. However, it is important to note that the ecosystem is not a complete list of all the awesome packages that exist to extend the functionality of PyTorch. If you are still missing inspiration for frameworks to use, we highly recommend these three that have been used in previous years of the course:</p> <ul> <li> <p>PyTorch Image Models. PyTorch Image Models (also known as TIMM)     is the absolutely most used computer vision package (maybe except for <code>torchvision</code>). It contains models, scripts     and pre-trained for a lot of state-of-the-art image models within computer vision.</p> </li> <li> <p>Transformers. The transformers repository from the Huggingface group     focuses on state-of-the-art Natural Language Processing (NLP). It provides many pre-trained model to perform tasks on     texts such as classification, information extraction, question answering, summarization, translation, text     generation, etc. in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.</p> </li> <li> <p>PyTorch-Geometric. PyTorch Geometric (PyG) is a geometric deep     learning. It consists of various methods for deep learning on graphs and other irregular structures, also known as     geometric deep learning, from a variety of published papers.</p> </li> </ul>"},{"location":"pages/projects/#project-days","title":"Project days","text":"<p>Each project day is fully dedicated to project work, except for maybe external inspirational lectures in the morning. The group decides exactly where they want to work on the project, how they want to work on the project, how do distribute the workload etc. We encourage strongly to parallelize work during the project, because there are a lot of tasks to do, but it is important that all group members at least have some understanding of the whole project.</p> <p>Remember that the focus of the project work is not to demonstrate that you can work with the biggest and baddest deep learning model, but instead that you show that you can incorporate the tools that are taught throughout the course in a meaningful way.</p> <p>Also note that the project is not expected to be very large in scope. It may simply be that you want to train X model on Y data. You will approximately be given 6 full days to work on the project. It is better that you start out with a smaller project and then add complexity along the way if you have time.</p>"},{"location":"pages/projects/#day-1","title":"Day 1","text":"<p>The first project day is all about getting started on the projects and formulating exactly what you want to work on as a group.</p> <ol> <li> <p>Start by brainstorming projects! Try to figure out exactly what you want to work with and begin to investigate what     third party package that can support the project.</p> </li> <li> <p>When you have come up with an idea, write a project description. The description is the delivery for today and should     be at least 300 words. Try to answer the following questions in the description:</p> <ul> <li>Overall goal of the project</li> <li>What framework are you going to use, and you do you intend to include the framework into your project?</li> <li>What data are you going to run on (initially, may change)</li> <li>What models do you expect to use</li> </ul> </li> <li> <p>(Optional) If you want to think more about the product design of your project, feel free to fill out the     ML canvas (or part of it). You can read more about the     different fields on canvas here.</p> </li> <li> <p>After having done the product description, you can start on the actual coding of the project. In the next section,     a to-do list is attached that summaries what we are doing in the course. You are NOT expected to fulfill all bullet     points from week 1 today.</p> </li> </ol> <p>The project description will serve as a guideline for us at the exam that you have somewhat reached the goals that you set out to do. By the end of the day, you should commit your project description to the <code>README.md</code> file belonging to your project repository. If you filled out the ML canvas, feel free to include that as part of the <code>README.md</code> file. Also remember to commit whatever you have done on the project until now. When you have done this, go to DTU Learn and hand-in (as a group) the link to your GitHub repository as an assignment.</p> <p>We will briefly (before next Monday) look over your GitHub repository and project description to check that everything is fine. If we have any questions/concerns we will contact you.</p>"},{"location":"pages/projects/#day-2","title":"Day 2","text":"<p>The goal for today is simply to continue working on your project. Start with bullet points in the checklist from week 1 and continue with bullet points for week 2.</p>"},{"location":"pages/projects/#day-3","title":"Day 3","text":"<p>Continue working on your project, today you should hopefully focus on the bullet points in the checklist from week 2. There is no delivery for this week, but make sure that you have committed all your progress at the end of the day. We will again briefly look over the repositories and will reach out to your group if we are worried about the progression of your project.</p>"},{"location":"pages/projects/#day-4","title":"Day 4","text":"<p>We have now entered the final week of the course and the second last project day. You are most likely continuing with bullet points from week 2, but should hopefully begin to look at the bullet points from week 3 today. These are in general much more complex, so we recommend looking at them until you have completed most from week 2. We also recommend that you begin to fill the report template.</p>"},{"location":"pages/projects/#day-5","title":"Day 5","text":"<p>Continue working on your project, checking off bullet points from the checklist. We recommend that you start by creating an architectural overview of your project similar to this figure, which needs to be included in your report. I recommend using draw.io for creating this kind of diagram, but feel free to use any tool you like.</p>"},{"location":"pages/projects/#day-6","title":"Day 6","text":"<p>Today you are finishing your project. Continue working on checking off bullet points and finish up answering the questions in the report. Remember that you have until midnight, where you need to hand in everything. This means that code and report should be committed to the main branch of your repository.</p>"},{"location":"pages/projects/#project-hints","title":"Project hints","text":"<p>Below are listed some hints to prevent you from getting stuck during the project work with problems that previous groups have encountered.</p> <p>Data</p> <ul> <li> <p>Start out small! We recommend that you start out with less than 1GB of data. If the dataset you want to work with     is larger, then subsample it. You can use dvc to version control your data and only download the full dataset     when you are ready to train the model.</p> </li> <li> <p>Be aware of many smaller files. <code>DVC</code> does not handle many small files well, and can take a long time to download.     If you have many small files, consider zipping them together and then unzip them at runtime.</p> </li> <li> <p>You do not need to use <code>DVC</code> for everything regarding data. You workflow is to just use <code>DVC</code> for version     controlling the data, but when you need to get it you can just download it from the source. For example if you     are storing your data in a GCP bucket, you can use the <code>gsutil</code> command to download the data or directly     accessing the it using the     cloud storage file system</p> </li> </ul> <p>Modelling</p> <ul> <li> <p>Again, start out small! Start with a simple model and then add complexity as you go along. It is better to have a     simple model that works than a complex model that does not work.</p> </li> <li> <p>Try fine-tuning a pre-trained model. This is often much faster than training a model from scratch.</p> </li> </ul> <p>Deployment</p> <ul> <li>When getting around to deployment always start out by running your application locally first, then run it locally     inside a docker container and then finally try to deploy it in the cloud. This way you can catch errors early     and not waste time on debugging cloud deployment issues.</li> </ul>"},{"location":"pages/projects/#project-checklist","title":"Project checklist","text":"<p>Please note that all the lists are exhaustive meaning that I do not expect you to have completed very point on the checklist for the exam. The parenthesis at the end indicates what module the bullet point is related to.</p>"},{"location":"pages/projects/#week-1","title":"Week 1","text":"<ul> <li> Create a git repository (M5)</li> <li> Make sure that all team members have write access to the GitHub repository (M5)</li> <li> Create a dedicated environment for you project to keep track of your packages (M2)</li> <li> Create the initial file structure using cookiecutter with an appropriate template (M6)</li> <li> Fill out the <code>data.py</code> file such that it downloads whatever data you need and preprocesses it (if necessary) (M6)</li> <li> Add a model to <code>model.py</code> and a training procedure to <code>train.py</code> and get that running (M6)</li> <li> Remember to fill out the <code>requirements.txt</code> and <code>requirements_dev.txt</code> file with whatever dependencies that you     are using (M2+M6)</li> <li> Remember to comply with good coding practices (<code>pep8</code>) while doing the project (M7)</li> <li> Do a bit of code typing and remember to document essential parts of your code (M7)</li> <li> Setup version control for your data or part of your data (M8)</li> <li> Add command line interfaces and project commands to your code where it makes sense (M9)</li> <li> Construct one or multiple docker files for your code (M10)</li> <li> Build the docker files locally and make sure they work as intended (M10)</li> <li> Write one or multiple configurations files for your experiments (M11)</li> <li> Used Hydra to load the configurations and manage your hyperparameters (M11)</li> <li> Use profiling to optimize your code (M12)</li> <li> Use logging to log important events in your code (M14)</li> <li> Use Weights &amp; Biases to log training progress and other important metrics/artifacts in your code (M14)</li> <li> Consider running a hyperparameter optimization sweep (M14)</li> <li> Use PyTorch-lightning (if applicable) to reduce the amount of boilerplate in your code (M15)</li> </ul>"},{"location":"pages/projects/#week-2","title":"Week 2","text":"<ul> <li> Write unit tests related to the data part of your code (M16)</li> <li> Write unit tests related to model construction and or model training (M16)</li> <li> Calculate the code coverage (M16)</li> <li> Get some continuous integration running on the GitHub repository (M17)</li> <li> Add caching and multi-os/python/pytorch testing to your continuous integration (M17)</li> <li> Add a linting step to your continuous integration (M17)</li> <li> Add pre-commit hooks to your version control setup (M18)</li> <li> Add a continues workflow that triggers when data changes (M19)</li> <li> Add a continues workflow that triggers when changes to the model registry is made (M19)</li> <li> Create a data storage in GCP Bucket for your data and link this with your data version control setup (M21)</li> <li> Create a trigger workflow for automatically building your docker images (M21)</li> <li> Get your model training in GCP using either the Engine or Vertex AI (M21)</li> <li> Create a FastAPI application that can do inference using your model (M22)</li> <li> Deploy your model in GCP using either Functions or Run as the backend (M23)</li> <li> Write API tests for your application and setup continues integration for these (M24)</li> <li> Load test your application (M24)</li> <li> Create a more specialized ML-deployment API using either ONNX or BentoML, or both (M25)</li> <li> Create a frontend for your API (M26)</li> </ul>"},{"location":"pages/projects/#week-3","title":"Week 3","text":"<ul> <li> Check how robust your model is towards data drifting (M27)</li> <li> Deploy to the cloud a drift detection API (M27)</li> <li> Instrument your API with a couple of system metrics (M28)</li> <li> Setup cloud monitoring of your instrumented application (M28)</li> <li> Create one or more alert systems in GCP to alert you if your app is not behaving correctly (M28)</li> <li> If applicable, optimize the performance of your data loading using distributed data loading (M29)</li> <li> If applicable, optimize the performance of your training pipeline by using distributed training (M30)</li> <li> Play around with quantization, compilation and pruning for you trained models to increase inference speed (M31)</li> </ul>"},{"location":"pages/projects/#extra","title":"Extra","text":"<ul> <li> Write some documentation for your application (M32)</li> <li> Publish the documentation to GitHub Pages (M32)</li> <li> Revisit your initial project description. Did the project turn out as you wanted?</li> <li> Create an architectural diagram over your MLOps pipeline</li> <li> Make sure all group members have an understanding about all parts of the project</li> <li> Uploaded all your code to GitHub</li> </ul>"},{"location":"pages/projects/#exam","title":"Exam","text":"<p>From January 2025 the exam only consist of a project report. The report should be handed in at midnight on the final day of the course. For January 2025, this means the 24th. We provide template folder called reports. As the first task you should copy the folder and all its content to your project repository. Then, you job is to fill out the <code>README.md</code> file which contains the report template. The file itself contains instructions on how to fill it out and instructions on using the included <code>report.py</code> file for validating your work. You will hand-in the template by simple including it in your project repository. By midnight on the final day of the course, we will automatically scrape the report and use it as the basis for grading you. Therefore, changes after this point are not registered.</p> <p>Importantly, for the scraping to work, your repository should as minimum have the following structure (which it will have if you have used the template from the code organization module):</p> <pre><code>&lt;repository-name&gt;\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 reports/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 figures/\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 ...\n</code></pre> <p>Which means that there must be a <code>reports</code> folder in the root of your repository and that inside this folder there must be a <code>README.md</code> file, which is your report.</p>"},{"location":"pages/timeplan/","title":"Time plan","text":"<p>Slides</p> <p>The course is organized into exercise (2/3 of the course) days and project days (1/3 of the course).</p> <p>Exercise days start at 9:00 in the morning with a lecture (usually 30-45 min) that will give some context about at least one of the topics of that day. Additionally, previous days exercises may shortly be touched upon. The remaining of the day will be spent on solving exercises either individually or in small groups. For some people the exercises may be fast to do and for others it will take the whole day. We will provide help throughout the day. We will try to answer questions on Slack, but help will be prioritized to students physically on campus.</p> <p>Project days are intended for project work, and you are therefore responsible for making an agreement with your group when and where you are going to work. The first project days there will be a lecture at 9:00 with project information. Other project days we may also start the day with an external lecture, which we highly recommend that you participate in. During each project day we will have office hours for you to ask questions regarding the project.</p> <p>Below is an overall time plan for each day, including the presentation topic of the day and the frameworks that you will be using in the exercises.</p> <p>Recordings (link to drive folder with mp4 files):</p> <ul> <li>\ud83c\udfa52025 Lectures</li> <li>\ud83c\udfa52024 Lectures</li> <li>\ud83c\udfa52023 Lectures</li> </ul>"},{"location":"pages/timeplan/#week-1","title":"Week 1","text":"<p>In the first week you will be introduced to a number of development practices for organizing and developing code, especially with a focus on making everything reproducible.</p> Date Day Presentation topic Frameworks Format 6/1/25 Monday Deep learning software\ud83d\udcdd Terminal, Conda, IDE, PyTorch Exercises 7/1/25 Tuesday MLOps: what is it?\ud83d\udcdd Git, CookieCutter, Pep8, DVC Exercises 8/1/25 Wednesday Reproducibility\ud83d\udcdd Docker, Hydra Exercises 9/1/25 Thursday Debugging\ud83d\udcdd Debugger, Profiler, Wandb, Lightning Exercises 10/1/25 Friday Project work\ud83d\udcdd - Projects"},{"location":"pages/timeplan/#week-2","title":"Week 2","text":"<p>The second week is about automatization and the cloud. Automatization will help use making sure that our code does not break when we make changes to it. The cloud will help us scale up our applications and we learn how to use different services to help develop a full machine learning pipeline.</p> Date Day Presentation topic Frameworks Format 13/1/25 Monday Continuous Integration\ud83d\udcdd Pytest, GitHub actions, Pre-commit, CML Exercises 14/1/25 Tuesday The Cloud\ud83d\udcdd GCP Engine, Bucket, Artifact registry, Vertex AI Exercises 15/1/25 Wednesday Deployment\ud83d\udcdd FastAPI, Torchserve, GCP Functions, GCP Run Exercises 16/1/25 Thursday External lecture - Projects 17/1/25 Friday No lecture - Projects"},{"location":"pages/timeplan/#week-3","title":"Week 3","text":"<p>For the final week we look into advance topics such as monitoring and scaling of applications. Monitoring is especially important for the longevity for the applications that we develop, that we can deploy them either locally or in the cloud and that we have the tools to monitor how they behave over time. Scaling of applications is an important topic if we ever want our applications to be used by many people at the same time.</p> Date Day Presentation topic Frameworks Format 20/1/25 Monday Monitoring\ud83d\udcdd Evidently AI, Prometheus, GCP Monitoring Exercises 21/1/25 Tuesday Scalable applications\ud83d\udcdd PyTorch, Lightning Exercises 22/1/25 Wednesday Summary lecture - Projects 23/1/25 Thursday No lecture - Projects 24/1/25 Friday No lecture - Projects"},{"location":"s10_extra/","title":"Extra learning modules","text":"<p>All modules listed here are not part of the core course but expand on some of the other topics. Some of them may still be under construction and may in the future be moved into other sessions.</p> <ul> <li> <p></p> <p>Learn how to setup a simple documentation system for your application</p> <p> M32: Documentation</p> </li> <li> <p></p> <p>Learn how to do hyperparameter optimization using Optuna</p> <p> M33: Hyperparameter Optimization</p> </li> <li> <p></p> <p>Learn how to use HPC systems that use PBS to do job scheduling</p> <p> M34: High Performance Clusters</p> </li> </ul>"},{"location":"s10_extra/documentation/","title":"M32 - Documentation","text":""},{"location":"s10_extra/documentation/#documentation","title":"Documentation","text":"<p>In today's rapidly evolving software development landscape, effective documentation is a crucial component of any project. The ability to create clear, concise, and user-friendly technical documentation can make a significant difference in the success of your codebase. We all probably encountered code that we wanted to use, only for us to abandon using it because it was missing documentation such that we could get started with it.</p> <p>Technical documentation or code documentation can be many things:</p> <ul> <li>Plain text, images and videos explaining core concepts for your software</li> <li>Documentation of API on how to call a function or class, what the different parameters are etc.</li> <li>Code examples of how to use certain functionality</li> </ul> <p>and many more. We are in this module going to focus on setting up a very basic documentation system that will automatically help you document the API of your code. For this reason we recommend that before continuing with this module that you have completed module M7 on good coding practices or have similar experience with writing docstrings for Python functions and classes.</p> <p>There are different systems for writing documentation. In fact there is a lot to choose from:</p> <ul> <li>MkDocs</li> <li>Sphinx</li> <li>GitBook</li> <li>Docusaurus</li> <li>Doxygen</li> <li>Jekyll</li> </ul> <p>Important to note that all these are static site generators. The word static here refers to that when the content is generated and served on a webside, the underlying HTML code will not change. It may contain HTML elements that dynamic (like video), but the site does not change (1).</p> <ol> <li> Good examples of dynamic sites are any social media or news media where new posts, pages etc.     are constantly added over time. Good examples of static sites are documentation, blogposts etc.</li> </ol> <p>We are in this module going to look at Mkdocs, which (in my opinion) is one of the easiest systems to get started with because all documentation is written in markdown and the build system is written in Python. As an alternativ, you can consider doing the exercises in Sphinx which is probably the most used documentation system for Python code. Sphinx offer more customization than Mkdocs, so is generally preferred for larger projects with complex documentation, but for smaller projects Mkdocs should be easier to get started with and is sufficient.</p> <p>Mkdocs by default does not include many features and for that reason we are directly going to dive into using the material for mkdocs theme that provides a lot of nice customization to create professional static sites. In fact, this whole course is written in mkdocs using the material theme.</p>"},{"location":"s10_extra/documentation/#mkdocs","title":"Mkdocs","text":"<p>The core file when using mkdocs is the <code>mkdocs.yaml</code> file, which is the configuration file for the project:</p> <pre><code>site_name: Documentation of my project\nsite_author: Jane Doe\ndocs_dir: source # (1)!\n\ntheme:\n    language: en\n    name: material # (2)!\n    features: # (3)!\n    - content.code.copy\n    - content.code.annotate\n\nplugins: # (4)!\n    - search\n    - mkdocstrings\n\nnav: # (5)!\n  - Home: index.md\n</code></pre> <ol> <li> <p> This indicates the source directory of our documentation. If the layout of your documentation is     a bit different than what described above, you may need to change this.</p> </li> <li> <p> The overall theme of your documentation. We recommend the <code>material</code> theme but there are     many more to choose from and you can also     create your own.</p> </li> <li> <p> The <code>featuers</code> section is where features that are supported by your given theme can be enabled.     In this example we have enabled <code>content.code.copy</code> feature which adds a small copy button to all code block and the     <code>content.code.annotate</code> feature which allows you to add annotations like this box to code blocks.</p> </li> <li> <p> Plugins add new functionality to your documentation.     In this case we have added two plugins that add functionality for searching through our documentation and     automatically adding documentation from docstrings. Remember that some plugins requires you to install additional     Python packages with those plugins, so remember to add them to your <code>requirements.txt</code> file.</p> </li> <li> <p> The <code>nav</code> section is where you define the navigation structure of your documentation. When you     add new <code>.md</code> files to the <code>source</code> folder you then need to add them to the <code>nav</code> section.</p> </li> </ol> <p>And that is more or less what you need to get started. In general, if you need help with configuration of your documentation in mkdocs I recommend looking at this page and this page.</p>"},{"location":"s10_extra/documentation/#exercises","title":"Exercises","text":"<p>In this set of exercises we assume that you have completed module M6 on code structure and therefore have a repository that at least contains the following:</p> <pre><code>\u251c\u2500\u2500 pyproject.toml     &lt;- Project configuration file with package metadata\n\u2502\n\u251c\u2500\u2500 docs               &lt;- Documentation folder\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 index.md       &lt;- Homepage for your documentation\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 mkdocs.yaml     &lt;- Configuration file for mkdocs\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 source/        &lt;- Source directory for documentation files\n\u2502\n\u2514\u2500\u2500 src                &lt;- Source code for use in this project.\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 __init__.py    &lt;- Makes src a Python module\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 models         &lt;- model implementations, training script\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 model.py\n\u2502   \u2502   \u251c\u2500\u2500 train_model.py\n...\n</code></pre> <p>It is not important exactly what is in the <code>src</code> folder for the exercises, but we are going to refer to the above structure in the exercises, so adjust accordingly if you diviate from this. Additionally, we are going to assume that your project code is installed in your environment such that it can be imported as normal Python code.</p> <ol> <li> <p>We are going to need two Python packages to get started: mkdocs and     material for mkdocs. Install with</p> <pre><code>pip install \"mkdocs-material &gt;= 4.8.0\" # (1)!\n</code></pre> <ol> <li>Since <code>mkdocs</code> is a dependency of <code>mkdocs-material</code> we only need to install the latter.</li> </ol> </li> <li> <p>Run in your terminal (from the <code>docs</code> folder):</p> <pre><code>mkdocs serve # (1)!\n</code></pre> <ol> <li> <code>mkdocs serve</code> will automatically rebuild the whole site whenever you save a file inside the     <code>docs</code> folder. This is not a problem if you have a fairly small site with not that many pages (or elements), but     can take a long time for large sites. Consider running with the <code>--dirty</code> option for only re-building the site     for files that have been changed.</li> </ol> <p>which should render the <code>index.md</code> file as the homepage. You can leave the documentation server running during the remaining exercises.</p> </li> <li> <p>We are no ready to document the API of our code:</p> <ol> <li> <p>Make sure you at least have one function and class inside your <code>src</code> module. If you do not have you can for     simplicity copy the following module to the <code>src/models/model.py</code> file</p> <pre><code>import torch\n\nclass MyNeuralNet(torch.nn.Module):\n    \"\"\"Basic neural network class.\n\n    Args:\n        in_features: number of input features\n        out_features: number of output features\n\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int) -&gt; None:\n        self.l1 = torch.nn.Linear(in_features, 500)\n        self.l2 = torch.nn.Linear(500, out_features)\n        self.r = torch.nn.ReLU()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x: input tensor expected to be of shape [N,in_features]\n\n        Returns:\n            Output tensor with shape [N,out_features]\n\n        \"\"\"\n        return self.l2(self.r(self.l1(x)))\n</code></pre> <p>and the following function to add <code>src/predict_model.py</code> file:</p> <pre><code>def predict(\n    model: torch.nn.Module,\n    dataloader: torch.utils.data.DataLoader\n) -&gt; None:\n    \"\"\"Run prediction for a given model and dataloader.\n\n    Args:\n        model: model to use for prediction\n        dataloader: dataloader with batches\n\n    Returns\n        Tensor of shape [N, d] where N is the number of samples and d is the output dimension of the model\n\n    \"\"\"\n    return [model(batch) for batch in dataloader]\n</code></pre> </li> <li> <p>Add a markdown file to the <code>docs/source</code> folder called <code>my_api.md</code> and add that file to the <code>nav:</code> section in     the <code>mkdocs.yaml</code> file.</p> </li> <li> <p>To that file add the following code:</p> <pre><code># My API\n\n::: src.models.model.MyNeuralNet\n\n::: src.predict_model.predict\n</code></pre> <p>The <code>:::</code> indicator tells mkdocs that it should look for the corresponding function/module and then render it on the given page. Thus, if you have a function/module located in another location change the paths accordingly.</p> </li> <li> <p>Make sure that the documentation correctly includes your function and module on the given page.</p> </li> <li> <p>(Optional) Include more functions/modules in your documentation.</p> </li> </ol> </li> <li> <p>(Optional) Look through the documentation for mkdocstrings and try to     improve the layout a bit. Especially, the     headings,     docstrings and     signatures could be of interest to adjust.</p> </li> <li> <p>Finally, try to build a final version of your documentation</p> <pre><code>mkdocs build\n</code></pre> <p>this should result in a <code>site</code> folder that contains the actual HTML code for documentation.</p> </li> </ol>"},{"location":"s10_extra/documentation/#publish-your-documentation","title":"Publish your documentation","text":"<p>To publish your documentation you need a place to host your build documentation e.g. the content of the <code>site</code> folder you build in the last exercise. There are many places to host your documentation, but if you only need a static site and are already hosting your code through GitHub, then a good option is GitHub Pages. GitHub pages is free to use for your public projects.</p> <p>Before getting started with this set of exercises you should have completed module M16 on GitHub actions so you already know about workflow files.</p>"},{"location":"s10_extra/documentation/#exercises_1","title":"Exercises","text":"<ol> <li> <p>Start by adding a new file called <code>deploy_docs.yaml</code> to the <code>.github/workflows</code> folder. Add the following cod to that     file and save it.</p> <pre><code>name: Deploy docs\n\non:\npush:\n    branches:\n        - main\n\npermissions:\n    contents: write # (1)!\n\njobs:\n  deploy:\n    name: Deploy docs\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: 3.11\n        cache: 'pip'\n        cache-dependency-path: setup.py\n\n    - name: Install dependencies\n      run: pip install -r requirements.txt\n\n    - name: Deploy docs\n      run: mkdocs gh-deploy --force\n</code></pre> <ol> <li> It is important to give <code>write</code> permissions to this actions because it is not only reading     your code but it will also push code.</li> </ol> <p>Before continuing, make sure you understand what the different steps of the workflow does and especially we recommend looking at the documentation of the <code>mkdocs gh-deploy</code> command.</p> </li> <li> <p>Commit and push the file. Check that the action is executed and if it succeeds, that your build project is pushed to     a branch called <code>gh-pages</code>. If the action does not succeeds, then figure out what is wrong and fix it!</p> </li> <li> <p>After confirming that our action is working, you need to configure GitHub to publish the content being     build by GitHub Actions. Do the following:</p> <ul> <li>Go to the Settings tab and then the Pages subsection</li> <li>In the <code>Source</code> setting choose the <code>Deploy from a branch</code></li> <li>In the <code>Branch</code> setting choose the <code>gh-pages</code> branch and <code>/(root)</code> folder and save</li> </ul> <p> </p> <p>This should then start deploying your site to <code>https://&lt;your-username&gt;.github.io/&lt;your-reponame&gt;/</code>. If it does not do this you may need to recommit and trigger the GitHub actions build again.</p> </li> <li> <p>Make sure your documentation is published and looks as it should.</p> </li> </ol> <p>This ends the module on technical documentation. We cannot stress enough how important it is to write proper documentation for larger projects that need to be maintained over a longer time. It is often a iterative process, but it is often best to do it while writing the code.</p>"},{"location":"s10_extra/high_performance_clusters/","title":"M34 - High Performance Clusters","text":""},{"location":"s10_extra/high_performance_clusters/#high-performance-clusters","title":"High Performance Clusters","text":"<p>As discussed in the intro session on the cloud, cloud providers offers near infinite compute resources. However, using these resources comes at a hefty price often and it is therefore important to be aware of another resource many have access to: High Performance Clusters or HPC. HPCs exist all over the world, and many time you already have access to one or can easily get access to one. If you are an university student you most likely have a local HPC that you can access through your institution. Else, there exist public HPC resources that everybody (with a project) can apply for. As an example in the EU we have EuroHPC initiative that currently has 8 different supercomputers with a centralized location for applying for resources that are both open for research projects and start-ups.</p> <p>Depending on your application, you may have different needs and it is therefore important to be aware also of the different tiers of HPC. In Europe, HPC are often categorized such that Tier-0 are European Centers with petaflop or hexascale machines,\u00a0Tier 1 are National centers of supercomputers, and Tier 2 are Regional centers. The lower the Tier, the larger applications it is possible to run.</p> <p></p>"},{"location":"s10_extra/high_performance_clusters/#cluster-architectures","title":"Cluster architectures","text":"<p>In very general terms, cluster can come as two different kind of systems: supercomputers and LSF (Load Sharing Facility). A supercomputer (as shown below) is organized into different modules, that are separated by network link. When you login to a supercomputer you will meet the front end which contains all the software needed to run computations. When you submit a job it will get sent to the backend modules which in most cases includes: general compute modules (CPU), acceleration modules (GPU), a memory module (RAM) and finally a storage module (HDD). Depending on your application you may need one module more than another. For example in deep learning the acceleration module is important but in physics simulation the general compute module / storage model is probably more important.</p> <p></p>  Overview of the Meluxina supercomputer that's part of EuroHPC.  Image credit  <p>Alternatively, LSF are a network of computers where each computer has its own CPU, GPU, RAM etc. and the individual computes (or nodes) are then connected by network. The important different between a supercomputer and as LSF systems is how the resources are organized. When comparing supercomputers to LSF system it is generally the case that it is better to run on a LSF system if you are only requesting resources that can be handled by a single node, however it is better to run on a supercomputer if you have a resource intensive application that requires many devices to communicate with each others.</p> <p>Regardless of cluster architectures, on the software side of HPC, the most important part is what's called the HPC scheduler. Without a HPC scheduler an HPC cluster would just be a bunch of servers with different jobs interfering with each other. The problem is when you have a large collection of resources and a large collection of users, you cannot rely on the users just running their applications without interfering with each other. A HPC scheduler is in charge of managing that whenever an user request to run an application, they get put in a queue and whenever the resources their application ask for are available the application gets run.</p> <p>The biggest bach control systems for doing scheduling on HPC are:</p> <ul> <li>SLURM</li> <li>MOAB HPC Suite</li> <li>PBS Works</li> </ul> <p>We are going to take a look at PBS works as that is what is installed on our local university cluster.</p>"},{"location":"s10_extra/high_performance_clusters/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>The following exercises are focused on local students at DTU that want to use our local HPC resources. That said, the steps in the exercise are fairly general to other types of cluster. For the purpose of this exercise we are going to see how we can run this image classifier script , but feel free to work with whatever application you want to.</p> <ol> <li> <p>Start by accessing the cluster. This can either be through <code>ssh</code> in a terminal or if you want a graphical interface     thinlinc can be installed. In general we recommend following the steps     here for DTU students as the setup depends on if you are on campus or not.</p> </li> <li> <p>When you have access to the cluster we are going to start with the setup phase. In the setup phase we are going     to setup the environment necessary for our computations. If you have accessed the cluster through graphical interface     start by opening a terminal.</p> <ol> <li> <p>Lets start by setting up conda for controlling our dependencies. If you have not already worked with <code>conda</code>,     please checkout module     M2 on package managers and virtual environments. In general     you should be able to setup (mini)conda through these two commands:</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\n</code></pre> </li> <li> <p>Close the terminal and open a new for the installation to complete. Type <code>conda</code> in the terminal to check that     everything is fine. Go ahead and create a new environment that we can install dependencies in</p> <pre><code>conda create -n \"hpc_env\" python=3.10 --no-default-packages\n</code></pre> <p>and activate it.</p> </li> <li> <p>Copy over any files you need. For the image classifier script you need the     requirements file     and the actual     application.</p> </li> <li> <p>Next, install all the requirements you need. If you want to run the image classifier script you can run this     command in the terminal</p> <pre><code>pip install -r image_classifier_requirements.txt\n</code></pre> <p>using this requirements file.</p> </li> </ol> </li> <li> <p>That's all the setup needed. You would need to go through the creating of environment and installation of requirements     whenever you start a new project (no need for reinstalling conda). For the next step we need to look at how to submit     jobs on the cluster. We are now ready to submit the our first job to the cluster:</p> <ol> <li> <p>Start by checking the statistics for the different clusters. Try to use both the <code>qstat</code> command which should give     an overview of the different cluster, number of running jobs and number of pending jobs. For many system you can     also try the much more user friendly command <code>classstat</code> command.</p> </li> <li> <p>Figure out which queue you want to use. For the sake of the exercises it needs to be one with GPU support. For     DTU students, any queue that starts with <code>gpu</code> are GPU accelerated.</p> </li> <li> <p>Now we are going to develop a bash script for submitting our job. We have provided an example of such     scripts. Take a     careful look and go each line and make sure you understand it. Afterwards, change it to your needs     (queue and student email).</p> </li> <li> <p>Try to submit the script:</p> <pre><code>bsub &lt; jobscript.sh\n</code></pre> <p>You can check the status of your script by running the <code>bstat</code> command. Hopefully, the job should go through really quickly. Take a look at the output file, it should be called something like <code>gpu_*.out</code>. Also take a look at the <code>gpu_*.err</code> file. Does both files look as they should?</p> </li> </ol> </li> <li> <p>Lets now try to run our application on the cluster. To do that we need to take care of two things:</p> <ol> <li> <p>First we need to load the correct version of CUDA. A cluster system often contains multiple versions of specific     software to suit the needs of all their users, and it is the users that are in charge of loading the correct     software during job submission. The only extra software that needs to be loaded for most PyTorch applications     are a CUDA module. You can check which modules are available on the cluster with</p> <pre><code>module avail\n</code></pre> <p>Afterwards, add the correct CUDA version you need to the <code>jobscript.sh</code> file. If you are trying to run the provided image classifier script then the correct version is <code>CUDA/11.7</code> (can be seen in the requirements file).</p> <pre><code># add to the bottom of the file\nmodule load cuda/11.7\n</code></pre> </li> <li> <p>We are now ready to add in our application. The only thing we need to take care of is telling the system to run     it using the <code>python</code> version that is connected to our <code>hpc_env</code> we created in the beginning. Try typing:</p> <pre><code>which python\n</code></pre> <p>which should give you the full path. Then add to the bottom of the <code>jobscript</code> file:</p> <pre><code>~/miniconda3/envs/hpc_env/bin/python \\\n    image_classifier.py \\\n    --trainer.accelerator 'gpu' --trainer.devices 1  --trainer.max_epochs 5\n</code></pre> <p>which will run the image classifier script (change it if you are running something else).</p> </li> <li> <p>Finally submit the job:</p> <pre><code>bsub &lt; jobscript.sh\n</code></pre> <p>and check when it is done that it has produced what you expected.</p> </li> <li> <p>(Optional) If you application supports multi GPUs also try that out. You would first need to change the     jobscript to request multiple GPUs and additionally you would need to tell your application to run on multiple     GPUs. For the image classifier script it can be done by changing the <code>--trainer.devices</code> flag     to <code>2</code> (or higher).</p> </li> </ol> </li> </ol> <p>This ends the module on using HPC systems.</p>"},{"location":"s10_extra/hyperparameters/","title":"M33 - Hyperparameter optimization","text":""},{"location":"s10_extra/hyperparameters/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>Outdated module</p> <p>This module has not been updated for a long time and therefore some functionality of Optuna, which is used in these exercises, may not be included. If you have completed the module on Weights &amp; Bias then we highly recommend instead using their sweep functionality.</p> <p>Hyperparameter optimization is not a new idea within machine learning but have somewhat seen a renaissance with the uprise of deep learning. This can mainly be contributed to the following:</p> <ul> <li>Trying to beat state-of-the-art often comes down to very small differences in performance, and hyperparameter     optimization can help squeeze out a bit more</li> <li>Deep learning models are in general not that robust towards the choice of hyparameter so choosing the wrong set     may lead to a model that does not work</li> </ul> <p>However the problem with doing hyperparameter optimization of a deep learning models is that it can take over a week to train a single model. In most cases we therefore cannot do a full grid search of all hyperparameter combinations to get the best model. Instead we have to do some tricks that will help us speed up our searching. In these exercises we are going to be integrating optuna into our different models, that will provide the tools for speeding up our search.</p> <p></p> <p>It should be noted that a lot of deep learning models does not optimize every hyperparameter that is included in the model but instead relies on heuristic guidelines (\"rule of thumb\") based on what seems to be working in general e.g. a learning rate of 0.01 seems to work great with the Adam optimizer. That said, these rules probably only apply to 80% of deep learning model, whereas for the last 20% the recommendations may be suboptimal Here is a great site that has collected an extensive list of these recommendations, taken from the excellent deep learning book by Ian Goodfellow, Yoshua Bengio and Aaron Courville.</p> <p>In practice, I recommend trying to identify (through experimentation) which hyperparameters that are important for the performance of your model and then spend your computational budget trying to optimize them while setting the rest to a \"recommended value\".</p>"},{"location":"s10_extra/hyperparameters/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <ol> <li> <p>Start by installing optuna:     <code>pip install optuna</code></p> </li> <li> <p>Initially we will look at the <code>cross_validate.py</code> file. It implements simple K-fold cross validation of     a random forest sklearn digits dataset (subset of MNIST). Look over the script and try to run it.</p> </li> <li> <p>We will now try to write the same code in optune. Please note that the script have a variable <code>OPTUNA=False</code>     that you can use to change what part of the code should run. The three main concepts of optuna is</p> <ul> <li> <p>A trial: a single experiment</p> </li> <li> <p>A study: a collection of trials</p> </li> <li> <p>The objective: function to determine how \"good\" a trial is</p> </li> </ul> <p>Lets start by writing the objective function, which we have already started in the script. For now you do not need to care about the <code>trial</code> argument, just assume that it contains the hyperparameters needed to define your random forest model. The output of the objective function should be a single number that we want to optimize. (HINT: did you remember to do K-fold crossvalidation inside your objective function?)</p> </li> <li> <p>Next lets focus on the trial. Inside the <code>objective</code> function the trial should be used to suggest what     parameters to use next. Take a look at the documentation for     trial or take a look at     the code examples and figure out how to define the hyperparameter of the model.</p> </li> <li> <p>Finally lets launch a study. It can be as simple as</p> <pre><code>study = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n</code></pre> <p>but lets play around a bit with it:</p> <ol> <li> <p>By default the <code>.optimize</code> method will minimize the objective (by definition the optimum of an objective     function is at its minimum). Is the score your objective function is returning something that should     be minimized? If not, a simple solution is to put a <code>-</code> in front of the metric. However, look through     the documentation on how to change the direction of the optimization.</p> </li> <li> <p>Optuna will by default do Bayesian optimization when sampling the hyperparameters (using a evolutionary     algorithm for suggesting new trials). However, since this example is quite simple, we can actually     perform a full grid search. How would you do this in Optuna?</p> </li> <li> <p>Compare the performance of a single optuna run using Bayesian optimization with <code>n_trials=10</code> with a     exhaustive grid search that have search through all hyperparameters. What is the performance/time     trade-off for these two solutions?</p> </li> </ol> </li> <li> <p>In addition to doing baysian optimization, the other great part about Optuna is that it have native support     for Pruning unpromising trials. Pruning refers to the user stopping trials for hyperparameter combinations     that does not seem to lead anywhere. You may have learning rate that is so high that training is diverging or     a neural network with too many parameters so it is just overfitting to the training data. This however begs the     question: what constitutes an unpromising trial? This is up to you to define based on prior experimentation.</p> <ol> <li> <p>Start by looking at the <code>fashion_trainer.py</code> script. Its a simple classification network for classifying     images in the FashionMNIST dataset. Run the script     with the default hyperparameters to get a feeling of how the training should be progress.     Note down the performance on the test set.</p> </li> <li> <p>Start by defining a validation set and a validation dataloader that we can use for hyperparameter optimization     (HINT: use 5-10% of you training data).</p> </li> <li> <p>Now, adjust the script to use Optuna. The 5 hyperparameters listed in the table above should at least be included     in the hyperparameter search. For some we have already defined the search space but for the remaining you need to     come up with a good range of values to investigate. We done integrating optuna, run a small study (<code>n_tirals=3</code>)     to check that the code is working.</p> Hyperparameter Search space Learning rate 1e-6 to 1e0 Number of output features in the second last layer ??? The amount of dropout to apply ??? Batch size ??? Use batch normalize or not {True, False} (Optional) Different activations functions {<code>nn.ReLU</code>, <code>nn.Tanh</code>, <code>nn.RReLU</code>, <code>nn.LeakyReLU</code>, <code>nn.ELU</code>} </li> <li> <p>If implemented correctly the number of hyperparameter combinations should be at least 1000, meaning that     we not only need baysian optimization but probably also need pruning to succeed. Checkout the page for     built-in pruners in Optuna. Implement     pruning in the script. I recommend using either the <code>MedianPruner</code> or the <code>ProcentilePruner</code>.</p> </li> <li> <p>Re-run the study using pruning with a large number of trials (<code>n_trials&gt;50</code>)</p> </li> <li> <p>Take a look at this     visualization page     for ideas on how to visualize the study you just did. Make at least two visualization of the study and     make sure that you understand them.</p> </li> <li> <p>Pruning is great for better spending your computational budged, however it comes with a trade-off. What is     it and what hyperparameter should one be especially careful about when using pruning?</p> </li> <li> <p>Finally, what parameter combination achieved the best performance? What is the test set performance for this     set of parameters. Did you improve over the initial set of hyperparameters?</p> </li> </ol> </li> <li> <p>The exercises until now have focused on doing the hyperparameter searching sequentially, meaning that we test one     set of parameters at the time. It is a fine approach because you can easily let it run for a week without any     interaction. However, assuming that you have the computational resources to run in parallel, how do you do that?</p> <ol> <li> <p>To run hyperparameter search in parallel we need a common database that all experiments can read and     write to. We are going to use the recommended <code>mysql</code>. You do not have to understand what SQL is to     complete this exercise, but it is basically a language (like python)     for managing databases. Install mysql.</p> </li> <li> <p>Next we are going to initialize a database that we can read and write to. For this exercises we are going     to focus on a locally stored database but it could of course also be located in the cloud.</p> <pre><code>mysql -u root -e \"CREATE DATABASE IF NOT EXISTS example\"\n</code></pre> <p>you can also do this directly in Python when calling the <code>create_study</code> command by also setting the <code>storage</code> and <code>load_if_exists=True</code> flags.</p> </li> <li> <p>Now we are going to create a Optuna study in our database</p> <pre><code>optuna create-study --study-name \"distributed-example\" --storage \"mysql://root@localhost/example\"\n</code></pre> </li> <li> <p>Change how you initialize the study to read and write to the database. Therefore, instead of doing</p> <pre><code>study = optuna.create_study()\n</code></pre> <p>then do</p> <pre><code>study = optuna.load_study(\n    study_name=\"distributed-example\", storage=\"mysql://root@localhost/example\"\n)\n</code></pre> <p>where the <code>study_name</code> and <code>storage</code> should match how the study was created.</p> </li> <li> <p>For running in parallel, you can either open up a extra terminal and simple launch your script once     per open terminal or you can use the provided <code>parallel_lancher.py</code> that will launch multiple executions     of your script. It should be used as:</p> <pre><code>python parallel_lancher.py myscript.py --num_parallel 2\n</code></pre> </li> <li> <p>Finally, make sure that you can access the results</p> </li> </ol> </li> </ol> <p>That's all on how to do hyperparameter optimization in a scalable way. If you feel like it you can try to apply these techniques on the ongoing corrupted MNIST example, where you are free to choose what hyperparameters that you want to use.</p>"},{"location":"s1_development_environment/","title":"Setting up a development environment","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn the basics of the command line, and how to use it to navigate your file system and run programs.</p> <p> M1: Command line</p> </li> <li> <p></p> <p>Learn how package managers work in Python and how to create reproducible virtual environments using <code>conda</code> and <code>pip</code>.</p> <p> M2: Package Manager</p> </li> <li> <p></p> <p>Learn how to use a modern editor for code development.</p> <p> M3: Editor</p> </li> <li> <p></p> <p>Refresh your PyTorch skills and implement a simple deep-learning model.</p> <p> M4: Deep Learning Software</p> </li> </ul> <p>Today, we begin our exploration of machine learning operations (MLOps). Before diving in, we need to ensure a basic understanding of several key topics that we'll use throughout the course. This session focuses on setting up a suitable development environment. Many of you likely have prior experience with these topics, so this will primarily serve as a review.</p> <p>We're starting here because many students lack fundamental skills that are often assumed but not explicitly taught. This session covers the essential skills to begin your MLOps journey. For a broader overview of computer science fundamentals, we recommend The Missing Semester of Your CS Education from MIT.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the basics of the command line</li> <li>Be able to create reproducible virtual environments</li> <li>Be able to use a modern editor for code development</li> <li>Write and run a Python program, implementing a simple deep-learning model</li> </ul>"},{"location":"s1_development_environment/command_line/","title":"M1 - The command line","text":""},{"location":"s1_development_environment/command_line/#the-command-line","title":"The command line","text":"<p>Core Module</p> <p></p>  Image credit  <p>The command line (also commonly known as the terminal) provides a text-based interface for interacting with your computer. It originated in an era before graphical user interfaces became standard. While Linux users are often familiar with the terminal, Mac and Windows users may encounter it less frequently. However, a basic understanding of the command line is valuable for improving workflow, especially in MLOps. Many MLOps tools lack graphical interfaces, making terminal interaction necessary. Furthermore, working effectively in cloud environments, which we will cover later in the course, often requires using the command line.</p> <p>Note that if you're already a terminal wizard then feel free to skip the exercises below. They are very elementary.</p>"},{"location":"s1_development_environment/command_line/#the-anatomy-of-the-command-line","title":"The anatomy of the command line","text":"<p>Regardless of the operating system, all command lines look more or less the same:</p> <p></p> <p>As the image illustrates, executing a command involves several components:</p> <ol> <li>The prompt is the part where you type your commands. It usually contains the name of the current directory you     are in, followed by some kind of symbol: <code>$</code>, <code>&gt;</code>, <code>:</code> are the usual ones. It can also contain other information,     such as in the case of the above image, which also shows the current <code>conda</code> environment.</li> <li>The command is the actual command you want to execute. For example, <code>ls</code> or <code>cd</code>.</li> <li>The options (or flags) modify the command's behavior. They often start with a hyphen (<code>-</code>) or double     hyphen (<code>--</code>). For example, <code>-l</code> in <code>ls -l</code>.</li> <li>The arguments specify what the command should operate on. For example, <code>figures</code> in <code>ls -l figures</code>.</li> </ol> <p>Generally, options are optional modifiers, while arguments provide the necessary inputs for the command.</p> <p></p>  Image credit"},{"location":"s1_development_environment/command_line/#exercises","title":"\u2754 Exercises","text":"<p>We have put a cheat sheet in the exercise files folder belonging to this session that gives a quick overview of the different commands that can be executed in the command line.</p> Windows users <p>We highly recommend that you install Windows Subsystem for Linux (WSL). This will install a full Linux system on your Windows machine. Please follow this guide. Remember to run commands from an elevated (as administrator) Windows Command Prompt. You can in general complete all exercises in the course from a normal Windows Command Prompt, but some are easier to do if you run from WSL.</p> <p>If you decide to run in WSL, you need to remember that you now have two different systems, and installing a package on one system does not mean that it is installed on the other. For example, if you install <code>pip</code> in WSL, you need to install it again in Windows if you want to use it there.</p> <p>If you decide to not run in WSL, please always work in a Windows Command Prompt and not Powershell.</p> <ol> <li> <p>Start by opening a terminal.</p> </li> <li> <p>To navigate inside a terminal, we rely on the <code>cd</code> command and <code>pwd</code> command. Make sure you know how to go back and     forth in your file system. (1)</p> <ol> <li> Your terminal should support     tab completion which can help finish commands for you!</li> </ol> </li> <li> <p>The <code>ls</code> command is important when we want to inspect the content of a folder. Try to use the command, and also try     it with the additional option <code>-l</code>. What does it show?</p> </li> <li> <p>Make sure to familiarize yourself with the <code>which</code>, <code>echo</code>, <code>cat</code>, <code>wget</code>, <code>less</code>, and <code>top</code> commands. Also,     familiarize yourself with the <code>&gt;</code> operator (used for output redirection). You are likely to use some of these     commands and concepts throughout the course or in your future career. For Windows users, these commands may have     different names (e.g., <code>where</code> corresponds to <code>which</code>).</p> </li> <li> <p>It is also important that you know how to edit a file through the terminal. Most systems should have the     <code>nano</code> editor installed; otherwise, try to figure out which one is installed on your system.</p> <ol> <li> <p>Type <code>nano</code> in the terminal.</p> </li> <li> <p>Write the following text in the script</p> <pre><code>if __name__ == \"__main__\":\n    print(\"Hello world!\")\n</code></pre> </li> <li> <p>Save the script as a <code>.py</code> file and try to execute it using <code>python &lt;filename&gt;.py</code>.</p> </li> <li> <p>Afterwards, try to edit the file through the terminal (change <code>Hello world</code> to something else).</p> </li> </ol> </li> <li> <p>All terminals come with a programming language. The most common system is called <code>bash</code>, which can come in handy     when you're able to write simple programs in bash. For example, if you want to execute multiple Python     programs sequentially, this can be done through a bash script.</p> Windows users <p>Bash is not part of Windows, so you need to run this part through WSL. If you did not install WSL, you can skip this part or as an alternative do the exercises in Powershell, which is the native Windows scripting language (not recommended).</p> <ol> <li> <p>Write a bash script (in <code>nano</code>) and try executing it:</p> <pre><code>#!/bin/bash\n# A sample Bash script, by Ryan\necho Hello World!\n</code></pre> </li> <li> <p>Change the bash script to call the Python program you just wrote.</p> </li> <li> <p>Try to Google how to write a simple for-loop that executes the Python script 10 times in a row.</p> </li> </ol> </li> <li> <p>A trick you may need throughout this course is setting environment variables. An environment variable is just a     dynamically named value that may alter the way running processes behave on a computer. The syntax for setting an     environment variable depends on your operating system:</p> WindowsLinux/Mac <pre><code>set MY_VAR=hello\necho %MY_VAR%\n</code></pre> <pre><code>export MY_VAR=hello\necho $MY_VAR\n</code></pre> <ol> <li> <p>Try to set an environment variable and print it out.</p> </li> <li> <p>To use an environment variable in a Python program, you can use the <code>os.environ</code> function from the <code>os</code> module.     Write a Python program that prints out the environment variable you just set.</p> </li> <li> <p>If you have a collection of environment variables, these can be stored in a file called <code>.env</code>. The file is     formatted as follows:</p> <pre><code>MY_VAR=hello\nMY_OTHER_VAR=world\n</code></pre> <p>To load the environment variables from the file, you can use the <code>python-dotenv</code> package. Install it with <code>pip install python-dotenv</code> and then try to load the environment variables from the file and print them out.</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()\nimport os\nprint(os.environ[\"MY_VAR\"])\n</code></pre> </li> </ol> </li> </ol>"},{"location":"s1_development_environment/command_line/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>Here is one command from later in the course when we will be working in the cloud</p> <pre><code>gcloud compute instances create-with-container instance-1 \\\n    --container-image=gcr.io/&lt;project-id&gt;/gcp_vm_tester\n    --zone=europe-west1-b\n</code></pre> <p>Identify the command, options, and arguments.</p> Solution <ul> <li>The command is <code>gcloud compute instances create-with-container</code>.</li> <li>The options are <code>--container-image=gcr.io/&lt;project-id&gt;/gcp_vm_tester</code> and <code>--zone=europe-west1-b</code>.</li> <li>The arguments are <code>instance-1</code>.</li> </ul> <p>The tricky part of this example is that commands can have subcommands, which are also commands. In this case, <code>compute</code> is a subcommand to <code>gcloud</code>, <code>instances</code> is a subcommand to <code>compute</code>, and <code>create-with-container</code> is a subcommand to <code>instances</code>.</p> </li> <li> <p>Two common arguments that nearly all commands have are the <code>-h</code> and <code>-V</code> options. What does each of them do?</p> Solution <p>The <code>-h</code> (or <code>--help</code>) option prints the help message for the command, including subcommands and arguments. Try it out by executing <code>python -h</code>.   The <code>-V</code> (or <code>--version</code>) option prints the version of the installed program. Try it out by executing <code>python --version</code>.</p> </li> </ol> <p>This concludes the module on the command line. Don't worry if you're not yet fully comfortable; we will use the command line extensively throughout the course, providing ample opportunity for practice. If you want to spend additional time on this topic, we highly recommend watching this video for a more in-depth introduction.</p> <p>If you are interested in personalizing your command line, you can check out the starship project, which allows you to customize your command line with a lot of different options.</p>"},{"location":"s1_development_environment/deep_learning_software/","title":"M4 - Deep Learning Software","text":""},{"location":"s1_development_environment/deep_learning_software/#deep-learning-software","title":"Deep Learning Software","text":"<p>Core Module</p> <p>Since its breakthrough in 2012, deep learning has revolutionized various aspects of our lives, from Google Translate and driverless cars to personal assistants and protein engineering. It is transforming nearly every sector of the economy. However, deploying deep learning models in production presents unique challenges. The concept of technical debt highlights the significant maintenance costs associated with running machine learning systems in production. MLOps, inspired by classical DevOps, aims to address these challenges by developing methods, processes, and tools to streamline the development and deployment of deep learning models.</p> <p>While MLOps concepts and tools can be applied to classical machine learning models (e.g., K-nearest neighbor, Random Forest), deep learning introduces specific challenges related to the size of data and models. Therefore, this course focuses on deep learning models.</p>"},{"location":"s1_development_environment/deep_learning_software/#software-landscape-for-deep-learning","title":"Software Landscape for Deep Learning","text":"<p>Regarding software for Deep Learning, the landscape is currently dominated by three software frameworks (listed in order of when they were published):</p> <p> </p> <ul> <li> <p>TensorFlow</p> </li> <li> <p>PyTorch</p> </li> <li> <p>JAX</p> </li> </ul> <p>A detailed comparison of these frameworks is unnecessary. PyTorch and TensorFlow, being the oldest, have larger communities and comprehensive feature sets for both research and production. JAX, a newer framework, incorporates improvements over PyTorch and TensorFlow but is still evolving. Given their different programming paradigms (object-oriented vs. functional), a direct comparison is not particularly meaningful.</p> <p>This course uses PyTorch due to its intuitive nature and its prevalence in our research. Currently, PyTorch is the dominant framework for published models, research papers, and competition winners.</p> <p>The intention behind this set of exercises is to get everyone's PyTorch skills up to date. If you're already a PyTorch-Jedi, feel free to skip the first set of exercises, but I still recommend that you go through them. The exercises are in large part taken directly from the deep learning course at Udacity. Note that these exercises are given as notebooks, which is the only time we are going to use them actively in the course. Instead, after this set of exercises, we are going to focus on writing code in Python scripts.</p> <p>The notebooks contain a lot of explanatory text. The exercises that you are supposed to complete are inlined in the text in small \"exercise\" blocks:</p> <p></p> <p>For a refresher on deep learning topics, consult the relevant chapters in the deep learning book by Goodfellow, Bengio, and Courville (also available in the literature folder). While expertise in deep learning is not required to pass this course, a basic understanding of the concepts is beneficial. The course focuses on the software aspects of deploying deep learning models in production.</p>"},{"location":"s1_development_environment/deep_learning_software/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <ol> <li> <p>Start a Jupyter Notebook session in your terminal (assuming you are at the root of the course material).     Alternatively, you should be able to open the notebooks directly in your code editor. For VS Code users you can read     more about how to work with Jupyter Notebooks in VS code     here</p> </li> <li> <p>Complete the     Tensors in PyTorch     notebook. It focuses on basic manipulation of PyTorch tensors. You can skip this notebook if you are comfortable     doing this.</p> </li> <li> <p>Complete the     Neural Networks in PyTorch     notebook. It focuses on building a very simple neural network using the PyTorch <code>nn.Module</code> interface.</p> </li> <li> <p>Complete the     Training Neural Networks     notebook. It focuses on how to write a simple training loop for training a neural network.</p> </li> <li> <p>Complete the     Fashion MNIST     notebook, which summarizes concepts learned in notebooks 2 and 3 on building a neural network for classifying the     Fashion MNIST dataset.</p> </li> <li> <p>Complete the     Inference and Validation     notebook. This notebook adds important concepts on how to do inference and validation on our neural network.</p> </li> <li> <p>Complete the     Saving_and_Loading_Models     notebook. This notebook addresses how to save and load model weights. This is important if you want to share a     model with someone else.</p> </li> </ol>"},{"location":"s1_development_environment/deep_learning_software/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>If tensor <code>a</code> has shape <code>[N, d]</code> and tensor <code>b</code> has shape <code>[M, d]</code> how can we calculate the pairwise distance     between rows in <code>a</code> and <code>b</code> without using a for loop?</p> Solution <p>We can take advantage of broadcasting to do this</p> <pre><code>a = torch.randn(N, d)\nb = torch.randn(M, d)\ndist = torch.sum((a.unsqueeze(1) - b.unsqueeze(0))**2, dim=2)  # shape [N, M]\n</code></pre> </li> <li> <p>What should be the size of <code>S</code> for an input image of size 1x28x28, and how many parameters does the neural network     then have?</p> <pre><code>from torch import nn\nneural_net = nn.Sequential(\n    nn.Conv2d(1, 32, 3), nn.ReLU(), nn.Conv2d(32, 64, 3), nn.ReLU(), nn.Flatten(), nn.Linear(S, 10)\n)\n</code></pre> Solution <p>Since both convolutions have a kernel size of 3, stride 1 (default value) and no padding that means that we lose 2 pixels in each dimension, because the kernel cannot be centered on the edge pixels. Therefore, the output of the first convolution would be 32x26x26. The output of the second convolution would be 64x24x24. The size of <code>S</code> must therefore be <code>64 * 24 * 24 = 36864</code>. The number of parameters in a convolutional layer is <code>kernel_size * kernel_size * in_channels * out_channels + out_channels</code> (last term is the bias) and the number of parameters in a linear layer is <code>in_features * out_features + out_features</code> (last term is the bias). Therefore, the total number of parameters in the network is <code>3*3*1*32 + 32 + 3*3*32*64 + 64 + 36864*10 + 10 = 387,466</code>, which could be calculated by running:</p> <pre><code>sum([prod(p.shape) for p in neural_net.parameters()])\n</code></pre> </li> <li> <p>A working training loop in PyTorch should have these three function calls: <code>optimizer.zero_grad()</code>,     <code>loss.backward()</code>, <code>optimizer.step()</code>. Explain what would happen in the training loop (or implement it) if you     forgot each of the function calls.</p> Solution <p><code>optimizer.zero_grad()</code> is in charge of zeroing the gradient. If this is not done, then gradients would accumulate over the steps, leading to exploding gradients. <code>loss.backward()</code> is in charge of calculating the gradients. If this is not done, then the gradients will not be calculated and the optimizer will not be able to update the weights. <code>optimizer.step()</code> is in charge of updating the weights. If this is not done, then the weights will not be updated and the model will not learn anything.</p> </li> </ol>"},{"location":"s1_development_environment/deep_learning_software/#final-exercise","title":"Final exercise","text":"<p>As the final exercise, we will develop a simple baseline model that we will continue to develop during the course. For this exercise, we provide a corrupted subset of the MNIST dataset which can be downloaded from this Google Drive folder or using these two commands:</p> <pre><code>pip install gdown\ngdown --folder 'https://drive.google.com/drive/folders/1ddWeCcsfmelqxF8sOGBihY9IU98S9JRP?usp=sharing'\n</code></pre> <p>The data should be placed in a subfolder called <code>data/corruptmnist</code> in the root of the project. Your overall task is the following:</p> <p>Implement an MNIST neural network that achieves at least 85% accuracy on the test set.</p> <p>Before any training can start, you should identify the corruption that we have applied to the MNIST dataset to create the corrupted version. This can help you identify what kind of neural network to use to get good performance, but any network should be able to achieve this.</p> <p>One key point of this course is trying to stay organized. Spending time now organizing your code will save time in the future as you start to add more and more features. As subgoals, please complete the following exercises:</p> <ol> <li> <p>Implement your model in a script called <code>model.py</code>.</p> Starting point for <code>model.py</code> model.py<pre><code>from torch import nn\n\n\nclass MyAwesomeModel(nn.Module):\n    \"\"\"My awesome model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.fc1 = nn.Linear(784, 128)\n</code></pre> Solution <p>The provided solution implements a convolutional neural network with 3 convolutional layers and a single fully connected layer. Because the MNIST dataset consists of images, we want an architecture that can take advantage of the spatial information in the images.</p> model.py<pre><code>import torch\nfrom torch import nn\n\n\nclass MyAwesomeModel(nn.Module):\n    \"\"\"My awesome model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(128, 10)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\"\"\"\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv3(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        return self.fc1(x)\n\n\nif __name__ == \"__main__\":\n    model = MyAwesomeModel()\n    print(f\"Model architecture: {model}\")\n    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    dummy_input = torch.randn(1, 1, 28, 28)\n    output = model(dummy_input)\n    print(f\"Output shape: {output.shape}\")\n</code></pre> </li> <li> <p>Implement your data setup in a script called <code>data.py</code>. The data was saved using <code>torch.save</code>, so to load it you     should use <code>torch.load</code>.</p> <p>Saving the model</p> <p>When saving the model, you should use <code>torch.save(model.state_dict(), \"model.pt\")</code>, and when loading the model, you should use <code>model.load_state_dict(torch.load(\"model.pt\"))</code>. If you do <code>torch.save(model, \"model.pt\")</code>, this can lead to problems when loading the model later on, as it will try to not only save the model weights but also the model definition. This can lead to problems if you change the model definition later (which you are most likely going to do).</p> Starting point for <code>data.py</code> data.py<pre><code>import torch\n\n\ndef corrupt_mnist():\n    \"\"\"Return train and test dataloaders for corrupt MNIST.\"\"\"\n    # exchange with the corrupted mnist dataset\n    train = torch.randn(50000, 784)\n    test = torch.randn(10000, 784)\n    return train, test\n</code></pre> Solution <p>Data is stored in <code>.pt</code> files which can be loaded using <code>torch.load</code> (1). We iterate over the files, load them and concatenate them into a single tensor. In particular, we have highlighted the use of <code>.unsqueeze</code> function. Convolutional neural networks (which we propose as a solution) need the data to be in the shape <code>[N, C, H, W]</code> where <code>N</code> is the number of samples, <code>C</code> is the number of channels, <code>H</code> is the height of the image and <code>W</code> is the width of the image. The dataset is stored in the shape <code>[N, H, W]</code> and therefore we need to add a channel.</p> <ol> <li> The <code>.pt</code> files are nothing else than a <code>.pickle</code> file in disguise. The     <code>torch.save/torch.load</code> function is essentially a wrapper around the <code>pickle</code> module in Python, which     produces serialized files. However, it is convention to use <code>.pt</code> to indicate that the file contains PyTorch     tensors.</li> </ol> <p>We have additionally in the solution added functionality for plotting the images together with the labels for inspection. Remember: all good machine learning starts with a good understanding of the data.</p> data.py<pre><code>from __future__ import annotations\n\nimport matplotlib.pyplot as plt  # only needed for plotting\nimport torch\nfrom mpl_toolkits.axes_grid1 import ImageGrid  # only needed for plotting\n\nDATA_PATH = \"data/corruptmnist\"\n\n\ndef corrupt_mnist() -&gt; tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    \"\"\"Return train and test dataloaders for corrupt MNIST.\"\"\"\n    train_images, train_target = [], []\n    for i in range(6):\n        train_images.append(torch.load(f\"{DATA_PATH}/train_images_{i}.pt\"))\n        train_target.append(torch.load(f\"{DATA_PATH}/train_target_{i}.pt\"))\n    train_images = torch.cat(train_images)\n    train_target = torch.cat(train_target)\n\n    test_images: torch.Tensor = torch.load(f\"{DATA_PATH}/test_images.pt\")\n    test_target: torch.Tensor = torch.load(f\"{DATA_PATH}/test_target.pt\")\n\n    train_images = train_images.unsqueeze(1).float()\n    test_images = test_images.unsqueeze(1).float()\n    train_target = train_target.long()\n    test_target = test_target.long()\n\n    train_set = torch.utils.data.TensorDataset(train_images, train_target)\n    test_set = torch.utils.data.TensorDataset(test_images, test_target)\n\n    return train_set, test_set\n\n\ndef show_image_and_target(images: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"Plot images and their labels in a grid.\"\"\"\n    row_col = int(len(images) ** 0.5)\n    fig = plt.figure(figsize=(10.0, 10.0))\n    grid = ImageGrid(fig, 111, nrows_ncols=(row_col, row_col), axes_pad=0.3)\n    for ax, im, label in zip(grid, images, target):\n        ax.imshow(im.squeeze(), cmap=\"gray\")\n        ax.set_title(f\"Label: {label.item()}\")\n        ax.axis(\"off\")\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    train_set, test_set = corrupt_mnist()\n    print(f\"Size of training set: {len(train_set)}\")\n    print(f\"Size of test set: {len(test_set)}\")\n    print(f\"Shape of a training point {(train_set[0][0].shape, train_set[0][1].shape)}\")\n    print(f\"Shape of a test point {(test_set[0][0].shape, test_set[0][1].shape)}\")\n    show_image_and_target(train_set.tensors[0][:25], train_set.tensors[1][:25])\n</code></pre> </li> <li> <p>Implement training and evaluation of your model in the <code>main.py</code> script. The <code>main.py</code> script should be able to take     additional subcommands indicating if the model is being trained or evaluated. It will look something like this:</p> <pre><code>python main.py train --lr 1e-4\npython main.py evaluate model.pth\n</code></pre> <p>which can be implemented in various ways. We provide you with a starting script that uses the <code>typer</code> library to define a command line interface (CLI), which you can learn more about in this module later in the course.</p> VS Code and command line arguments <p>If you try to execute the above code in VS Code using the debugger (F5) or the build run functionality in the upper right corner:</p> <p> </p> <p>you will get an error message saying that you need to select a command to run e.g. <code>main.py</code> either needs the <code>train</code> or <code>evaluate</code> command. This can be fixed by adding a <code>launch.json</code> to a specialized <code>.vscode</code> folder in the root of the project. The <code>launch.json</code> file should look something like this:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Train\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"args\": [\n                \"train\",\n                \"--lr\",\n                \"1e-4\"\n            ],\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": true\n        }\n    ]\n}\n</code></pre> <p>This will inform VS Code that when we execute the current file (in this case <code>main.py</code>), we want to run it with the <code>train</code> command and additionally pass the <code>--lr</code> argument with the value <code>1e-4</code>. You can read more about creating a <code>launch.json</code> file here. If you want to have multiple configurations you can add them to the <code>configurations</code> list as additional dictionaries.</p> Starting point for <code>main.py</code> main.py<pre><code>import torch\nimport typer\nfrom data_solution import corrupt_mnist\nfrom model import MyAwesomeModel\n\napp = typer.Typer()\n\n\n@app.command()\ndef train(lr: float = 1e-3) -&gt; None:\n    \"\"\"Train a model on MNIST.\"\"\"\n    print(\"Training day and night\")\n    print(lr)\n\n    # TODO: Implement training loop here\n    model = MyAwesomeModel()\n    train_set, _ = corrupt_mnist()\n\n\n@app.command()\ndef evaluate(model_checkpoint: str) -&gt; None:\n    \"\"\"Evaluate a trained model.\"\"\"\n    print(\"Evaluating like my life depends on it\")\n    print(model_checkpoint)\n\n    # TODO: Implement evaluation logic here\n    model = torch.load(model_checkpoint)\n    _, test_set = corrupt_mnist()\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> Solution <p>The solution implements a simple training loop and evaluation loop. Furthermore, we have added additional hyperparameters that can be passed to the training loop. Highlighted in the solution are the different lines where we ensure that our model and data are moved to the GPU (or Apple MPS accelerator if you have a newer Mac) if available.</p> main.py<pre><code>import matplotlib.pyplot as plt\nimport torch\nimport typer\nfrom data_solution import corrupt_mnist\nfrom model_solution import MyAwesomeModel\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\napp = typer.Typer()\n\n\n@app.command()\ndef train(lr: float = 1e-3, batch_size: int = 32, epochs: int = 10) -&gt; None:\n    \"\"\"Train a model on MNIST.\"\"\"\n    print(\"Training day and night\")\n    print(f\"{lr=}, {batch_size=}, {epochs=}\")\n\n    model = MyAwesomeModel().to(DEVICE)\n    train_set, _ = corrupt_mnist()\n\n    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    statistics = {\"train_loss\": [], \"train_accuracy\": []}\n    for epoch in range(epochs):\n        model.train()\n        for i, (img, target) in enumerate(train_dataloader):\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(img)\n            loss = loss_fn(y_pred, target)\n            loss.backward()\n            optimizer.step()\n            statistics[\"train_loss\"].append(loss.item())\n\n            accuracy = (y_pred.argmax(dim=1) == target).float().mean().item()\n            statistics[\"train_accuracy\"].append(accuracy)\n\n            if i % 100 == 0:\n                print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n\n    print(\"Training complete\")\n    torch.save(model.state_dict(), \"model.pth\")\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n    axs[0].plot(statistics[\"train_loss\"])\n    axs[0].set_title(\"Train loss\")\n    axs[1].plot(statistics[\"train_accuracy\"])\n    axs[1].set_title(\"Train accuracy\")\n    fig.savefig(\"training_statistics.png\")\n\n\n@app.command()\ndef evaluate(model_checkpoint: str) -&gt; None:\n    \"\"\"Evaluate a trained model.\"\"\"\n    print(\"Evaluating like my life depended on it\")\n    print(model_checkpoint)\n\n    model = MyAwesomeModel().to(DEVICE)\n    model.load_state_dict(torch.load(model_checkpoint))\n\n    _, test_set = corrupt_mnist()\n    test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=32)\n\n    model.eval()\n    correct, total = 0, 0\n    for img, target in test_dataloader:\n        img, target = img.to(DEVICE), target.to(DEVICE)\n        y_pred = model(img)\n        correct += (y_pred.argmax(dim=1) == target).float().sum().item()\n        total += target.size(0)\n    print(f\"Test accuracy: {correct / total}\")\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> </li> <li> <p>As documentation that your model is working when running the <code>train</code> command, the script needs to produce a single     plot with the training curve (training step vs. training loss). When the <code>evaluate</code> command is run, it should write     the test set accuracy to the terminal.</p> </li> </ol> <p>It is part of the exercise not to implement this in notebooks, as code development in real life happens in scripts. As the model is simple to run (for now), you should be able to complete the exercise on your laptop, even if you are only training on CPU. That said, you are allowed to upload your scripts to your own \"Google Drive\" and then you can call your scripts from a Google Colab notebook, which is shown in the image below where all code is placed in the <code>fashion_trainer.py</code> script and the Colab notebook is just used to execute it.</p> <p></p> <p>Be sure to have completed the final exercise before the next session, as we will be building on top of the model you have created.</p>"},{"location":"s1_development_environment/editor/","title":"M3 - Editor","text":""},{"location":"s1_development_environment/editor/#editoride","title":"Editor/IDE","text":"<p>Core Module</p> <p>Notebooks are useful for prototyping, developing simple code, and explaining/visualizing aspects of a codebase. However, larger machine learning projects require working with multiple <code>.py</code> files, making notebooks a suboptimal workflow. Therefore, a good editor/IDE is essential for efficient development.</p> <p>Many opinions exist on this matter, but for simplicity, we recommend getting started with one of the following three:</p> Editor Webpage Comment (Biased opinion) Spyder https://www.spyder-ide.org/ A Matlab-like environment that is easy to get started with Visual Studio Code https://code.visualstudio.com/ Support for multiple languages with fairly easy setup PyCharm https://www.jetbrains.com/pycharm/ An IDE for Python professionals. Will take a bit of time to get used to <p>We recommend Visual Studio (VS) Code if you don't have an editor installed or want to try something new. The following sections will focus on explaining VS Code.</p> <p>Below, you can see an overview of the VS Code interface.</p> <p></p>  Image credit  <p>The main components of VS Code are:</p> <ul> <li> <p>The action bar: VS Code is not an editor meant for any one language and can do many things. One of the core reasons     that VS Code has become so popular is that custom plug-ins called extensions can be installed to add     functionality to VS Code. It is in the action bar that you can navigate between these different applications     when you have installed them.</p> </li> <li> <p>The sidebar: The sidebar has different functionality depending on what extension you have open.     In most cases, the sidebar will just contain the file explorer.</p> </li> <li> <p>The editor: This is where your code is. VS Code supports several layouts in the editor (one column, two columns,     etc.). You can make a custom layout by dragging a file to where you want the layout to split.</p> </li> <li> <p>The panel: The panel contains a terminal for you to interact with. This can quickly be used to try out code by     opening a <code>python</code> interpreter, management of environments, etc.</p> </li> <li> <p>The status bar: The status bar contains information based on the extensions you have installed. In particular,     for Python development, the status bar can be used to change the conda environment.</p> </li> </ul>"},{"location":"s1_development_environment/editor/#exercises","title":"\u2754 Exercises","text":"<p>The overall goal of the exercises is that you should start familiarizing yourself with the editor you have chosen. If you are already an expert in one of them, feel free to skip the rest. You should at least be able to:</p> <ul> <li>Create a new file</li> <li>Run a Python script</li> <li>Change the Python environment</li> </ul> <p>The instructions below are specific to Visual Studio Code, but we recommend that you try to answer the questions if using another editor. In the <code>exercise_files</code> folder belonging to this session, we have put cheat sheets for VS Code (one for Windows and one for Mac/Linux) that can give you an easy overview of the different macros in VS Code. The following exercises are just to get you started, but you can find many more tutorials here.</p> <ol> <li> <p>VS Code is a versatile editor that supports many languages. For Python development, install the following extensions:</p> <ul> <li>Python: general Python support for VS Code</li> <li>Pylance: language server for     Python that provides better code completion and type-checking</li> <li>Jupyter: support for Jupyter notebooks     directly in VS Code</li> <li>Python Environments:     allows for easy management of virtual environments</li> </ul> </li> <li> <p>If you install the <code>Python</code> package, you should see something like this in your status bar:</p> <p> </p> <p>which indicates that you are using the stock Python installation instead of the one you have created using <code>conda</code>. Click it and change the Python environment to the one you want to use.</p> </li> <li> <p>One of the most useful tools in VS Code is the built-in <code>Explorer</code>, which allows you to navigate the entire project.     To take advantage of VS Code, make sure that what you are working on is a project. Create a folder called <code>hello</code>     (somewhere on your laptop) and open it in VS Code (Click <code>File</code> -&gt; <code>Open Folder</code>). You should end up with a     completely clean workspace (as shown below). Click the <code>New file</code> button and create a file called <code>hello.py</code>.</p> <p>  Image credit  </p> </li> <li> <p>Finally, let's run some code. Add something simple to the <code>hello.py</code> file like:</p> <p>  Image credit  </p> <p>and click the <code>run</code> button as shown in the image. It should create a new terminal, activate the environment that you have chosen, and finally run your script. In addition to clicking the <code>run</code> button, you can also:</p> <ul> <li>Select some code and press <code>Shift+Enter</code> to run it in the terminal</li> <li>Select some code and right-click, choosing to run it in an interactive window (where you can interact with the     results like in a Jupyter Notebook)</li> </ul> </li> </ol> <p>That's the basics of using VS Code. We highly recommend that you revisit this tutorial during the course when we get to topics such as debugging and version control, which VS Code can help with. We can also recommend this blog post that goes over some good extensions for AI/ML development in VS Code.</p>"},{"location":"s1_development_environment/editor/#a-note-on-jupyter-notebooks-in-production-environments","title":"A note on Jupyter notebooks in production environments","text":"<p>As already stated, Jupyter Notebooks are great for development as they allow developers to easily test out new ideas. However, they often lead to pain points when models need to be deployed. We highly recommend reading section 5.1.1 of this paper by Shankar et al. which in more detail discusses the strong opinions on Jupyter notebooks that exist within the developer community.</p> <p>All this said, there exists one simple tool to make notebooks work better in a production setting. It's called <code>nbconvert</code> and can be installed with</p> <pre><code>pip install nbconvert\n</code></pre> <p>You may need some further dependencies such as Pandoc, TeX and Pyppeteer for it to work (see install instructions here). After this, converting a notebook to a <code>.py</code> script is as simple as:</p> <pre><code>jupyter nbconvert --to=script my_notebook.ipynb\n</code></pre> <p>which will produce a similarly named script called <code>my_notebook.py</code>. We highly recommend that you stick to developing scripts directly during the course to get experience with doing so, but <code>nbconvert</code> can be a fantastic tool to have in your toolbox.</p>"},{"location":"s1_development_environment/editor/#ai-assistance","title":"AI assistance","text":"<p>You are probably all familiar with using AI tools for solving different tasks in your daily life, and you have most likely also used AI tools like ChatGPT or similar for programming. However, most of these tools are not directly integrated into your editor, which can lead to a lot of context switching that in general leads to lower productivity.</p> <p>We are therefore in this section going to be looking at GitHub Copilot, which is an AI tool that directly integrates into your editor, eliminating the need to switch between browser tabs or external tools. In addition, the strength of having AI directly in your editor is that it can provide suggestions based on the code you are currently writing and in general just has access to a larger context than a standalone tool.</p>"},{"location":"s1_development_environment/editor/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>As of writing this, GitHub Copilot is free for all students, teachers and maintainers of popular open-source     projects. As a student, sign up for the Student Developer Pack.</p> </li> <li> <p>Install the GitHub Copilot extension in your     editor.</p> </li> <li> <p>GitHub Copilot has many different features, but the most important one is the ability to provide suggestions based     on the code you are currently writing. Try to write some code in a new Python file and see if you can get some     suggestions from GitHub Copilot on how to complete the code. If you have no idea what to try out here is a     simple example of starting out coding a neural network in PyTorch:</p> <pre><code>import torch\nfrom torch import nn\nclass Net(nn.Module):\n</code></pre> <p>GitHub Copilot will most likely suggest that you complete the code using linear layers with an input dimension of <code>28*28</code>. Can you explain why it suggests this and where this bias comes from?</p> </li> <li> <p>The second feature that can be very useful is the ability to directly chat or ask questions about     your code. Try highlighting (in your code editor) the code from the previous exercise and press <code>Ctrl+i</code> which     should open a chat window. Ask it to complete it with a convolutional neural network instead of a linear one.</p> <p> </p> </li> <li> <p>Finally, let's try the built-in chat feature. You can get to this by clicking the <code>Chat</code> icon in the Activity bar and     begin to ask questions similar to how you would ask ChatGPT. However, we also have the option of providing context     either from the code editor or the terminal. Try saving the following code in a Python script called <code>copilot.py</code>:</p> <pre><code>import torch\nfrom torch import nn\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28*28, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 10)\n    def forward(self, x):\n        x = x.view(-1, 28*28)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = Net()\nprint(model(torch.randn(1, 1, 14, 14)))\n</code></pre> <p>and run it in the terminal: <code>python copilot.py</code>. It will naturally give you an error, but you can now ask GitHub Copilot for help. The easiest way to do this is by highlighting the output in the terminal and then running the <code>GitHub Copilot: Explain This (Terminal)</code> command (see the image below, use <code>Ctrl+Shift+P</code> to open the command palette and search for the command). Does the explanation make sense e.g. can you figure out what to change to get the code running?</p> <p> </p> </li> <li> <p>(Optional) Just to investigate the difference between using GitHub Copilot and ChatGPT, try to redo the previous     exercises using ChatGPT. What are the main differences between the two tools? (1)</p> <ol> <li> Remember that ChatGPT is a general AI model, meaning that it was trained to be good at many     tasks, whereas GitHub Copilot (which uses OpenAI's Codex model under the hood) was specifically trained to be     good at coding.</li> </ol> </li> </ol> <p>That was a small introduction to GitHub Copilot. We highly recommend that you try to use it during the course to see how it can help you solve both the exercises and the final project. However, when using AI tools it is always important to remember that they are not perfect and that you need to critically evaluate the suggestions they provide. In the end, you are the one responsible for the code you write, not the AI tool.</p>"},{"location":"s1_development_environment/package_manager/","title":"M2 - Package Manager","text":""},{"location":"s1_development_environment/package_manager/#package-managers-and-virtual-environments","title":"Package managers and virtual environments","text":"<p>Core Module</p> <p>Python's extensive package ecosystem is a major strength. It's rare to write a program relying solely on the Python standard library. Therefore, package managers are essential for installing third-party packages.</p> <p>You've likely used <code>pip</code>, the default Python package manager. While suitable for beginners, <code>pip</code> lacks a crucial feature for developers and data scientists: virtual environments. Virtual environments prevent dependency conflicts between projects. For example, if project A requires <code>torch==1.3.0</code> and project B requires <code>torch==2.0</code>, the following scenario illustrates the problem:</p> <pre><code>cd project_A  # move to project A\npip install torch==1.3.0  # install old torch version\ncd ../project_B  # move to project B\npip install torch==2.0  # install new torch version\ncd ../project_A  # move back to project A\npython main.py  # try executing main script from project A\n</code></pre> <p>will mean that even though we are executing the main script from project A's folder, it will use <code>torch==2.0</code> instead of <code>torch==1.3.0</code> because that is the last version we installed. In both cases, <code>pip</code> will install the package into the same environment, in this case, the global environment. Instead, if we did something like:</p> Unix/macOSWindows <pre><code>cd project_A  # move to project A\npython -m venv env  # create a virtual environment in project A\nsource env/bin/activate  # activate that virtual environment\npip install torch==1.3.0  # Install the old torch version into the virtual environment belonging to project A\ncd ../project_B  # move to project B\npython -m venv env  # create a virtual environment in project B\nsource env/bin/activate  # activate that virtual environment\npip install torch==2.0  # Install new torch version into the virtual environment belonging to project B\ncd ../project_A  # Move back to project A\nsource env/bin/activate  # Activate the virtual environment belonging to project A\npython main.py  # Succeed in executing the main script from project A\n</code></pre> <pre><code>cd project_A  # Move to project A\npython -m venv env  # Create a virtual environment in project A\n.\\\\env\\\\Scripts\\\\activate  # Activate that virtual environment\npip install torch==1.3.0  # Install the old torch version into the virtual environment belonging to project A\ncd ../project_B  # Move to project B\npython -m venv env  # Create a virtual environment in project B\n.\\\\env\\\\Scripts\\\\activate  # Activate that virtual environment\npip install torch==2.0  # Install new torch version into the virtual environment belonging to project B\ncd ../project_A  # Move back to project A\n.\\\\env\\\\Scripts\\\\activate  # Activate the virtual environment belonging to project A\npython main.py  # Succeed in executing the main script from project A\n</code></pre> <p>then we would be sure that <code>torch==1.3.0</code> is used when executing <code>main.py</code> in project A because we are using two different virtual environments. In the above case, we used the venv module, which is the built-in Python module for creating virtual environments. <code>venv+pip</code> is arguably a good combination, but when working on multiple projects it can quickly become a hassle to manage all the different virtual environments yourself, remembering which Python version to use, which packages to install and so on.</p> <p>Therefore, several package managers have been developed to manage virtual environments and dependencies. Some popular options include:</p> \ud83c\udf1f Framework \ud83d\udcc4 Docs \ud83d\udcc2 Repository \u2b50 GitHub Stars Conda \ud83d\udd17 Link \ud83d\udd17 Link 7.0k Pipenv \ud83d\udd17 Link \ud83d\udd17 Link 25.1k Poetry \ud83d\udd17 Link \ud83d\udd17 Link 33.4k Pipx \ud83d\udd17 Link \ud83d\udd17 Link 11.8k Hatch \ud83d\udd17 Link \ud83d\udd17 Link 6.7k PDM \ud83d\udd17 Link \ud83d\udd17 Link 8.4k uv \ud83d\udd17 Link \ud83d\udd17 Link 59.9k <p>The lack of a standard dependency management approach, unlike <code>npm</code> for <code>node.js</code> or <code>cargo</code> for <code>rust</code>, is a known issue in the Python community. However, <code>uv</code> is gaining popularity and may become a standard.</p> <p></p>  Image credit  <p>This course doesn't mandate a specific package manager, but using one is essential. If you're already familiar with a package manager, continue using it. The best approach is to choose one you like and stick with it. While it's tempting to find the \"perfect\" package manager, they all accomplish the same goal with minor differences. For a recent comparison of Python environment management and packaging tools, see this blog post.</p> <p>If you're new to package managers, we recommend using <code>conda</code> and <code>pip</code> for this course. You may already have conda installed. Conda excels at creating virtual environments with different Python versions, which is helpful for managing dependencies that haven't been updated recently. Specifically, we recommend the following workflow:</p> <ul> <li>Use <code>conda</code> to create virtual environments with specific Python versions</li> <li>Use <code>pip</code> to install packages in that environment</li> </ul> <p>Installing packages with <code>pip</code> inside <code>conda</code> environments was once discouraged, but it's now safe with <code>conda&gt;=4.6</code>. Conda includes a compatibility layer that ensures <code>pip</code>-installed packages are compatible with other packages in the environment.</p>"},{"location":"s1_development_environment/package_manager/#python-dependencies","title":"Python dependencies","text":"<p>Before we get started with the exercises, let's first talk a bit about Python dependencies. One of the most common ways to specify dependencies in the Python community is through a <code>requirements.txt</code> file, which is a simple text file that contains a list of all the packages that you want to install. The format allows you to specify the package name and version number you want, with 7 different operators:</p> <pre><code>package1           # any version\npackage2 == x.y.z  # exact version\npackage3 &gt;= x.y.z  # at least version x.y.z\npackage4 &gt;  x.y.z  # newer than version x.y.z\npackage4 &lt;= x.y.z  # at most version x.y.z\npackage5 &lt;  x.y.z  # older than version x.y.z\npackage6 ~= x.y.z  # install version newer than x.y.z and older than x.y+1\n</code></pre> <p>In general, all packages (should) follow the semantic versioning standard, which means that the version number is split into three parts: <code>x.y.z</code> where <code>x</code> is the major version, <code>y</code> is the minor version and <code>z</code> is the patch version.</p> <p>Specifying version numbers ensures code reproducibility. Without version numbers, you risk API changes by package maintainers. This is especially important in machine learning, where reproducing the exact same model is crucial.</p> <p>Finally, we also need to discuss dependency resolution, which is the process of figuring out which packages are compatible. This is a complex problem with various algorithms. If <code>pip</code> or <code>conda</code> take a long time to install packages, it's likely due to the dependency resolution process. For example, attempting to install</p> <pre><code>pip install \"matplotlib &gt;= 3.8.0\" \"numpy &lt;= 1.19\" --dry-run\n</code></pre> <p>then it would simply fail because there are no versions of <code>matplotlib</code> and <code>numpy</code> under the given constraints that are compatible with each other. In this case, we would need to relax the constraints to something like</p> <pre><code>pip install \"matplotlib &gt;= 3.8.0\" \"numpy &lt;= 1.21\" --dry-run\n</code></pre> <p>to make it work.</p>"},{"location":"s1_development_environment/package_manager/#exercises","title":"\u2754 Exercises","text":"<p>Conda vs. Mamba</p> <p>If you are using <code>conda</code> then you can also use <code>mamba</code> which is a drop-in replacement <code>conda</code> that is faster. This means that any <code>conda</code> command can be replaced with <code>mamba</code> and it should work. Feel free to use <code>mamba</code> if you are already familiar with <code>conda</code> or after having gone through the exercises below. Install instructions can be found here.</p> <p>For guidance on using <code>conda</code>, refer to the cheat sheet in the exercise folder.</p> <ol> <li> <p>Download and install <code>conda</code>. You are free to either install full <code>conda</code> or the much simpler version <code>miniconda</code>.     Conda includes many pre-installed packages, while Miniconda is a smaller, minimal installation. Conda's larger size     can be a disadvantage on smaller devices. Verify your installation by running <code>conda help</code> in a terminal; it should     display the conda help message. If it doesn't work, you may need to configure a system variable to point to the     conda installation.</p> </li> <li> <p>If you have successfully installed conda, then you should be able to execute the <code>conda</code> command in a terminal.</p> <p> </p> <p>Conda will always tell you what environment you are currently in, indicated by the <code>(env_name)</code> in the prompt. By default, it will always start in the <code>(base)</code> environment.</p> </li> <li> <p>Try creating a new virtual environment. Make sure that it is called <code>my_environment</code> and that it installs version    3.11 of Python. What command should you execute to do this?</p> Use Python 3.8 or higher <p>We recommend using Python 3.8 or higher for this course. Generally, using the second latest Python version (currently 3.12) is advisable, as the newest version may lack support from all dependencies. Check the status of different Python versions here.</p> Solution <pre><code>conda create --name my_environment python=3.11\n</code></pre> </li> <li> <p>Which <code>conda</code> command gives you a list of all the environments that you have created?</p> Solution <pre><code>conda env list\n</code></pre> </li> <li> <p>Which <code>conda</code> command gives you a list of the packages installed in the current environment?</p> Solution <pre><code>conda list\n</code></pre> <ol> <li> <p>How do you easily export this list to a text file? Do this, and make sure you export it to     a file called <code>environment.yaml</code>, as conda uses another format by default than <code>pip</code>.</p> Solution <pre><code>conda list --explicit &gt; environment.yaml\n</code></pre> </li> <li> <p>Inspect the file to see what is in it.</p> </li> <li> <p>The <code>environment.yaml</code> file you have created is one way to secure reproducibility between users because     anyone should be able to get an exact copy of your environment if they have your <code>environment.yaml</code> file.     Try creating a new environment directly from your <code>environment.yaml</code> file and check that the packages being     installed exactly match what you originally had.</p> Solution <pre><code>conda env create --name &lt;environment-name&gt; --file environment.yaml\n</code></pre> </li> </ol> </li> <li> <p>As the introduction states, it is fairly safe to use <code>pip</code> inside <code>conda</code> today. What is the corresponding <code>pip</code>     command that gives you a list of all <code>pip</code> installed packages? And how do you export this to a <code>requirements.txt</code>     file?</p> Solution <pre><code>pip list # List all installed packages\npip freeze &gt; requirements.txt # Export all installed packages to a requirements.txt file\n</code></pre> </li> <li> <p>If you look through the requirements that both <code>pip</code> and <code>conda</code> produce, you will see that they     are often filled with a lot more packages than what you are using in your project. What you are interested in are     the packages that you import in your code: <code>from package import module</code>. One way to get around this is to use the     package <code>pipreqs</code>, which will automatically scan your project and create a requirements file specific to that.     Let's try it out:</p> <ol> <li> <p>Install <code>pipreqs</code>:</p> <pre><code>pip install pipreqs\n</code></pre> </li> <li> <p>Either try out <code>pipreqs</code> on one of your own projects or try it out on some other online project.     What does the <code>requirements.txt</code> file that <code>pipreqs</code> produces look like compared to the files produced     by either <code>pip</code> or <code>conda</code>?</p> </li> </ol> </li> </ol>"},{"location":"s1_development_environment/package_manager/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>Try executing the command</p> <pre><code>pip install \"pytest &lt; 4.6\" pytest-cov==2.12.1\n</code></pre> <p>based on the error message you get, what would be a compatible way to install these?</p> Solution <p>As <code>pytest-cov==2.12.1</code> requires a version of <code>pytest</code> newer than <code>4.6</code>, we can simply change the command to be:</p> <pre><code>pip install \"pytest &gt;= 4.6\" pytest-cov==2.12.1\n</code></pre> <p>but there of course exist other solutions as well.</p> </li> </ol> <p>This ends the module on setting up virtual environments. While the methods mentioned in the exercises are great ways to construct requirement files automatically, sometimes it is just easier to sit down and manually create the files, as you in that way ensure that only the most necessary requirements are installed when creating a new environment.</p>"},{"location":"s2_organisation_and_version_control/","title":"Organization and version control","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn the basics of version control and how to use <code>git</code> to track changes in your code and collaborate with others.</p> <p> M5: Git</p> </li> <li> <p></p> <p>Learn how to organize Python code into a library, package it and use templates to create new projects.</p> <p> M6: Code Structure</p> </li> <li> <p></p> <p>Learn different coding practices and how to use them to improve the quality of your code.</p> <p> M7: Good Coding Practice</p> </li> <li> <p></p> <p>Learn how to version control data using <code>dvc</code>.</p> <p> M8: Data Version Control</p> </li> <li> <p></p> <p>Learn the different ways to setup command line interfaces for your applications.</p> <p> M9: Command Line Interfaces</p> </li> </ul> <p>Today we take our first steps into the world of MLOps. The set of modules in this session focuses on getting organized and making sure that you are familiar with good development practices. While many of the practices you will learn about in these modules do not seem that important when you are a single person working on a project, it becomes crucial when working in large groups that the difference in how different people organize and write their code is minimized. The topics in this session will focus on:</p> <ul> <li>Version control to help track and manage changes to your code and data</li> <li>Coding practices for staying organized in large projects</li> </ul> <p></p>  Image credit  <p>Some exercises in this course are very loosely stated (including the exercises today). You are expected to seek out information before you ask for help (Google is your friend!) as you will both learn more from trying to solve the problems yourself, and it is more realistically how the \"real world\" works.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the basics of version control and be able to use <code>git</code> to track changes to your code</li> <li>Know how to package Python code into a library and how to organize your code for reuse</li> <li>Understand different coding practices and how to use them to improve the quality of your code</li> <li>Be able to use <code>dvc</code> to version control data</li> </ul>"},{"location":"s2_organisation_and_version_control/cli/","title":"M9 - Command Line Interfaces","text":""},{"location":"s2_organisation_and_version_control/cli/#command-line-interfaces","title":"Command line interfaces","text":"<p>As discussed in the initial module, the command line offers a robust interface for interacting with your computer. You should already be comfortable executing basic Python commands in the terminal:</p> <pre><code>python my_script.py\n</code></pre> <p>However, as projects increase in size and complexity, more sophisticated methods of interacting with your code become necessary. This is where a command line interface (CLI) becomes useful. A CLI allows you to define the user interface of your application directly in the terminal, and the best approach depends on the specific needs of your application.</p> <p>In this module, we will explore three distinct methods for creating CLIs for your machine learning projects. Each method serves a slightly different purpose, and they can be combined within the same project. You may find some overlap between them, which is perfectly acceptable. The choice of which method to use depends on your specific requirements.</p>"},{"location":"s2_organisation_and_version_control/cli/#project-scripts","title":"Project scripts","text":"<p>You may already be familiar with executable scripts. An executable script is a Python script that can be run directly from the terminal without needing to call the Python interpreter. This has been possible in Python for a long time, using a shebang line at the top of the script. However, we will explore a specific method for defining executable scripts using the standard <code>pyproject.toml</code> file, as covered in this module.</p>"},{"location":"s2_organisation_and_version_control/cli/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>We are going to assume that you have a training script in your project that you would like to be able to run from the     terminal directly without having to call the Python interpreter. Lets assume it is located like this:</p> <pre><code>src/\n\u251c\u2500\u2500 my_project/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 train.py\npyproject.toml\n</code></pre> <p>In your <code>pyproject.toml</code> file, add the following lines, adjusting the paths to match your project structure:</p> <pre><code>[project.scripts]\ntrain = \"my_project.train:main\"\n</code></pre> <p>what do you think the <code>train = \"my_project.train:main\"</code> line does?</p> Solution <p>The line instructs Python to create an executable script named <code>train</code> that executes the <code>main</code> function within the <code>train.py</code> file, located in the <code>my_project</code> package.</p> </li> <li> <p>Now, all that is left to do is to install the project again in editable mode.</p> <pre><code>pip install -e .\n</code></pre> <p>You should now be able to run the following command in the terminal:</p> <pre><code>train\n</code></pre> <p>Try it out and see if it works.</p> </li> <li> <p>Add additional commands to your <code>pyproject.toml</code> file that allow you to run other scripts in your project from the     terminal.</p> Solution <p>We assume that you also have a script called <code>evaluate.py</code> in the <code>my_project</code> package.</p> <pre><code>[project.scripts]\ntrain = \"my_project.train:main\"\nevaluate = \"my_project.evaluate:main\"\n</code></pre> </li> </ol> <p>That is all there really is to it. You can now run your scripts directly from the terminal without having to call the Python interpreter. Some good examples of Python packages that use this approach are numpy, pylint and kedro.</p>"},{"location":"s2_organisation_and_version_control/cli/#command-line-arguments","title":"Command line arguments","text":"<p>If you have experience with Python, you are likely familiar with the <code>argparse</code> package, which enables you to pass arguments directly to your script via the terminal.</p> <pre><code>python my_script.py --arg1 val1 --arg2 val2\n</code></pre> <p><code>argparse</code> is a very simple way of constructing what is called a command line interface. However, one limitation of <code>argparse</code> is that it is not easy to define a CLI with subcommands. If we take <code>git</code> as an example, <code>git</code> is the main command, but it has multiple subcommands: <code>push</code>, <code>pull</code>, <code>commit</code> etc. that can all take their own arguments. This kind of second CLI with subcommands is somewhat possible to do using only <code>argparse</code>, but it requires some hacks.</p> <p>You could of course ask the question why we at all want to have the possibility of defining such a CLI. The main argument here is to give users of our code a single entrypoint to interact with our application instead of having multiple scripts. As long as all subcommands are properly documented, then our interface should be simple to interact with (again think <code>git</code> where each subcommand can be given the <code>-h</code> arg to get specific help).</p> <p>Instead of using <code>argparse</code> we are here going to look at the typer package. <code>typer</code> extends the functionalities of <code>argparse</code> to allow for easy definition of subcommands and many other things, which we are not going to touch upon in this module. For completeness we should also mention that <code>typer</code> is not the only package for doing this, and another excellent framework for creating command line interfaces easily is click.</p>"},{"location":"s2_organisation_and_version_control/cli/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by installing the <code>typer</code> package</p> <pre><code>pip install typer\n</code></pre> <p>and remember to add the package to your <code>requirements.txt</code> file.</p> </li> <li> <p>To get you started with <code>typer</code>, let's just create a simple hello world type of script. Create a new Python file     called <code>greetings.py</code> and use the <code>typer</code> package to create a command line interface such that running the     following lines</p> <pre><code>python greetings.py            # should print \"Hello World!\"\npython greetings.py --count=3  # should print \"Hello World!\" three times\npython greetings.py --help     # should print the help message, informing the user of the possible arguments\n</code></pre> <p>executes and gives the expected output. Relevant documentation.</p> Solution <p>Important for <code>typer</code> is that you need to provide type hints for the arguments. This is because <code>typer</code> needs these to be able to work properly.</p> <pre><code>import typer\napp = typer.Typer()\n\n@app.command()\ndef hello(count: int = 1, name: str = \"World\"):\n    for x in range(count):\n        typer.echo(f\"Hello {name}!\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> </li> <li> <p>Next, let's try on a slightly harder example. Below is a simple script that trains a support vector machine on the     iris dataset.</p> <p>iris_classifier.py</p> iris_classifier.py<pre><code>from sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n\ndef train():\n    \"\"\"Train and evaluate the model.\"\"\"\n    # Load the dataset\n    data = load_breast_cancer()\n    x = data.data\n    y = data.target\n\n    # Split the dataset into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    # Standardize the features\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_test = scaler.transform(x_test)\n\n    # Train a Support Vector Machine (SVM) model\n    model = SVC(kernel=\"linear\", random_state=42)\n    model.fit(x_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(x_test)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    # Print the results\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(\"Classification Report:\")\n    print(report)\n    return accuracy, report\n\n\n# this \"if\"-block is added to enable the script to be run from the command line\nif __name__ == \"__main__\":\n    train()\n</code></pre> <p>Implement a CLI for the script such that the following commands can be run</p> <pre><code>python iris_classifier.py --output 'model.ckpt'  # should train the model and save it to 'model.ckpt'\npython iris_classifier.py -o 'model.ckpt'        # should be the same as above\n</code></pre> Solution <p>We are here making use of the short name option in typer for giving a shorter alias to the <code>--output</code> option.</p> iris_classifier.py<pre><code>import pickle\nfrom typing import Annotated\n\nimport typer\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\napp = typer.Typer()\n\n\n@app.command()\ndef train(output: Annotated[str, typer.Option(\"--output\", \"-o\")] = \"model.ckpt\"):\n    \"\"\"Train and evaluate the model.\"\"\"\n    # Load the dataset\n    data = load_breast_cancer()\n    x = data.data\n    y = data.target\n\n    # Split the dataset into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    # Standardize the features\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_test = scaler.transform(x_test)\n\n    # Train a Support Vector Machine (SVM) model\n    model = SVC(kernel=\"linear\", random_state=42)\n    model.fit(x_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = model.predict(x_test)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    with open(output, \"wb\") as f:\n        pickle.dump(model, f)\n\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(\"Classification Report:\")\n    print(report)\n    return accuracy, report\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> </li> <li> <p>Next let's create a CLI that has more than a single command. Continue working in the basic machine learning     application from the previous exercise, but this time we want to define two separate commands:</p> <pre><code>python iris_classifier.py train --output 'model.ckpt'\npython iris_classifier.py evaluate 'model.ckpt'\n</code></pre> Solution <p>The only key difference between the two is that in the <code>train</code> command we define the <code>output</code> argument to to be an optional parameter, i.e., we provide a default and for the <code>evaluate</code> command it is a required parameter.</p> iris_classifier.py<pre><code>import pickle\nfrom typing import Annotated\n\nimport typer\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\napp = typer.Typer()\n\n# Load the dataset\ndata = load_breast_cancer()\nx = data.data\ny = data.target\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n\n@app.command()\ndef train(output_file: Annotated[str, typer.Option(\"--output\", \"-o\")] = \"model.ckpt\") -&gt; None:\n    \"\"\"Train the model.\"\"\"\n    # Train a Support Vector Machine (SVM) model\n    model = SVC(kernel=\"linear\", random_state=42)\n    model.fit(x_train, y_train)\n\n    with open(output_file, \"wb\") as f:\n        pickle.dump(model, f)\n\n\n@app.command()\ndef evaluate(model_file):\n    \"\"\"Evaluate the model.\"\"\"\n    with open(model_file, \"rb\") as f:\n        model = pickle.load(f)\n\n    # Make predictions on the test set\n    y_pred = model.predict(x_test)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(\"Classification Report:\")\n    print(report)\n    return accuracy, report\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> </li> <li> <p>Finally, let's try to define subcommands for our subcommands, e.g., something similar to how <code>git</code> has the subcommand     <code>remote</code> which in itself has multiple subcommands like <code>add</code>, <code>rename</code>, etc. Continue on the simple machine     learning application from the previous exercises, but this time define a CLI for these commands:</p> <pre><code>python iris_classifier.py train svm --kernel 'linear'\npython iris_classifier.py train knn --n-neighbors 5\n</code></pre> <p>i.e., the <code>train</code> command now has two subcommands for training different machine learning models (in this case SVM and KNN) which each takes arguments that are unique to that model. Relevant documentation.</p> <p><code>_</code> vs <code>-</code></p> <p>When using typer note that variables with <code>_</code> in the name will be converted to <code>-</code> in the CLI. Meaning that if you have a variable <code>n_neighbors</code> in your code, you should use <code>--n-neighbors</code> in the CLI.</p> Success iris_classifier.py<pre><code>import pickle\nfrom typing import Annotated\n\nimport typer\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\napp = typer.Typer()\ntrain_app = typer.Typer()\napp.add_typer(train_app, name=\"train\")\n\n# Load the dataset\ndata = load_breast_cancer()\nx = data.data\ny = data.target\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n\n@train_app.command()\ndef svm(kernel: str = \"linear\", output_file: Annotated[str, typer.Option(\"--output\", \"-o\")] = \"model.ckpt\") -&gt; None:\n    \"\"\"Train a SVM model.\"\"\"\n    model = SVC(kernel=kernel, random_state=42)\n    model.fit(x_train, y_train)\n\n    with open(output_file, \"wb\") as f:\n        pickle.dump(model, f)\n\n\n@train_app.command()\ndef knn(n_neighbors: int = 5, output_file: Annotated[str, typer.Option(\"--output\", \"-o\")] = \"model.ckpt\") -&gt; None:\n    \"\"\"Train a KNN model.\"\"\"\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    model.fit(x_train, y_train)\n\n    with open(output_file, \"wb\") as f:\n        pickle.dump(model, f)\n\n\n@app.command()\ndef evaluate(model_file):\n    \"\"\"Evaluate the model.\"\"\"\n    with open(model_file, \"rb\") as f:\n        model = pickle.load(f)\n\n    # Make predictions on the test set\n    y_pred = model.predict(x_test)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(\"Classification Report:\")\n    print(report)\n    return accuracy, report\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> </li> <li> <p>(Optional) Let's try to combine what we have learned until now. Try to make your <code>typer</code> CLI into an executable     script using the <code>pyproject.toml</code> file and try it out!</p> Solution <p>Assuming that our <code>iris_classifier.py</code> script from before is placed in the <code>src/my_project</code> folder, we should just add</p> <pre><code>[project.scripts]\ngreetings = \"src.my_project.iris_classifier:app\"\n</code></pre> <p>and remember to install the project in editable mode</p> <pre><code>pip install -e .\n</code></pre> <p>and you should now be able to run the following command in the terminal</p> <pre><code>iris_classifier train knn\n</code></pre> </li> </ol> <p>This covers the basics of <code>typer</code> but feel free to deep dive into how the package can help you custimize your CLIs. Checkout this page on adding colors to your CLI or this page on validating the inputs to your CLI.</p>"},{"location":"s2_organisation_and_version_control/cli/#non-python-code","title":"Non-Python code","text":"<p>The two above sections have shown you how to create a simple CLI for your Python scripts. However, when doing machine learning projects, you often have a lot of non-Python code that you would like to run from the terminal. In the learning modules you have already completed, you have already encountered a couple of CLI tools that are used in our projects:</p> <ul> <li>conda for managing environments</li> <li>git for version control of code</li> <li>dvc for version control of data</li> </ul> <p>As we begin to move into the next couple of learning modules, we are going to encounter even more CLI tools that we need to interact with. Here is an example of a long command that you might need to run in your project in the future:</p> <pre><code>docker run -v $(pwd):/app -w /app --gpus all --rm -it my_image:latest python my_script.py --arg1 val1 --arg2 val2\n</code></pre> <p>This can be a lot to remember, and it can be easy to make mistakes. Instead, it would be nice if we could just do</p> <pre><code>run my_command --arg1=val1 --arg2=val2\n</code></pre> <p>i.e. easier to remember because we have removed a lot of the hard-to-remember stuff, but we are still able to configure it to our liking. To help with this, we are going to look at the invoke package. <code>invoke</code> is a Python package that allows you to define tasks that can be run from the terminal. It is a bit like a more advanced version of the Makefile that you might have encountered in other programming languages. Some good alternatives to <code>invoke</code> are just and task, but we have chosen to focus on <code>invoke</code> in this module because it can be installed as a Python package, making installation across different systems easier.</p>"},{"location":"s2_organisation_and_version_control/cli/#exercises_2","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by installing <code>invoke</code></p> <pre><code>pip install invoke\n</code></pre> <p>and remember to add the package to your <code>requirements.txt</code> file.</p> </li> <li> <p>Add a <code>tasks.py</code> file to your repository and try to just run</p> <pre><code>invoke --list\n</code></pre> <p>which should work but inform you that no tasks have been added yet.</p> </li> <li> <p>Let's now try to add a task to the <code>tasks.py</code> file. The way to do this with invoke is to import the <code>task</code>     decorator from <code>invoke</code> and then decorate a function with it:</p> <pre><code>from invoke import task\nimport os\n\n@task\ndef python(ctx):\n    \"\"\" \"\"\"\n    ctx.run(\"which python\" if os.name != \"nt\" else \"where python\")\n</code></pre> <p>the first argument of any task-decorated function is the <code>ctx</code> context argument that implements the <code>run</code> method for running any command as we would run it in the terminal. In this case we have simply implemented a task that returns the current Python interpreter, but it works for all operating systems. Check that it works by running:</p> <pre><code>invoke python\n</code></pre> </li> <li> <p>Let's try to create a task that simplifies the process of <code>git add</code>, <code>git commit</code>, <code>git push</code>. Create a task such     that the following command can be run</p> <pre><code>invoke git --message \"My commit message\"\n</code></pre> <p>Implement it and use the command to commit the taskfile you just created!</p> Solution <pre><code>@task\ndef git(ctx, message):\n    ctx.run(f\"git add .\")\n    ctx.run(f\"git commit -m '{message}'\")\n    ctx.run(f\"git push\")\n</code></pre> </li> <li> <p>As you have hopefully realized by now, the most important method in <code>invoke</code> is the <code>ctx.run</code> method which actually     run the commands you want to run in the terminal. This command takes multiple additional arguments. Try out the     arguments <code>warn</code>, <code>pty</code>, <code>echo</code> and explain in your own words what they do.</p> Solution <ul> <li><code>warn</code>: If set to <code>True</code> the command will not raise an exception if the command fails. This can be useful if     you want to run multiple commands and you do not want the whole process to stop if one of the commands fails.</li> <li><code>pty</code>: If set to <code>True</code> the command will be run in a pseudo-terminal. Whether you want to enable this or not     depends on the command you are running.     Here     is a good explanation of when/why you should use it.</li> <li><code>echo</code>: If set to <code>True</code> the command will be printed to the terminal before it is run.</li> </ul> </li> <li> <p>Create a command that simplifies the process of bootstrapping a <code>conda</code> environment and installs the relevant     dependencies of your project.</p> Solution <pre><code>@task\ndef conda(ctx, name: str = \"dtu_mlops\"):\n    ctx.run(f\"conda env create -f environment.yml\", echo=True)\n    ctx.run(f\"conda activate {name}\", echo=True)\n    ctx.run(f\"pip install -e .\", echo=True)\n</code></pre> <p>and try to run the following command</p> <pre><code>invoke conda\n</code></pre> </li> <li> <p>Assuming you have completed the exercises on using dvc for version control of data, let's also try to add     a task that simplifies the process of adding new data. This is the list of commands that need to be run to add new     data to a dvc repository: <code>dvc add</code>, <code>git add</code>, <code>git commit</code>, <code>git push</code>, <code>dvc push</code>. Try to implement a task     that simplifies this process. It needs to take two arguments for defining the folder to add and the commit message.</p> Solution <pre><code>@task\ndef dvc(ctx, folder=\"data\", message=\"Add new data\"):\n    ctx.run(f\"dvc add {folder}\")\n    ctx.run(f\"git add {folder}.dvc .gitignore\")\n    ctx.run(f\"git commit -m '{message}'\")\n    ctx.run(f\"git push\")\n    ctx.run(f\"dvc push\")\n</code></pre> <p>and try to run the following command</p> <pre><code>invoke dvc --folder 'data' --message 'Add new data'\n</code></pre> </li> <li> <p>As the final exercise, let's try to combine every way of defining CLIs we have learned about in this module. Define     a task that does the following</p> <ul> <li>calls <code>dvc pull</code> to download the data</li> <li>calls an entrypoint <code>my_cli</code> with the subcommand <code>train</code> with the arguments <code>--output 'model.ckpt'</code></li> </ul> Solution <pre><code>from invoke import task\n\n@task\ndef pull_data(ctx):\n    ctx.run(\"dvc pull\")\n\n@task(pull_data)\ndef train(ctx)\n    ctx.run(\"my_cli train\")\n</code></pre> </li> </ol> <p>That is all there is to it. You should now be able to define tasks that can be run from the terminal to simplify the process of running your code. We recommend that as you go through the learning modules in this course that you slowly start to add tasks to your <code>tasks.py</code> file that simplify the process of running the code you are writing.</p>"},{"location":"s2_organisation_and_version_control/cli/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>What is the purpose of a command line interface?</p> Solution <p>A command line interface is a way for you to define the user interface of your application directly in the terminal. It allows you to interact with your code in a more advanced way than just running Python scripts.</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/code_structure/","title":"M6 - Code structure","text":""},{"location":"s2_organisation_and_version_control/code_structure/#code-organization","title":"Code organization","text":"<p>Core Module</p> <p>With a basic understanding of version control, it is now time to really begin filling up our code repository. However, the question then remains: how to organize our code? As developers, we tend to not think about code organization that much. It is instead something that is just dynamically created as we may need it. However, maybe we should spend some time initially getting organized with the chance of this making our code easier to develop and maintain in the long run. If we do not spend time organizing our code, we may end up with a mess of code that is hard to understand or maintain</p> <p>Big Ball of Mud</p> <p>A Big Ball of Mud is a haphazardly structured, sprawling, sloppy, duct-tape-and-baling-wire, spaghetti-code jungle. These systems show unmistakable signs of unregulated growth, and repeated, expedient repair. Information is shared promiscuously among distant elements of the system, often to the point where nearly all the important information becomes global or duplicated. The overall structure of the system may never have been well defined. If it was, it may have eroded beyond recognition. Programmers with a shred of architectural sensibility shun these quagmires. Only those who are unconcerned about architecture, and, perhaps, are comfortable with the inertia of the day-to-day chore of patching the holes in these failing dikes, are content to work on such systems.  Brian Foote and Joseph Yoder, Big Ball of Mud. Fourth Conference on Patterns Languages of Programs (PLoP '97/EuroPLoP '97) Monticello, Illinois, September 1997</p> <p>We are here going to focus on the organization of data science projects and machine learning projects. The core difference these kinds of projects introduce compared to more traditional systems is data. The key to modern machine learning is without a doubt the vast amounts of data that we have access to today. It is therefore not unreasonable that data should influence our choice of code structure. If we had another kind of application, then the layout of our codebase should probably be different.</p>"},{"location":"s2_organisation_and_version_control/code_structure/#cookiecutter","title":"Cookiecutter","text":"<p>We are in this course going to use the tool cookiecutter, which is a tool for creating projects from project templates. A project template is in short just an overall structure for how you want your folders, files, etc. to be organized from the beginning. For this course we are going to be using a custom MLOps template. The template is essentially a fork of the cookiecutter data science template that has been used for a couple of years in the course, but specialized a bit more towards MLOps instead of general data science.</p> <p>We are not going to argue that this template is better than every other template; it is just a standardized way of creating project structures for machine learning projects. By standardized we mean that if two people are using <code>cookiecutter</code> with the same template, the layout of their code follows some specific rules, enabling them to understand each other's code faster. Code organization is therefore not only to make the code easier for you to maintain but also for others to read and understand.</p> <p>Shown below is the default code structure of cookiecutter for data science projects.</p> <p></p> <p>What is important to keep in mind when using a template is that it is precisely a template. By definition, a template is a guide to making something. Therefore, not all parts of a template may be important for your project at hand. Your job is to pick the parts from the template that are useful for organizing your machine learning project and add the parts that are missing.</p>"},{"location":"s2_organisation_and_version_control/code_structure/#python-projects","title":"Python projects","text":"<p>While the same template in principle could be used regardless of what language we were using for our machine learning or data science application, there are certain considerations to take into account based on what language we are using. Python is the dominant language for machine learning and data science currently, which is why we in this section are focusing on some special files you will need for your Python projects.</p> <p>The first file you may or may not know about is the <code>__init__.py</code> file. In Python the <code>__init__.py</code> file is used to mark a directory as a Python package. Therefore, as a bare minimum, any Python package should look something like this:</p> <pre><code>\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 file1.py\n\u2502   \u251c\u2500\u2500 file2.py\n\u251c\u2500\u2500 pyproject.toml\n</code></pre> <p>The second file to focus on is the <code>pyproject.toml</code>. This file is important for actually converting your code into a Python project. Essentially, whenever you run <code>pip install</code>, <code>pip</code> is in charge of both downloading the package you want but also in charge of installing it. For <code>pip</code> to be able to install a package it needs instructions on what part of the code it should install and how to install it. This is the job of the <code>pyproject.toml</code> file.</p> <p>Below we have both added a description of the structure of the <code>pyproject.toml</code> file but also <code>setup.py + setup.cfg</code> which is the \"old\" way of providing project instructions regarding Python projects. However, you may still encounter a lot of projects using <code>setup.py + setup.cfg</code>, so it is good to at least know about them.</p> pyproject.tomlsetup.py + setup.cfg <p><code>pyproject.toml</code> is the new standardized way of describing project metadata in a declarative way, introduced in PEP 621. It is written in toml format which is easy to read. At the very least your <code>pyproject.toml</code> file should include the <code>[build-system]</code> and <code>[project]</code> sections:</p> <pre><code>[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package-name\"\nversion = \"0.1.0\"\nauthors = [{name = \"EM\", email = \"me@em.com\"}]\ndescription = \"Something cool here.\"\nrequires-python = \"&gt;=3.8\"\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n</code></pre> <p>The <code>[build-system]</code> informs <code>pip</code>/<code>python</code> that to build this Python project it needs the two packages <code>setuptools</code> and <code>wheel</code> and that it should call the setuptools.build_meta function to actually build the project. The <code>[project]</code> section essentially contains metadata regarding the package, what it's called, etc. for if we ever want to publish it to PyPI.</p> <p>For specifying dependencies of your project you have two options. Either you specify them in a <code>requirements.txt</code> file and put that as a dynamic field in <code>pyproject.toml</code> as shown above. Alternatively, you can add a <code>dependencies</code> field under the <code>[project]</code> header like this:</p> <pre><code>[project]\ndependencies = [\n    'torch==2.1.0',\n    'matplotlib&gt;=3.8.1'\n]\n</code></pre> <p>The improvement over <code>setup.py + setup.cfg</code> is that <code>pyproject.toml</code> also allows for metadata from other tools to be specified in it, essentially making sure you only need a single file for your project. For example, in the next [module M7 on good coding practices] you will learn about the tool <code>ruff</code> and how it can help format your code. If we want to configure <code>ruff</code> for our project we can do that directly in <code>pyproject.toml</code> by adding additional headers:</p> <pre><code>[ruff]\nruff_option = ...\n</code></pre> <p>To read more about how to specify <code>pyproject.toml</code> this page is a good place to start.</p> <p><code>setup.py</code> is the original way to describe how a Python package should be built. The most basic <code>setup.py</code> file will look like this:</p> <pre><code>from setuptools import setup\nfrom pip.req import parse_requirements\nrequirements = [str(ir.req) for ir in parse_requirements(\"requirements.txt\")]\nsetup(\n    name=\"my-package-name\",\n    version=\"0.1.0\",\n    author=\"EM\",\n    description=\"Something cool here.\"\n    install_requires=requirements,\n)\n</code></pre> <p>Essentially, it is the exact same meta information as in <code>pyproject.toml</code>, just written directly in Python syntax instead of <code>toml</code>. Because there was a desire to separate this meta information into its own file, the <code>setup.cfg</code> file was created which can contain the exact same information as <code>setup.py</code> just in a declarative config.</p> <pre><code>[metadata]\nname = my-package-name\nversion = 0.1.0\nauthor = EM\ndescription = \"Something cool here.\"\n# ...\n</code></pre> <p>This non-standardized way of providing meta information regarding a package was essentially what led to the creation of <code>pyproject.toml</code>.</p> <p>Regardless of what way a project is configured, after creating the above files, the correct way to install them would be the same</p> <pre><code>pip install .\n# or in developer mode\npip install -e .\n</code></pre> <p>Developer mode in Python</p> <p>The <code>-e</code> is short for <code>--editable</code> mode also called developer mode. Since we will continuously be iterating on our package this is the preferred way to install our package, because that means that we do not have to run <code>pip install</code> every time we make a change. Essentially, in developer mode changes in the Python source code can immediately take place without requiring a new installation.</p> <p>After running this your code should be available to import as <code>from project_name import ...</code> like any other Python package you use. This is the most essential information you need to know about creating Python packages.</p>"},{"location":"s2_organisation_and_version_control/code_structure/#exercises","title":"\u2754 Exercises","text":"<p>After having installed cookiecutter and created your first template project (exercise 1 and 2 below), the remaining exercises are intended to be used on the simple CNN MNIST classifier from yesterday's exercise, with the goal of forcing it into this structure. You are not required to fill out every folder and file in the project structure, but try to at least follow the steps in the exercises. Whenever you need to run a file I recommend always doing so from the root directory e.g.</p> <pre><code>python src/&lt;project_name&gt;/data.py data/raw data/processed\npython src/&lt;project_name&gt;/train_model.py &lt;arguments&gt;\n</code></pre> <p>In this way paths (for saving and loading files) are always relative to the root, and it is in general easier to wrap your head around where files are located.</p> <ol> <li> <p>Install the cookiecutter framework</p> <pre><code>pip install cookiecutter\n</code></pre> </li> <li> <p>Start a new project using this template, which is specialized for     this course (1).</p> <ol> <li>If you feel like the template can be improved in some way, feel free to either open an issue with the proposed     improvement or directly send a pull request to the repository \ud83d\ude04.</li> </ol> <p>You do this by running the cookiecutter command using the template URL:</p> <pre><code>cookiecutter &lt;url-to-template&gt;\n</code></pre> <p>Valid project names</p> <p>When asked for a project name you should follow the PEP8 guidelines for naming packages. This means that the name should be all lowercase and if you want to separate words, you should use underscores. For example <code>my_project</code> is a valid name, while <code>MyProject</code> is not. Additionally, the package name cannot start with a number.</p> Flat-layout vs. src-layout <p>There are two common choices on how layout your source directory. The first is called src-layout where the source code is always placed in a <code>src/&lt;project_name&gt;</code> folder and the second is called flat-layout where the source code is just placed in a <code>&lt;project_name&gt;</code> folder. The template we are using in this course uses the src-layout, but there are pros and cons to both.</p> </li> <li> <p>After having created your new project, the first step is to also create a corresponding virtual environment and     install any needed requirements. If you have a virtual environment from yesterday feel free to use that. Otherwise,     create a new one. Then install the project in that environment.</p> <pre><code>pip install -e .\n</code></pre> </li> <li> <p>Start by filling out the <code>src/&lt;project_name&gt;/data.py</code> file. When this file runs, it should take the raw data, e.g. the     corrupted MNIST files from yesterday (<code>../data/corruptmnist</code>), which now should be located in a <code>data/raw</code> folder and     process them into a single tensor, normalize the tensor and save this intermediate representation to the     <code>data/processed</code> folder. By normalization here we refer to making sure the images have mean 0 and standard     deviation 1.</p> Solution data.py<pre><code>import torch\nimport typer\n\n\ndef normalize(images: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Normalize images.\"\"\"\n    return (images - images.mean()) / images.std()\n\n\ndef preprocess_data(raw_dir: str, processed_dir: str) -&gt; None:\n    \"\"\"Process raw data and save it to processed directory.\"\"\"\n    train_images, train_target = [], []\n    for i in range(6):\n        train_images.append(torch.load(f\"{raw_dir}/train_images_{i}.pt\"))\n        train_target.append(torch.load(f\"{raw_dir}/train_target_{i}.pt\"))\n    train_images = torch.cat(train_images)\n    train_target = torch.cat(train_target)\n\n    test_images: torch.Tensor = torch.load(f\"{raw_dir}/test_images.pt\")\n    test_target: torch.Tensor = torch.load(f\"{raw_dir}/test_target.pt\")\n\n    train_images = train_images.unsqueeze(1).float()\n    test_images = test_images.unsqueeze(1).float()\n    train_target = train_target.long()\n    test_target = test_target.long()\n\n    train_images = normalize(train_images)\n    test_images = normalize(test_images)\n\n    torch.save(train_images, f\"{processed_dir}/train_images.pt\")\n    torch.save(train_target, f\"{processed_dir}/train_target.pt\")\n    torch.save(test_images, f\"{processed_dir}/test_images.pt\")\n    torch.save(test_target, f\"{processed_dir}/test_target.pt\")\n\n\ndef corrupt_mnist() -&gt; tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    \"\"\"Return train and test datasets for corrupt MNIST.\"\"\"\n    train_images = torch.load(\"data/processed/train_images.pt\")\n    train_target = torch.load(\"data/processed/train_target.pt\")\n    test_images = torch.load(\"data/processed/test_images.pt\")\n    test_target = torch.load(\"data/processed/test_target.pt\")\n\n    train_set = torch.utils.data.TensorDataset(train_images, train_target)\n    test_set = torch.utils.data.TensorDataset(test_images, test_target)\n    return train_set, test_set\n\n\nif __name__ == \"__main__\":\n    typer.run(preprocess_data)\n</code></pre> </li> <li> <p>This template comes with <code>tasks.py</code>, which uses the invoke framework to define project     tasks. You can learn more about the framework in the last optional module in today's session. However, for     now just know that <code>tasks.py</code> is a file that can be used to specify common tasks that you want to run in your     project. It is similar to <code>Makefile</code>s if you are familiar with them. Try out some of the pre-defined tasks:</p> <pre><code># first install invoke\npip install invoke\n# then you can execute the tasks\ninvoke preprocess-data  # runs the data.py file\ninvoke requirements     # installs all requirements in the requirements.txt file\ninvoke train            # runs the train.py file\n# or get a list of all tasks\ninvoke --list\n</code></pre> <p>In general, we recommend that you add commands to the <code>tasks.py</code> file as you move along in the course.</p> </li> <li> <p>Transfer your model file <code>model.py</code> into the <code>src/&lt;project_name&gt;/model.py</code> file. When you call the script, e.g.</p> <pre><code>python src/&lt;project_name&gt;/model.py\n</code></pre> <p>it should print out the model architecture and number of parameters in the model.</p> Solution <p>This is the CNN solution from yesterday and it may differ from the model architecture you have created.</p> model.py<pre><code>import torch\nfrom torch import nn\n\n\nclass MyAwesomeModel(nn.Module):\n    \"\"\"My awesome model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(128, 10)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\"\"\"\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv3(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        return self.fc1(x)\n\n\nif __name__ == \"__main__\":\n    model = MyAwesomeModel()\n    print(f\"Model architecture: {model}\")\n    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    dummy_input = torch.randn(1, 1, 28, 28)\n    output = model(dummy_input)\n    print(f\"Output shape: {output.shape}\")\n</code></pre> </li> <li> <p>Transfer the relevant parts of the <code>main.py</code> script to the <code>src/&lt;project-name&gt;/train.py</code> script, i.e. the parts that     have to do with training the model. In addition, make sure it also does the following two things when run:</p> <ul> <li>Saves the trained model to the <code>models</code> folder</li> <li>Saves some statistics/visualizations from training to the <code>reports/figures</code> folder. This could be a simple</li> </ul> Solution train.py<pre><code>import matplotlib.pyplot as plt\nimport torch\nimport typer\nfrom data import corrupt_mnist\nfrom model import MyAwesomeModel\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\n\ndef train(lr: float = 1e-3, batch_size: int = 32, epochs: int = 10) -&gt; None:\n    \"\"\"Train a model on MNIST.\"\"\"\n    print(\"Training day and night\")\n    print(f\"{lr=}, {batch_size=}, {epochs=}\")\n\n    model = MyAwesomeModel().to(DEVICE)\n    train_set, _ = corrupt_mnist()\n\n    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    statistics = {\"train_loss\": [], \"train_accuracy\": []}\n    for epoch in range(epochs):\n        model.train()\n        for i, (img, target) in enumerate(train_dataloader):\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(img)\n            loss = loss_fn(y_pred, target)\n            loss.backward()\n            optimizer.step()\n            statistics[\"train_loss\"].append(loss.item())\n\n            accuracy = (y_pred.argmax(dim=1) == target).float().mean().item()\n            statistics[\"train_accuracy\"].append(accuracy)\n\n            if i % 100 == 0:\n                print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n\n    print(\"Training complete\")\n    torch.save(model.state_dict(), \"models/model.pth\")\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n    axs[0].plot(statistics[\"train_loss\"])\n    axs[0].set_title(\"Train loss\")\n    axs[1].plot(statistics[\"train_accuracy\"])\n    axs[1].set_title(\"Train accuracy\")\n    fig.savefig(\"reports/figures/training_statistics.png\")\n\n\nif __name__ == \"__main__\":\n    typer.run(train)\n</code></pre> <ol> <li>Transfer the remaining parts of the <code>main.py</code> script into the <code>src/&lt;project-name&gt;/evaluate.py</code> script, i.e. the parts that have to do with evaluating the model. When run, it should load the model from the <code>models</code> folder and print out the accuracy of the model on the test set.</li> </ol> Solution evaluate.py<pre><code>import torch\nimport typer\nfrom data import corrupt_mnist\nfrom model import MyAwesomeModel\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\n\ndef evaluate(model_checkpoint: str) -&gt; None:\n    \"\"\"Evaluate a trained model.\"\"\"\n    print(\"Evaluating like my life depended on it\")\n    print(model_checkpoint)\n\n    model = MyAwesomeModel().to(DEVICE)\n    model.load_state_dict(torch.load(model_checkpoint))\n\n    _, test_set = corrupt_mnist()\n    test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=32)\n\n    model.eval()\n    correct, total = 0, 0\n    for img, target in test_dataloader:\n        img, target = img.to(DEVICE), target.to(DEVICE)\n        y_pred = model(img)\n        correct += (y_pred.argmax(dim=1) == target).float().sum().item()\n        total += target.size(0)\n    print(f\"Test accuracy: {correct / total}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(evaluate)\n</code></pre> </li> <li> <p>Fill out the file <code>src/&lt;project-name&gt;/visualize.py</code> with this (as minimum, feel free to add more visualizations)</p> <ul> <li>Loads a pre-trained network.</li> <li>Extracts some intermediate representation of the data (your training set) from your CNN. This could be the     features just before the final classification layer.</li> <li>Visualize features in 2D space using     t-SNE to do the dimensionality     reduction.</li> <li>Save the visualization to a file in the <code>reports/figures/</code> folder.</li> </ul> Solution <p>The solution here depends a bit on the choice of model. However, in most cases the last layer in the model will be a fully connected layer, which we assume is named <code>fc</code>. The easiest way to get the features before this layer is to replace the layer with <code>torch.nn.Identity</code> which essentially does nothing (see the highlighted line below). Alternatively, if you implemented everything in <code>torch.nn.Sequential</code> you can just remove the last layer from the <code>Sequential</code> object: <code>model = model[:-1]</code>.</p> visualize.py<pre><code>import matplotlib.pyplot as plt\nimport torch\nimport typer\nfrom my_project_name.model import MyAwesomeModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\n\ndef visualize(model_checkpoint: str, figure_name: str = \"embeddings.png\") -&gt; None:\n    \"\"\"Visualize model predictions.\"\"\"\n    model: torch.nn.Module = MyAwesomeModel().to(DEVICE)\n    model.load_state_dict(torch.load(model_checkpoint))\n    model.eval()\n    model.fc = torch.nn.Identity()\n\n    test_images = torch.load(\"data/processed/test_images.pt\")\n    test_target = torch.load(\"data/processed/test_target.pt\")\n    test_dataset = torch.utils.data.TensorDataset(test_images, test_target)\n\n    embeddings, targets = [], []\n    with torch.inference_mode():\n        for batch in torch.utils.data.DataLoader(test_dataset, batch_size=32):\n            images, target = batch\n            predictions = model(images)\n            embeddings.append(predictions)\n            targets.append(target)\n        embeddings = torch.cat(embeddings).numpy()\n        targets = torch.cat(targets).numpy()\n\n    if embeddings.shape[1] &gt; 500:  # Reduce dimensionality for large embeddings\n        pca = PCA(n_components=100)\n        embeddings = pca.fit_transform(embeddings)\n    tsne = TSNE(n_components=2)\n    embeddings = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(10, 10))\n    for i in range(10):\n        mask = targets == i\n        plt.scatter(embeddings[mask, 0], embeddings[mask, 1], label=str(i))\n    plt.legend()\n    plt.savefig(f\"reports/figures/{figure_name}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(visualize)\n</code></pre> <ol> <li>Make sure to update the <code>README.md</code> file with a short description of how your scripts should be run.</li> </ol> </li> <li> <p>Finally, make sure to update the <code>requirements.txt</code> file with any packages that are necessary for running your     code (see this set of exercises for help).</p> </li> <li> <p>(Optional) Feel free to create more files/visualizations (what about investigating/exploring the data distribution?).</p> </li> <li> <p>(Optional) Let's say that you are not satisfied with the template I have recommended that you use, which is     completely fine. What should you then do? You should of course create your own template! This is actually not that     hard to do.</p> <ol> <li> <p>Just as a starting point I would recommend that you fork either the     mlops template which you have already been using or     alternatively fork the data science template.</p> </li> <li> <p>After forking the template, clone it locally and let's start modifying it. The first step is changing     the <code>cookiecutter.json</code> file. For the MLOps template it looks like this:</p> <pre><code>{\n    \"repo_name\": \"repo_name\",\n    \"project_name\": \"project_name\",\n    \"author_name\": \"Your name (or your organization/company/team)\",\n    \"description\": \"A short description of the project.\",\n    \"python_version\": \"3.11\",\n    \"open_source_license\": [\"No license file\", \"MIT\", \"BSD-3-Clause\"],\n}\n</code></pre> <p>Simply add a new line to the JSON file with the name of the variable you want to add and the default value you want it to have.</p> </li> <li> <p>The actual template is located in the <code>{{ cookiecutter.project_name }}</code> folder. <code>cookiecutter</code> works by replacing     everywhere that it sees <code>{{ cookiecutter.&lt;variable_name&gt; }}</code> with the value of the variable. Therefore, if you     want to add a new file to the template, just add it to the <code>{{ cookiecutter.project_name }}</code> folder and make     sure to add the <code>{{ cookiecutter.&lt;variable_name&gt; }}</code> where you want the variable to be replaced.</p> </li> <li> <p>After you have made the changes you want to the template, you should test it locally. Just run</p> <pre><code>cookiecutter . -f --no-input\n</code></pre> <p>And it should create a new folder using the default values of the <code>cookiecutter.json</code> file.</p> </li> <li> <p>Finally, make sure to push any changes you made to the template to GitHub, so that you in the future can use it     by simply running</p> <pre><code>cookiecutter https://github.com/&lt;username&gt;/&lt;my_template_repo&gt;\n</code></pre> </li> </ol> </li> </ol>"},{"location":"s2_organisation_and_version_control/code_structure/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>Starting completely from scratch, what are the steps needed to create a new GitHub repository and push a specific     template to it as the very first commit?</p> Solution <ol> <li> <p>Create a completely barebone repository, either using the GitHub UI or if you have the GitHub CLI installed     (not <code>git</code>) you can run</p> <pre><code>gh repo create &lt;repo_name&gt; --public --confirm\n</code></pre> </li> <li> <p>Run <code>cookiecutter</code> with the template you want to use.</p> <pre><code>cookiecutter &lt;template&gt;\n</code></pre> <p>The name of the folder created by <code>cookiecutter</code> should be the same as the  you just used. <li> <p>Run the following sequence of commands.</p> <pre><code>cd &lt;project_name&gt;\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit remote add origin https://github.com/&lt;username&gt;/&lt;repo_name&gt;\ngit push origin master\n</code></pre> </li> <p>That's it. The template should now have been pushed to the repository as the first commit.</p> <p>That ends the module on code structure and <code>cookiecutter</code>. We again want to stress the point of using <code>cookiecutter</code> is not about following one specific template, but instead just to use any template for organizing your code. What often happens in a team is that multiple templates are needed in different stages of the development phase or for different product types because they share a common structure, while still having some specifics. Keeping templates up-to-date then becomes critical such that no team member is using an outdated template. If you ever end up in this situation, we highly recommend to checkout cruft that works alongside <code>cookiecutter</code> to not only make projects but also update existing ones as the template evolves. Cruft additionally also has template validation capabilities to ensure projects match the latest version of a template.</p>"},{"location":"s2_organisation_and_version_control/dvc/","title":"M8 - Data version control","text":""},{"location":"s2_organisation_and_version_control/dvc/#data-version-control","title":"Data Version Control","text":"<p>Core Module</p> <p>Warning</p> <p>Since August 2024, Google has changed their policy for the Google Drive API. This means that the proceduce for setting up DVC with Google Drive has changed. The following exercises therefore need extra authentication to work. You therefore have two options:</p> <ol> <li> <p>Skip these exercises for now. We are going to revisit DVC later in the course when we get access to a more     permanent storage solution in this module.</p> </li> <li> <p>Follow the instructions below to authenticate DVC with Google Drive. As a starting point read the following     GitHub issue and then follow the     instructions     here.     for setting up a custom Google Cloud project.</p> </li> </ol> <p>In this module, we are going to return to version control. However, this time we are going to focus on version control of data. The reason we need to separate between standard version control and data version control comes down to one problem: size.</p> <p>Classic version control was developed to keep track of code files, which are all simple text files. Even a codebase that contains 1000+ files with millions of lines of code can probably be stored in less than a single gigabyte (GB). On the other hand, the size of data can be drastically bigger. As most machine learning algorithms only get better with the more data that you feed them, we are seeing models today that are being trained on petabytes of data (1.000.000 GB).</p> <p>Because this is an important concept there exist a couple of frameworks that have specialized in versioning data such as DVC, DAGsHub, Hub, Modelstore and ModelDB. Regardless of the framework, they all implement more or less the same concept: instead of storing the actual data files or in general storing any large artifact files, we instead store a pointer to these large flies. We then version control the point instead of the artifact.</p> <p></p>  Image credit  <p>We are in this course going to use <code>DVC</code> provided by iterative.ai, as they also provide tools for automating machine learning, which we are going to focus on later.</p>"},{"location":"s2_organisation_and_version_control/dvc/#dvc-what-is-it","title":"DVC: What is it?","text":"<p>DVC (Data Version Control) is simply an extension of <code>git</code> to not only take versioning data but also models and experiments in general. But how does it deal with these large data files? Essentially, <code>DVC</code> will just keep track of a small metafile that will then point to some remote location where your original data is stored. Metafiles essentially work as placeholders for your data files. Your large data files are then stored in some remote location such as Google Drive or an <code>S3</code> bucket from Amazon.</p> <p></p>  Image credit  <p>As the figure shows, we now have two remote locations: one for code and one for data. We use <code>git pull/push</code> for the code and <code>dvc pull/push</code> for the data. The key concept is the connection between the data file <code>model.pkl</code> which is fairly large and its respective metafile <code>model.pkl.dvc</code> which is very small. The large file is stored in the data remote and the metafile is stored in the code remote.</p>"},{"location":"s2_organisation_and_version_control/dvc/#exercises","title":"\u2754 Exercises","text":"<p>If in doubt about some exercises, we recommend checking out the documentation for DVC as it contains excellent tutorials.</p> <ol> <li> <p>For these exercises, we are going to use Google Drive as a remote storage     solution for our data. If you do not already have a Google account, please create one (we are going to use it again     in later exercises). Please make sure that you at least have 1 GB of free space.</p> </li> <li> <p>Next, install DVC and the Google Drive extension</p> <pre><code>pip install dvc\npip install dvc-gdrive\n</code></pre> <p>If you installed DVC via pip and plan to use cloud services as remote storage, you might need to install these optional dependencies: <code>[s3]</code>, <code>[azure]</code>, <code>[gdrive]</code>, <code>[gs]</code>, <code>[oss]</code>, <code>[ssh]</code>. Alternatively, use <code>[all]</code> to include them all. If you encounter that the installation fails, we recommend that you start by updating pip and then trying to update <code>dvc</code>:</p> <pre><code>pip install -U pip\npip install -U dvc-gdrive\n</code></pre> <p>If this does not work for you, it is most likely due to a problem with <code>pygit2</code> and in that case we recommend that you follow the instructions here.</p> </li> <li> <p>In your MNIST repository run the following command from the terminal:</p> <pre><code>dvc init\n</code></pre> <p>This will set up <code>dvc</code> for this repository (similar to how <code>git init</code> will initialize a git repository). These files should be committed using standard <code>git</code> to your repository.</p> </li> <li> <p>Go to your Google Drive and create a new folder called <code>dtu_mlops_data</code>. Then copy the unique identifier     belonging to that folder as shown in the figure below</p> <p> </p> <p>Using this identifier, add it as remote storage.</p> <pre><code>dvc remote add -d storage gdrive://&lt;your_identifier&gt;\n</code></pre> </li> <li> <p>Check the content of the file <code>.dvc/config</code>. Does it contain a pointer to your remote storage? Afterwards, make sure     to add this file to the next commit we are going to make:</p> <pre><code>git add .dvc/config\n</code></pre> </li> <li> <p>Call the <code>dvc add</code> command on your data files exactly like you would add a file with <code>git</code> (you do not need to     add every file by itself as you can directly add the <code>data/</code> folder). Doing this should create a human-readable     file with the extension <code>.dvc</code>. This is the metafile as explained earlier that will serve as a placeholder for     your data. If you are on Windows and this step fails you may need to install <code>pywin32</code>. At the same time, the <code>data</code>     folder should have been added to the <code>.gitignore</code> file that marks which files should not be tracked by git. Confirm     that this is correct.</p> </li> <li> <p>Now we are going to add, commit and tag the metafiles, so we can restore to this stage later on. Commit and tag     the files, which should look something like this:</p> <pre><code>git add data.dvc .gitignore\ngit commit -m \"First datasets, containing 25000 images\"\ngit tag -a \"v1.0\" -m \"data v1.0\"\n</code></pre> </li> <li> <p>Finally, push your data to the remote storage using <code>dvc push</code>. You will be asked to authenticate, which involves     copy-pasting the code in the link prompted. Check out your Google Drive folder. You will see that the data is not     in a recognizable format anymore due to the way that <code>dvc</code> packs and tracks the data. The boring detail is that     <code>dvc</code> converts the data into content-addressable storage     which makes data much faster to get. Finally, make sure that your data is not stored in your GitHub repository.</p> <p>After authenticating the first time, DVC should be set up without having to authenticate again. If you for some reason encounter that DVC fails to authenticate, you can try to reset the authentication. Locate the file <code>$CACHE_HOME/pydrive2fs/{gdrive_client_id}/default.json</code>, where <code>$CACHE_HOME</code> depends on your operating system:</p> macOSLinuxWindows <p><code>~/Library/Caches</code></p> <p><code>~/.cache</code>  This is the typical location, but it may vary depending on what distro you are running</p> <p><code>{user}/AppData/Local</code></p> <p>Delete the complete <code>{gdrive_client_id}</code> folder and retry authenticating with <code>dvc push</code>.</p> </li> <li> <p>After completing the above steps, it is very easy for others (or yourself) to get set up with both     code and data by simply running</p> <pre><code>git clone &lt;my_repository&gt;\ncd &lt;my_repository&gt;\ndvc pull\n</code></pre> <p>(assuming that you give them access rights to the folder in your drive). Try doing this (in some other location than your standard code) to make sure that the two commands indeed download both your code and data.</p> </li> <li> <p>Let's now look at the process of creating a new version of our data. We are going to add some new data to our     dataset and version control this as well. The new data can be downloaded from this     Google Drive folder     or by running these two commands:</p> <pre><code>pip install gdown\ngdown --folder 'https://drive.google.com/drive/folders/1JTjbom7IrB41Chx6uxLCN16ZwIxHHVw1?usp=sharing'\n</code></pre> <p>Copy the data to your <code>data/raw</code> folder and then rerun your data pipeline to incorporate the new data into the files in your <code>processed</code> folder. The new data should be 4 files with train images and 4 files with train targets, a total of 20,000 additional observations.</p> </li> <li> <p>Redo the above steps, adding the new data using <code>dvc</code>, committing and tagging the metafiles e.g. the following     commands should be executed (with appropriate input):</p> <p><code>dvc add -&gt; git add -&gt; git commit -&gt; git tag -&gt; dvc push -&gt; git push</code>.</p> </li> <li> <p>Let's say that you wanted to go back to the state of your data in v1.0. If the above steps have been done correctly,     you should be able to do this using:</p> <pre><code>git checkout v1.0\ndvc checkout\n</code></pre> <p>Confirm that you have reverted to the original data.</p> </li> <li> <p>(Optional) Finally, it is important to note that <code>dvc</code> is not only intended to be used to store data files but also     any other large files such as trained model weights (with billions of parameters these can be quite large). For     example, if we always store our best-performing model in a file called <code>best_model.ckpt</code> then we can use <code>dvc</code> to     version control it, store it online and make it easy for others to download. Feel free to experiment with this using     your model checkpoints.</p> </li> </ol> <p>In general <code>dvc</code> is a great framework for version-controlling data and models. However, it is important to note that it does have some performance issues when dealing with datasets that consist of many files. Therefore, if you are ever working with a dataset that consists of many small files, it can be a good idea to:</p> <ul> <li> <p>Zip files into a single archive and then version control the archive. The <code>zip</code> archive should be placed in a     <code>data/raw</code> folder and then unzipped in the <code>data/processed</code> folder.</p> </li> <li> <p>If possible turn your data into 1D arrays, then it can be stored in a single file such as <code>.parquet</code> or <code>.csv</code>.     This is especially useful for tabular data. Then you can version control the single file instead of the many files.</p> </li> </ul>"},{"location":"s2_organisation_and_version_control/dvc/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>How do you know that a repository is using dvc?</p> Solution <p>Similar to a git repository having a <code>.git</code> directory, a repository using dvc needs to have a <code>.dvc</code> folder. Alternatively you can use the <code>dvc status</code> command.</p> </li> <li> <p>Assume you just added a folder called <code>data/</code> that you want to track with <code>dvc</code>. What is the sequence of 5 commands     to successfully version control the folder? (assuming you already set up a remote)</p> Solution <pre><code>dvc add data/\ngit add .\ngit commit -m \"added raw data\"\ngit push\ndvc push\n</code></pre> </li> </ol> <p>That's all for today. With the combined power of <code>git</code> and <code>dvc</code> we should be able to version control everything in our development pipeline such that no changes are lost (assuming we commit regularly). It should be noted that <code>dvc</code> offers more than just data version control, so if you want to deep dive into <code>dvc</code> we recommend their pipeline feature and how this can be used to set up version-controlled experiments. Note that we are going to revisit <code>dvc</code> later for a more permanent (and large-scale) storage solution.</p>"},{"location":"s2_organisation_and_version_control/git/","title":"M5 - Git","text":""},{"location":"s2_organisation_and_version_control/git/#git","title":"Git","text":"<p>Core Module</p> <p>Proper collaboration with other people will require that you can work on the same codebase in an organized manner. This is the reason that version control exists. Simply stated, it is a way to keep track of:</p> <ul> <li>Who made changes to the code</li> <li>When did the change happen</li> <li>What changes were made</li> </ul> <p>For a full explanation, please see this page.</p> <p>Secondly, it is important to note that GitHub is not git! GitHub is the dominating player when it comes to hosting repositories, but that does not mean that they are the only ones providing free repository hosting (see bitbucket or gitlab for some other examples).</p> <p>That said, we will be using git and GitHub throughout this course. It is a requirement for passing this course that you create a public repository with your code and use git to upload any code changes. How much you choose to integrate this into your own projects depends, but you are at least expected to be familiar with git+GitHub.</p> <p></p>  Image credit"},{"location":"s2_organisation_and_version_control/git/#initial-config","title":"Initial config","text":"<p>What does Git stand for?</p> <p>The name \"git\" was given by Linus Torvalds when he wrote the very first version. He described the tool as \"the stupid content tracker\" and the name as (depending on your mood):</p> <ul> <li>Random three-letter combination that is pronounceable, and not actually used by any common UNIX command. The fact     that it is a mispronunciation of \"get\" may or may not be relevant.</li> <li>Stupid. Contemptible and Despicable. Simple. Take your pick from the dictionary of slang.</li> <li>\"Global information tracker\": you're in a good mood, and it actually works for you.     Angels sing, and a light suddenly fills the room.</li> <li>\"Goddamn idiotic truckload of sh*t\": when it breaks</li> </ul> <ol> <li> <p>Install git on your computer and make sure     that your installation is working by writing <code>git help</code> in a terminal and it should show you the help message for     git.</p> </li> <li> <p>Create a GitHub account if you do not already have one.</p> </li> <li> <p>To make sure that we do not have to type in our GitHub username every time that we want to make some changes,     we can once and for all set it on our local machine.</p> <pre><code># type in a terminal\ngit config credential.helper store\ngit config --global user.email &lt;email&gt;\n</code></pre> </li> </ol>"},{"location":"s2_organisation_and_version_control/git/#git-overview","title":"Git overview","text":"<p>The most simple way to think of version control is that it is just nodes with lines connecting them:</p> <p></p> <p>Each node, which we call a commit, is uniquely identified by a hash string. Each node stores what our code looked like at that point in time (when we made the commit) and using the hash codes we can easily revert to a specific point in time.</p> <p>The commits are made up of local changes that we make to our code. A basic workflow for adding commits can be seen below:</p> <p></p> <p>Assuming that we have made some changes to our local working directory and that we want to get these updates to be online in the remote repository we have to do the following steps:</p> <ul> <li> <p>First we run the command <code>git add</code>. This will move our changes to the staging area. While changes are in the     staging area we can very easily revert them (using <code>git restore</code>). There has therefore not been assigned a unique     hash to the code yet, and so we can still overwrite it.</p> </li> <li> <p>To take our code from the staging area and make it into a commit, we simply run <code>git commit</code> which will locally     add a node to the graph. Note again that we have not pushed the commit to the online repository yet.</p> </li> <li> <p>Finally, we want others to be able to use the changes that we made. We do a simple <code>git push</code> and our     commit goes online.</p> </li> </ul> <p>Of course, the real power of version control is the ability to make branches, as in the image below.</p> <p></p>  Image credit  <p>Each branch can contain code that is not present on other branches. This is useful when you are many developers working together on the same project.</p>"},{"location":"s2_organisation_and_version_control/git/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>In your GitHub account create a repository, where the intention is that you upload the code from the final     exercise from yesterday.</p> <ol> <li> <p>After creating the repository, clone it to your computer.</p> <pre><code>git clone https://github.com/my_user_name/my_repository_name.git\n</code></pre> </li> <li> <p>Move/copy the three files from yesterday into the repository (and any others that you made).</p> </li> <li> <p>Add the files to a commit by using the <code>git add</code> command.</p> </li> <li> <p>Commit the files using the <code>git commit</code> command where you use the <code>-m</code> argument to provide a commit message (1).</p> <ol> <li> Writing a good commit message is a skill in and of itself. A commit message should be     short but informative about the work you are trying to commit. Try to practice writing good commit messages     throughout the course. You can see     this guideline for help.</li> </ol> </li> <li> <p>Finally push the files to your repository using <code>git push</code>. Make sure to check online that the files have been     updated in your repository (1).</p> <ol> <li> Be aware that you either need to generate a token to remote push from you local     terminal or install the     GitHub CLI.</li> </ol> </li> <li> <p>You can always use the command <code>git status</code> to check where you are in the process of making a commit.</p> </li> <li> <p>Also checkout the <code>git log</code> command, which will show you the history of commits that you have made.</p> </li> </ol> </li> <li> <p>Make sure that you understand how to make branches, as this will allow you to try out code changes without     messing with your working code. Creating a new branch can be done using:</p> <pre><code># create a new branch\ngit checkout -b &lt;my_branch_name&gt;\n</code></pre> <p>Afterwards, you can use <code>git checkout</code> (1) to change between branches (remember to commit your work!). Try adding something (a file, a new line of code, etc.) to the newly created branch, commit it and try changing back to main afterwards. You should hopefully see whatever you added on the branch is not present on the main branch.</p> <ol> <li> The <code>git checkout</code> command is used for a lot of different things in git. It can be used to     change branches, to revert changes and to create new branches. An alternative is using <code>git switch</code> and     <code>git restore</code> which are more modern commands.</li> </ol> </li> <li> <p>If you do not already have a cloned version of the repository belonging to the course, make sure to make one!     I am continuously updating/changing some of the material during the course and I therefore recommend that you     each day before the lecture do a <code>git pull</code> on your local copy.</p> </li> <li> <p>Git may seem like a waste of time when solutions like Dropbox, Google Drive, etc. exist, and it is     not completely untrue when you are only one or two working on a project. However, these file management     systems fall short when hundreds to thousands of people work together. For this exercise you will     go through the steps of sending an open-source contribution:</p> <ol> <li> <p>Go online and find a project you do not own, where you can improve the code. You can either look at this     page of good issues to get started with or for simplicity you can just choose     the repository belonging to the course. Now fork the project by     clicking the Fork button.</p> <p></p> <p>This will create a local copy of the repository which you have complete writing access to. Note that code updates to the original repository do not update code in your local repository.</p> </li> <li> <p>Clone your local fork of the project using <code>git clone</code>.</p> </li> <li> <p>As default your local repository will be on the <code>main branch</code> (HINT: you can check this with the     <code>git status</code> command). It is good practice to make a new branch when working on some changes. Use     the <code>git branch</code> command followed by the <code>git checkout</code> command to create a new branch.</p> </li> <li> <p>You are now ready to make changes to the repository. Try to find something to improve (any spelling mistakes?).     When you have made the changes, do the standard git cycle: <code>add -&gt; commit -&gt; push</code>.</p> </li> <li> <p>Go online to the original repository and go to the <code>Pull requests</code> tab. Find the <code>compare</code> button and     choose the button to compare the <code>master branch</code> of the original repo with the branch that you just created     in your own repository. Check the diff on the page to make sure that it contains the changes you have made.</p> </li> <li> <p>Write a bit about the changes you have made and click <code>Create pull request</code> :).</p> </li> </ol> </li> <li> <p>Forking a repository has the consequence that your fork and the repository that you forked can diverge. To     mitigate this we can set what is called a remote upstream. Take a look at this     page     , and set a remote upstream for the repository you just forked.</p> Solution <pre><code>git remote add upstream &lt;url-to-original-repo&gt;\n</code></pre> </li> <li> <p>After setting the upstream branch, we need to pull and merge any updates. Take a look at this     page     and figure out how to do this.</p> Solution <pre><code>git fetch upstream\ngit checkout main\ngit merge upstream/main\n</code></pre> </li> <li> <p>As a final exercise we want to simulate a merge conflict, which happens when two users try to commit changes     to exactly the same lines of code in the codebase, and git is not able to resolve how the different commits should be     integrated.</p> <ol> <li> <p>In your browser, open your favorite repository (it could be the one you just worked on), go to any file of     your choosing and click the edit button (see image below) and make some change to the file. For example, if     you choose a Python file you can just import some random packages at the top of the file. Commit the change.</p> <p> </p> </li> <li> <p>Make sure not to pull the change you just made to your local computer. Locally make changes to the same     file in the same lines and commit them afterwards.</p> </li> <li> <p>Now try to <code>git pull</code> the online changes. What should (hopefully) happen is that git will tell you that it found     a merge conflict that needs to be resolved. Open the file and you should see something like this</p> <pre><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nthis is some content to mess with\ncontent to append\n=======\ntotally different content to merge later\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; master\n</code></pre> <p>this should be interpreted as: everything that's between <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code> and <code>=======</code> are the changes made by your local commit and everything between <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code> are the changes you are trying to pull. To fix the merge conflict you simply have to make the code in the two \"cells\" work together. When you are done, remove the identifiers <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>, <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>.</p> <p>Merge, rebase or fast-forward?</p> <p>On a <code>git pull</code> you can get messages like this the first time you try to pull after a merge conflict:</p> <pre><code>hint: You have divergent branches and need to specify how to reconcile them.\nhint: You can do so by running one of the following commands sometime before\nhint: your next pull:\nhint:\nhint:   git config pull.rebase false  # merge\nhint:   git config pull.rebase true   # rebase\nhint:   git config pull.ff only       # fast-forward only\nhint:\nhint: You can replace \"git config\" with \"git config --global\" to set a default\nhint: preference for all repositories. You can also pass --rebase, --no-rebase,\nhint: or --ff-only on the command line to override the configured default per\nhint: invocation.\n</code></pre> <p>In general we recommend setting <code>git config pull.rebase false</code> to merge the changes. This is the default behavior of git and is the most common way to resolve merge conflicts. However, if you are working on a project with many people and you want to keep the commit history clean, you can use <code>git config pull.rebase true</code> to rebase the changes. This will make it look like you made the changes directly on top of the latest commit. The last option <code>git config pull.ff only</code> will only allow you to pull changes that can be fast-forwarded. This is the most strict option and will not allow you to pull changes that have been made to the same lines of code as you have made changes to.</p> </li> <li> <p>Finally, commit the merge and try to push.</p> </li> </ol> </li> <li> <p>(Optional) The above exercises have focused on how to use git from the terminal, which I highly recommend learning.     However, if you are using a proper editor they also have built-in support for version control. We recommend getting     familiar with these features (here is a tutorial for     VS Code).</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/git/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>How do you know if a certain directory is a git repository?</p> Solution <p>You can check if there is a \".git\" directory. Alternatively you can use the <code>git status</code> command.</p> </li> <li> <p>Explain what the file <code>gitignore</code> is used for?</p> Solution <p>The file <code>gitignore</code> is used to tell git which files to ignore when doing a <code>git add .</code> command. This is useful for files that are not part of the codebase, but are needed for the code to run (e.g. data files) or files that contain sensitive information (e.g. <code>.env</code> files that contain API keys and passwords).</p> </li> <li> <p>You have two branches - main and devel. What sequence of commands would you need to execute to make sure that     devel is in sync with main?</p> Solution <pre><code>git checkout main\ngit pull\ngit checkout devel\ngit merge main\n</code></pre> </li> <li> <p>What best practices are you familiar with regarding version control?</p> Solution <ul> <li>Use a descriptive commit message</li> <li>Make each commit a logical unit</li> <li>Incorporate others' changes frequently</li> <li>Share your changes frequently</li> <li>Coordinate with your co-workers</li> <li>Don't commit generated files</li> </ul> </li> </ol> <p>That covers the basics of git to get you started. In the exercise folder you can find a git cheat sheet with the most useful commands for future reference. Finally, we want to point out another awesome feature of GitHub: in-browser editor. Sometimes you have a small edit that you want to make, but still would like to do this in an IDE/editor. Or you may be in the situation where you are working from a device other than your usual developer machine. GitHub has a built-in editor that can simply be enabled by changing any URL from</p> <pre><code>https://github.com/username/repository\n</code></pre> <p>to</p> <pre><code>https://github.dev/username/repository\n</code></pre> <p>Try it out on your newly created repository.</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/","title":"M7 - Good coding practice","text":""},{"location":"s2_organisation_and_version_control/good_coding_practice/#good-coding-practice","title":"Good coding practice","text":"<p>Quote</p> <p>Code is read more often than it is written.  Guido Van Rossum (author of Python)</p> <p>It is hard to define exactly what good coding practices are. But the above quote by Guido does hint at what it could be, namely that it has to do with how others observe and perceive your code. In general, good coding practice is about making sure that your code is easy to read and understand, not only by others but also by your future self. The key concept to keep in mind when we are talking about good coding practice is consistency. In many cases it does not matter exactly how you choose to style your code, etc., the important part is that you are consistent about it.</p> <p></p>  Image credit"},{"location":"s2_organisation_and_version_control/good_coding_practice/#documentation","title":"Documentation","text":"<p>Most programmers have a love-hate relationship with documentation: We absolute hate writing it ourselves, but love when someone else has actually taken the time to add it to their code. There is no doubt that well documented code is much easier to maintain, as you do not need to remember all the details about the code to still maintain it. It is key to remember that good documentation saves more time than it takes to write.</p> <p>The problem with documentation is that there is no right or wrong way to do it. You can end up doing:</p> <ul> <li> <p>Under-documentation: You document information that is clearly visible from the code and not the complex     parts that are actually hard to understand.</p> </li> <li> <p>Over-documentation: Writing too much documentation will have likely have an effect on most people opposite to     what you want: there is too much to read, so people will skip it.</p> </li> </ul> <p>Writing good documentation is a skill that takes time to train, so let's try to do it.</p> <p>Quote</p> <p>Code tells you how; Comments tell you why.  Jeff Atwood</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Go over the most complicated file in your project. Be critical and add comments where the logic     behind the code is not easily understandable. (1)</p> <ol> <li> <p> In deep learning we often work with multi-dimensional tensors that constantly change shape     after each operation. It is good practice to annotate with comments when tensors undergo some reshaping.     In the following example we compute the pairwise euclidean distance between two tensors using broadcasting     which results in multiple shape operations.</p> <pre><code>x = torch.randn(5, 10)  # N x D\ny = torch.randn(7, 10)  # M x D\nxy = x.unsqueeze(1) - y.unsqueeze(0)  # (N x 1 x D) - (1 x M x D) = (N x M x D)\npairwise_euc_dist = xy.abs().pow(2.0).sum(dim=-1)  # N x M\n</code></pre> </li> </ol> </li> <li> <p>Add docstrings to at least two Python functions/methods.     You can see here (example 5) a good example of     how to use identifiable keywords such as <code>Parameters</code>, <code>Args</code>, <code>Returns</code>, which standardizes the way of     writing docstrings.</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#styling","title":"Styling","text":"<p>While Python already enforces some styling (e.g. code should be indented in a specific way), this is not enough to ensure that code from different users actually looks similar. Maybe even more troubling is that you will often see that your own style of coding changes as you become more and more experienced. This kind of difference in coding style is not that important to keep in mind when you are working on a personal project, but when multiple people are working together on the same project it is important to consider.</p> <p>The question then remains what styling you should use. This is where Pep8 comes into play, which is the official style guide for python. It essentially contains what is considered \"good practice\" and \"bad practice\" when coding python.</p> <p>For many years the most commonly used tool to check if your code was PEP8-compliant was to use flake8. However, we are in this course going to be using ruff, which is quickly gaining popularity due to how fast it is and how quickly the developers are adding new features. (1)</p> <ol> <li> both <code>flake8</code> and <code>ruff</code> are what is called a     linter or lint tool, which is any kind of static code analysis     program that is used to flag programming errors, bugs, and styling errors.</li> </ol>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Install <code>ruff</code>.</p> <pre><code>pip install ruff\n</code></pre> </li> <li> <p>Run <code>ruff</code> on your project or part of your project.</p> <pre><code>ruff check .  # Lint all files in the current directory (and any subdirectories)\nruff check path/to/code/  # Lint all files in `/path/to/code` (and any subdirectories).\n</code></pre> <p>Are you PEP8-compliant or are you a mere mortal?</p> </li> </ol> <p>You could go and fix all the small errors that <code>ruff</code> is giving. However, in practice large projects instead rely on some kind of code formatter that will automatically format your code for you to be PEP8-compliant. Some of the biggest formatters for the longest time in Python have been black and yapf, but we are going to use <code>ruff</code>, which also has a built-in formatter that should be a drop-in replacement for <code>black</code>.</p> <ol> <li> <p>Try to use <code>ruff format</code> to format your code.</p> <pre><code>ruff format .  # Format all files in the current directory.\nruff format /path/to/file.py  # Format a single file.\n</code></pre> </li> </ol> <p>By default <code>ruff</code> will apply a selection of rules when we are either checking it or formatting it. However, many more rules can be activated through configuration. If you have completed module M6 on code structure you will have encountered the <code>pyproject.toml</code> file, which can store both build instructions about our package but also configuration of developer tools. Let's try to configure <code>ruff</code> using the <code>pyproject.toml</code> file.</p> <ol> <li> <p>One aspect that is not covered by PEP8 is how <code>import</code> statements in Python should be organized. If you are like     most people, you place your <code>import</code> statements at the top of the file and they are ordered simply by when you     needed them. A better practice is to introduce some clear structure in our imports. In older versions of this course     we have used isort to do the job, but we are here going to configure <code>ruff</code> to do     the job. In your <code>pyproject.toml</code> file add the following lines</p> <pre><code>[tool.ruff]\nlint.select = [\"I\"]\n</code></pre> <p>and try re-running <code>ruff check</code> and <code>ruff format</code>. Hopefully this should reorganize your imports to follow common practice. (1)</p> <ol> <li> the common practise is to first list built-in Python packages (like <code>os</code>) in one block,     followed by third-party dependencies (like <code>torch</code>) in a second block and finally imports from your own package     in a third block. Each block is then put in alphabetical order.</li> </ol> </li> <li> <p>One PEP8 styling rule that is often diverged from is the recommended line length of 79 characters, which by many     (including myself) is considered very restrictive. If your code consists of multiple levels of indentation, you can     quickly run into 79 characters being limiting. For this reason many projects increase it, often to 120 characters,     which seems to be the sweet spot of how many characters fit in a coding window on a laptop.     Add the line</p> <pre><code>line-length=120\n</code></pre> <p>under the <code>[tool.ruff]</code> section in the <code>pyproject.toml</code> file and rerun <code>ruff check</code> and <code>ruff format</code> on your code.</p> </li> <li> <p>Experiment with further configuration of <code>ruff</code>. In particular we recommend adding more     rules and looking at <code>[tool.ruff.pydocstyle]</code> configuration to indicate how you     have styled your documentation.</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#typing","title":"Typing","text":"<p>In addition to writing documentation and following a specific styling method, in Python we have a third way of improving the quality of our code: through typing. Typing goes back to the earlier programming languages like <code>c</code>, <code>c++</code> etc. where data types needed to be explicitly stated for variables:</p> <pre><code>int main() {\n    int x = 5 + 6;\n    float y = 0.5;\n    cout &lt;&lt; \"Hello World! \" &lt;&lt; x &lt;&lt; std::endl();\n}\n</code></pre> <p>This is not required by Python but it can really improve the readability of code, since then you can directly read from the code what the expected types of input arguments and returns are. In Python the <code>:</code> character has been reserved for type hints. Here is one example of adding typing to a function:</p> <pre><code>def add2(x: int, y: int) -&gt; int:\n    return x+y\n</code></pre> <p>Here we mark that both <code>x</code> and <code>y</code> are integers and using the arrow notation <code>-&gt;</code> we mark that the output type is also an integer. Assuming that we are also going to use the function for floats and <code>torch.Tensor</code>s we could improve the typing by specifying a union of types. Depending on the version of Python you are using the syntax for this can be different.</p> python &lt;3.10python &gt;=3.10 <pre><code>from torch import Tensor  # note it is Tensor with upper case T. This is the base class of all tensors\nfrom typing import Union\ndef add2(x: Union[int, float, Tensor], y: Union[int, float, Tensor]) -&gt; Union[int, float, Tensor]:\n    return x+y\n</code></pre> <pre><code>from torch import Tensor  # note it is Tensor with upper case T. This is the base class of all tensors\ndef add2(x: int | float | Tensor, y: int | float | Tensor) -&gt; int | float | Tensor:\n    return x+y\n</code></pre> <p>Finally, since this is a very generic function it also works on <code>numpy</code> arrays, etc. We can always default to the <code>Any</code> type if we are not sure about all the specific types that a function can take.</p> <pre><code>from typing import Any\ndef add2(x: Any, y: Any) -&gt; Any:\n    return x+y\n</code></pre> <p>However, that is basically the same as if our function were not typed, as the type hints do not help us at all. Therefore, use <code>Any</code> only when necessary.</p>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#exercises_2","title":"\u2754 Exercises","text":"<p>Exercise files</p> <ol> <li> <p>We provide a file called <code>typing_exercise.py</code>. Add typing everywhere in the file. Please note that you will     need the following imports</p> <pre><code>from typing import Callable, Optional, Tuple, Union, List  # you will need all of them in your code\n</code></pre> <p>for it to work. This cheat sheet is a good resource on typing. We also provide <code>typing_exercise_solution.py</code>, but try to solve the exercise yourself.</p> <code>typing_exercise.py</code> typing_exercise.py<pre><code>import torch\nfrom torch import nn\n\n\nclass Network(nn.Module):\n    \"\"\"Builds a feedforward network with arbitrary hidden layers.\n\n    Arguments:\n        input_size: integer, size of the input layer\n        output_size: integer, size of the output layer\n        hidden_layers: list of integers, the sizes of the hidden layers\n\n    \"\"\"\n\n    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5) -&gt; None:\n        super().__init__()\n        # Input to a hidden layer\n        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n\n        # Add a variable number of more hidden layers\n        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n\n        self.output = nn.Linear(hidden_layers[-1], output_size)\n\n        self.dropout = nn.Dropout(p=drop_p)\n\n    def forward(self, x):\n        \"\"\"Forward pass through the network, returns the output logits.\"\"\"\n        for each in self.hidden_layers:\n            x = nn.functional.relu(each(x))\n            x = self.dropout(x)\n        x = self.output(x)\n\n        return nn.functional.log_softmax(x, dim=1)\n\n\ndef validation(model, testloader, criterion):\n    \"\"\"Validation pass through the dataset.\"\"\"\n    accuracy = 0\n    test_loss = 0\n    for images, labels in testloader:\n        images = images.resize_(images.size()[0], 784)\n\n        output = model.forward(images)\n        test_loss += criterion(output, labels).item()\n\n        ## Calculating the accuracy\n        # Model's output is log-softmax, take exponential to get the probabilities\n        ps = torch.exp(output)\n        # Class with highest probability is our predicted class, compare with true label\n        equality = labels.data == ps.max(1)[1]\n        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n        accuracy += equality.type_as(torch.FloatTensor()).mean()\n\n    return test_loss, accuracy\n\n\ndef train(model, trainloader, testloader, criterion, optimizer=None, epochs=5, print_every=40) -&gt; None:\n    \"\"\"Train a PyTorch Model.\"\"\"\n    if optimizer is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n    steps = 0\n    running_loss = 0\n    for e in range(epochs):\n        # Model in training mode, dropout is on\n        model.train()\n        for images, labels in trainloader:\n            steps += 1\n\n            # Flatten images into a 784 long vector\n            images.resize_(images.size()[0], 784)\n\n            optimizer.zero_grad()\n\n            output = model.forward(images)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            if steps % print_every == 0:\n                # Model in inference mode, dropout is off\n                model.eval()\n\n                # Turn off gradients for validation, will speed up inference\n                with torch.no_grad():\n                    test_loss, accuracy = validation(model, testloader, criterion)\n\n                print(\n                    f\"Epoch: {e + 1}/{epochs}.. \",\n                    f\"Training Loss: {running_loss / print_every:.3f}.. \",\n                    f\"Test Loss: {test_loss / len(testloader):.3f}.. \",\n                    f\"Test Accuracy: {accuracy / len(testloader):.3f}\",\n                )\n\n                running_loss = 0\n\n                # Make sure dropout and grads are on for training\n                model.train()\n</code></pre> Solution typing_exercise_solution.py<pre><code>from __future__ import annotations\n\nfrom collections.abc import Callable\n\nimport torch\nfrom torch import nn\n\n\nclass Network(nn.Module):\n    \"\"\"Builds a feedforward network with arbitrary hidden layers.\n\n    Arguments:\n        input_size: integer, size of the input layer\n        output_size: integer, size of the output layer\n        hidden_layers: list of integers, the sizes of the hidden layers\n\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        hidden_layers: list[int],\n        drop_p: float = 0.5,\n    ) -&gt; None:\n        super().__init__()\n        # Input to a hidden layer\n        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n\n        # Add a variable number of more hidden layers\n        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n\n        self.output = nn.Linear(hidden_layers[-1], output_size)\n\n        self.dropout = nn.Dropout(p=drop_p)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the network, returns the output logits.\"\"\"\n        for each in self.hidden_layers:\n            x = nn.functional.relu(each(x))\n            x = self.dropout(x)\n        x = self.output(x)\n\n        return nn.functional.log_softmax(x, dim=1)\n\n\ndef validation(\n    model: nn.Module,\n    testloader: torch.utils.data.DataLoader,\n    criterion: Callable | nn.Module,\n) -&gt; tuple[float, float]:\n    \"\"\"Validation pass through the dataset.\"\"\"\n    accuracy = 0\n    test_loss = 0\n    for images, labels in testloader:\n        images = images.resize_(images.size()[0], 784)\n\n        output = model.forward(images)\n        test_loss += criterion(output, labels).item()\n\n        ## Calculating the accuracy\n        # Model's output is log-softmax, take exponential to get the probabilities\n        ps = torch.exp(output)\n        # Class with highest probability is our predicted class, compare with true label\n        equality = labels.data == ps.max(1)[1]\n        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n        accuracy += equality.type_as(torch.FloatTensor()).mean().item()\n\n    return test_loss, accuracy\n\n\ndef train(\n    model: nn.Module,\n    trainloader: torch.utils.data.DataLoader,\n    testloader: torch.utils.data.DataLoader,\n    criterion: Callable | nn.Module,\n    optimizer: None | torch.optim.Optimizer = None,\n    epochs: int = 5,\n    print_every: int = 40,\n) -&gt; None:\n    \"\"\"Train a PyTorch Model.\"\"\"\n    if optimizer is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n    steps = 0\n    running_loss = 0\n    for e in range(epochs):\n        # Model in training mode, dropout is on\n        model.train()\n        for images, labels in trainloader:\n            steps += 1\n\n            # Flatten images into a 784 long vector\n            images.resize_(images.size()[0], 784)\n\n            optimizer.zero_grad()\n\n            output = model.forward(images)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            if steps % print_every == 0:\n                # Model in inference mode, dropout is off\n                model.eval()\n\n                # Turn off gradients for validation, will speed up inference\n                with torch.no_grad():\n                    test_loss, accuracy = validation(model, testloader, criterion)\n\n                print(\n                    f\"Epoch: {e + 1}/{epochs}.. \",\n                    f\"Training Loss: {running_loss / print_every:.3f}.. \",\n                    f\"Test Loss: {test_loss / len(testloader):.3f}.. \",\n                    f\"Test Accuracy: {accuracy / len(testloader):.3f}\",\n                )\n\n                running_loss = 0\n\n                # Make sure dropout and grads are on for training\n                model.train()\n</code></pre> </li> <li> <p>mypy is what is called a static type checker. If you are using     typing in your code, then a static type checker can help you find common mistakes. <code>mypy</code> does not run your code,     but it scans it and checks that the types you have given are compatible. Install <code>mypy</code>.</p> <pre><code>pip install mypy\n</code></pre> </li> <li> <p>Try to run <code>mypy</code> on the <code>typing.py</code> file</p> <pre><code>mypy typing_exercise.py\n</code></pre> <p>If you have solved exercise 11 correctly then you should get no errors. If not <code>mypy</code> should tell you where your types are incompatible.</p> </li> </ol>"},{"location":"s2_organisation_and_version_control/good_coding_practice/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>According to PEP8 what is wrong with the following code?</p> <pre><code>class myclass(nn.Module):\n    def TrainNetwork(self, X, y):\n        ...\n</code></pre> Solution <p>According to PEP8 classes should follow the CapWords convention, meaning that the first letter in each word of the class name should be capitalized. Thus <code>myclass</code> should instead be <code>MyClass</code>. On the other hand, functions and methods should be all lowercase with words separated by underscore. Thus <code>TrainNetwork</code> should be <code>train_network</code>.</p> </li> <li> <p>What would be the type of argument <code>x</code> for a function <code>def f(x):</code> if it needs to support the following input:</p> <pre><code>x1 = [1, 2, 3, 4]\nx2 = (1, 2, 3, 4)\nx3 = None\nx4 = {1: \"1\", 2: \"2\", 3: \"3\", 4: \"4\"}\n</code></pre> Solution <p>The easy solution would be to do <code>def f(x : Any)</code>. But instead we could also go with:</p> <pre><code>def f(x: None | Tuple[int, ...] | List[int] | Dict[int, str]):\n</code></pre> <p>Alternatively, we could do</p> <pre><code>def f(x: None | Iterable[int]):\n</code></pre> <p>because <code>list</code>, <code>tuple</code> and <code>dict</code> are all iterables and therefore can be covered by one type (in this specific case).</p> </li> </ol> <p>This ends the module on coding style. We again want to emphasize that a good coding style is more about having a consistent style than strictly following PEP8. A good example of this is Google, which has their own style guide for Python. This guide does not match PEP8 exactly, but it makes sure that different teams within Google that are working on different projects are still to a large degree following the same style and therefore if a project is handed from one team to another then at least that will not be a problem.</p>"},{"location":"s3_reproducibility/","title":"Reproducibility","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn how to create reproducible computing environments using <code>docker</code> and how to use them to run your code.</p> <p> M10: Docker</p> </li> <li> <p></p> <p>Learn how to use <code>hydra</code> to manage configuration files and how to integrate them into your code.</p> <p> M11: Config Files</p> </li> </ul> <p>Today is all about reproducibility - one of those concepts that everyone agrees is very important and something should be done about, but the reality is that it is very hard to ensure complete reproducibility. The last sessions have already touched a bit on how tools like <code>conda</code> and code organization can help make code more reproducible. Today we are going all the way to ensure that our scripts and our computing environment are fully reproducible.</p>"},{"location":"s3_reproducibility/#why-does-reproducibility-matter","title":"Why does reproducibility matter","text":"<p>Reproducibility is closely related to the scientific method:</p> <p>Observe -&gt; Question -&gt; Hypotheses -&gt; Experiment -&gt; Conclude -&gt; Result -&gt; Observe -&gt; ...</p> <p>Not having reproducible experiments essentially breaks the cycle between doing experiments and making conclusions. If experiments are not reproducible, then we do not expect that others will arrive at the same conclusion as ourselves. As machine learning experiments are fundamentally the same as doing chemical experiments in a laboratory, we should be equally careful in making sure our environments are reproducible (think of your laptop as your laboratory).</p> <p>Secondly, if we focus on why reproducibility matters especially in machine learning, it is part of the bigger challenge of making sure that machine learning is trustworthy.</p> <p></p>  Many different aspects are needed if trustworthy machine learning is ever going to be a reality. We need robustness pipelines so we can trust that they will not fail under heavy load. We need integrity to make sure that pipelines are deployed if they are of high quality. We need explainability to make sure that we understand what our machine learning models are doing, so it is not just a black box. We need reproducibility to make sure that the results of our models can be reproduced over and over again. Finally, we need fairness to make sure that our models are not biased toward specific populations. Figure inspired by this paper.  <p>Trustworthy ML is the idea that machine learning agents can be trusted. Take the example of a machine learning agent being responsible for medical diagnoses. It is very clear that we need to be able to trust that the agent gives us the correct diagnosis for the system to work in practice. Reproducibility plays a big role here, because without it we cannot be sure that the same agent deployed at two different hospitals will give the same diagnosis (given the same input).</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the importance of reproducibility in computer science</li> <li>Be able to use <code>docker</code> to create a reproducible container, including how to build them from scratch</li> <li>Understand different ways of configuring your code and how to use <code>hydra</code> to integrate with config files</li> </ul>"},{"location":"s3_reproducibility/config_files/","title":"M11 - Config Files","text":""},{"location":"s3_reproducibility/config_files/#config-files","title":"Config files","text":"<p>With docker we can make sure that our compute environment is reproducible, but that does not mean that all our experiments magically become reproducible. There are other factors that are important for creating reproducible experiments.</p> <p>In this paper (highly recommended read) the authors tried to reproduce the results of 255 papers and tried to figure out which factors were significant to succeed. One of those factors was \"Hyperparameters Specified\" e.g. whether or not the authors of the paper had precisely specified the hyperparameters that were used to run the experiments. It should come as no surprise that this can be a determining factor for reproducibility. However it is not a given that hyperparameters are always well specified.</p>"},{"location":"s3_reproducibility/config_files/#configure-experiments","title":"Configure experiments","text":"<p>There is really no way around it: deep learning contains a lot of hyperparameters. In general, a hyperparameter is any parameter that affects the learning process (e.g. the weights of a neural network are not hyperparameters because they are a consequence of the learning process). The problem with having many hyperparameters to control in your code is that if you are not careful and structure them it may be hard after running an experiment to figure out which hyperparameters were actually used. Lack of proper configuration management can cause serious problems with reliability, uptime, and the ability to scale a system.</p> <p>One of the most basic ways of structuring hyperparameters is just to put them directly into your <code>train.py</code> script in some object:</p> <pre><code>class my_hp:\n    batch_size: 64\n    lr: 128\n    other_hp: 12345\n\n# easy access to them\ndl = DataLoader(Dataset, batch_size=my_hp.batch_size)\n</code></pre> <p>the problem here is configuration is not easy. Each time you want to run a new experiment, you basically have to change the script. If you run the code multiple times without committing the changes in between then the exact hyperparameter configuration for some experiments may be lost. Alright, with this in mind you change strategy to use an argument parser e.g. run experiments like this:</p> <pre><code>python train.py --batch_size 256 --learning_rate 1e-4 --other_hp 12345\n</code></pre> <p>This at least solves the problem with configurability. However, we again can end up losing experiments if we are not careful.</p> <p>What we really want is some way to easily configure our experiments where the hyperparameters are systematically saved with the experiment. For this we turn our attention to Hydra, a configuration tool that is based around writing config files to keep track of hyperparameters. Hydra operates on top of OmegaConf which is a <code>yaml</code> based hierarchical configuration system.</p> <p>A simple <code>yaml</code> configuration file could look like</p> <pre><code>#config.yaml\nhyperparameters:\n  batch_size: 64\n  learning_rate: 1e-4\n</code></pre> <p>with the corresponding Python code for loading the file</p> <pre><code>from omegaconf import OmegaConf\n# loading\nconfig = OmegaConf.load('config.yaml')\n\n# accessing in two different ways\ndl = DataLoader(dataset, batch_size=config.hyperparameters.batch_size)\noptimizer = torch.optim.Adam(model.parameters(), lr=config['hyperparameters']['learning_rate'])\n</code></pre> <p>or using <code>hydra</code> for loading the configuration</p> <pre><code>import hydra\n\n@hydra.main(config_name=\"config.yaml\")\ndef main(cfg):\n    print(cfg.hyperparameters.batch_size, cfg.hyperparameters.learning_rate)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>The idea behind refactoring our hyperparameters into <code>.yaml</code> files is that we disentangle the model configuration from the model. In this way it is easier to do version control of the configuration because we have it in a separate file.</p>"},{"location":"s3_reproducibility/config_files/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>The main idea behind the exercises is to take a single script (that we provide) and use Hydra to make sure that everything gets correctly logged such that you would be able to exactly report to others how each experiment was configured. In the provided script, the hyperparameters are hardcoded into the code and your job will be to separate them out into a configuration file.</p> <p>Note that we provide a solution (in the <code>vae_solution</code> folder) that can help you get through the exercise, but try to look online for your answers before looking at the solution. Remember: it's not about the result; it's about the journey.</p> <ol> <li> <p>Start by installing hydra.</p> <pre><code>pip install hydra-core\n</code></pre> <p>Remember to add it to your <code>requirements.txt</code> file.</p> </li> <li> <p>Next, take a look at the <code>vae_mnist.py</code> and <code>model.py</code> files and understand what is going on. It is a model we will     revisit during the course.</p> </li> <li> <p>Identify the key hyperparameters of the script. Some of them should be easy to find, but at least three have made it     into the core part of the code. One essential hyperparameter is also not included in the script but is needed for     the code to be completely reproducible (HINT: the weights of any neural network are initialized at random).</p> Solution <p>From the top of the file <code>batch_size</code>, <code>x_dim</code>, <code>hidden_dim</code> can be found as hyperparameters. Looking through the code it can be seen that the <code>latent_dim</code> of the encoder and decoder, <code>lr</code> for the optimzer, and <code>epochs</code> in the training loop are also hyperparameters. Finally, the <code>seed</code> is not included in the script but is needed to make the script fully reproducible, e.g. <code>torch.manual_seed(seed)</code>.</p> </li> <li> <p>Write a configuration file <code>config.yaml</code> where you write down the hyperparameters that you have found.</p> </li> <li> <p>Get the script running by loading the configuration file inside your script (using hydra) that incorporates the     hyperparameters into the script. Note: you should only edit the <code>vae_mnist.py</code> file and not the <code>model.py</code> file.</p> </li> <li> <p>Run the script.</p> </li> <li> <p>By default hydra will write the results to an <code>outputs</code> folder, with a sub-folder for the day the experiment was     run and further the time it was started. Inspect your run by going over each file that hydra has generated and     check that the information has been logged. Can you find the hyperparameters?</p> </li> <li> <p>Hydra also allows for dynamically changing and adding parameters on the fly from the command-line:</p> <ol> <li> <p>Try changing one parameter from the command-line.</p> <pre><code>python vae_mnist.py hyperparameters.seed=1234\n</code></pre> </li> <li> <p>Try adding one parameter from the command-line.</p> <pre><code>python vae_mnist.py +experiment.stuff_that_i_want_to_add=42\n</code></pre> </li> </ol> </li> <li> <p>By default the file <code>vae_mnist.log</code> should be empty, meaning that whatever you printed to the terminal did not get     picked up by Hydra. This is due to Hydra under the hood making use of the native python     logging package. This means that to also save all printed output     from the script we need to convert all calls to <code>print</code> with <code>log.info</code></p> <ol> <li> <p>Create a logger in the script.</p> <pre><code>import logging\nlog = logging.getLogger(__name__)\n</code></pre> </li> <li> <p>Replace all calls to <code>print</code> with calls to <code>log.info</code>.</p> </li> <li> <p>Try re-running the script and make sure that the output printed to the terminal also gets saved to the     <code>vae_mnist.log</code> file.</p> </li> </ol> </li> <li> <p>Make sure that your script is fully reproducible. To check this you will need two runs of the script to compare.     Then run the <code>reproducibility_tester.py</code> script as</p> <pre><code>python reproducibility_tester.py path/to/run/1 path/to/run/2\n</code></pre> <p>the script will go over trained weights to see if they match and that the hyperparameters are the same. Note: for the script to work, the weights should be saved to a file called <code>trained_model.pt</code> (this is the default of the <code>vae_mnist.py</code> script, so only relevant if you have changed the saving of the weights).</p> </li> <li> <p>Make a new experiment using a new configuration file where you have changed a hyperparameter of your own     choice. You are not allowed to change the configuration file in the script but should instead be able to provide it     as an argument when launching the script e.g. something like</p> <pre><code>python vae_mnist.py experiment=exp2\n</code></pre> <p>We recommend that you use a file structure like this</p> <pre><code>|--conf\n|  |--config.yaml\n|  |--experiments\n|     |--exp1.yaml\n|     |--exp2.yaml\n|--my_app.py\n</code></pre> </li> <li> <p>Finally, an awesome feature of hydra is the     instantiate feature. This allows you to define a     configuration file that can be used to directly instantiate objects in python. Try to create a configuration file     that can be used to instantiate the <code>Adam</code> optimizer in the <code>vae_mnist.py</code> script.</p> Solution <p>The configuration file could look like this</p> <pre><code>optimizer:\n  _target_: torch.optim.Adam\n  lr: 1e-3\n  betas: [0.9, 0.999]\n  eps: 1e-8\n  weight_decay: 0\n</code></pre> <p>and the python code to load the configuration file and instantiate the optimizer could look like this</p> <pre><code>import os\n\nimport hydra\nimport torch.optim as optim\n\n@hydra.main(config_name=\"adam.yaml\", config_path=f\"{os.getcwd()}/configs\")\ndef main(cfg):\n    model = ...  # define the model we want to optimize\n    # the first argument of any optimize is the parameters to optimize\n    # we add those dynamically when we instantiate the optimizer\n    optimizer = hydra.utils.instantiate(cfg.optimizer, params=model.parameters())\n    print(optimizer)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This will print the optimizer object that is created from the configuration file.</p> </li> </ol>"},{"location":"s3_reproducibility/config_files/#final-exercise","title":"Final exercise","text":"<p>Make your MNIST code reproducible! Apply what you have just done to the simple script to your MNIST code. The only requirement is that you this time use multiple configuration files, meaning that you should have at least one <code>model_conf.yaml</code> file and a <code>training_conf.yaml</code> file that separates out the hyperparameters that have to do with the model definition and those that have to do with the training. You can also choose to work with even more complex config setups: in the image below the configuration has two layers such that we individually can specify hyperparameters belonging to a specific model architecture and hyperparameters for each individual optimizer that we may try.</p> <p> </p>  Image credit"},{"location":"s3_reproducibility/docker/","title":"M10 - Docker","text":""},{"location":"s3_reproducibility/docker/#docker","title":"Docker","text":"<p>Core Module</p> <p></p>  Image credit  <p>While the above picture may seem silly at first, it is actually pretty close to how Docker came into existence. A big part of creating an MLOps pipeline is being able to reproduce it. Reproducibility goes beyond versioning our code with <code>git</code> and using <code>conda</code> environments to keep track of our Python installations. To truly achieve reproducibility, we need to capture system-level components such as:</p> <ul> <li>Operating system</li> <li>Software dependencies (other than Python packages)</li> </ul> <p>Docker provides this kind of system-level reproducibility by creating isolated program dependencies. In addition to providing reproducibility, one of the key features of Docker is scalability, which is important when we later discuss deployment. Because Docker ensures system-level reproducibility, it does not (conceptually) matter whether we try to start our program on a single machine or on 1,000 machines at once.</p>"},{"location":"s3_reproducibility/docker/#docker-overview","title":"Docker Overview","text":"<p>Docker has three main concepts: Dockerfile, Docker image, and Docker container:</p> <p></p> <ul> <li> <p>A Dockerfile is a basic text document that contains all the commands a user could call on the command line to     run an application. This includes installing dependencies, pulling data from online storage, setting up code, and     specifying commands to run (e.g., <code>python train.py</code>).</p> </li> <li> <p>Running, or more correctly, building a Dockerfile will create a Docker image. An image is a lightweight,     standalone/containerized, executable package of software that includes everything (application code, libraries,     tools, dependencies, etc.) necessary to make an application run.</p> </li> <li> <p>Actually running an image will create a Docker container. This means that the same image can be launched     multiple times, creating multiple containers.</p> </li> </ul> <p>The exercises today will focus on how to construct the actual Dockerfile, as this is the first step to constructing your own container.</p>"},{"location":"s3_reproducibility/docker/#docker-sharing","title":"Docker Sharing","text":"<p>The whole point of using Docker is that sharing applications becomes much easier. In general, we have two options:</p> <ul> <li> <p>After creating the <code>Dockerfile</code>, we can simply commit it to GitHub (it's just a text file) and then ask other users     to simply build the image themselves.</p> </li> <li> <p>After building the image ourselves, we can choose to upload it to an image registry such as     Docker Hub, where others can get our image by simply running <code>docker pull</code>, allowing them     to instantaneously run it as a container, as shown in the figure below:</p> </li> </ul> <p></p>  Image credit"},{"location":"s3_reproducibility/docker/#exercises","title":"\u2754 Exercises","text":"<p>In the following exercises, we guide you on how to build a dockerfile for your MNIST repository that will make the training and prediction a self-contained application. Please make sure that you somewhat understand each step and do not just copy the exercise. Also, note that you probably need to execute the exercise from an elevated terminal, i.e. with administrative privilege.</p> <p>The exercises today are only an introduction to docker and some of the steps are going to be unoptimized from a production setting view. For example, we often want to keep the size of the docker image as small as possible, which we are not focusing on for these exercises.</p> <p>If you are using <code>VScode</code> then we recommend installing the VScode docker extension for easily getting an overview of which images have been building and which are running. Additionally, the extension named Dev Containers may also be beneficial for you to download.</p> <ol> <li> <p>Start by installing docker. How much trouble you need to go through     depends on your operating system. For Windows and Mac, we recommend you install Docker Desktop, which comes with     a graphical user interface (GUI) for quickly viewing docker images and docker containers currently built/in use.     Windows users that have not installed WSL yet are going to have to do it now (as docker needs it as a backend for     starting virtual machines) but you do not need to install docker in WSL. After installing docker we recommend that     you restart your laptop.</p> </li> <li> <p>Try running the following to confirm that your installation is working:</p> <pre><code>docker run hello-world\n</code></pre> <p>which should give the message</p> <pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre> </li> <li> <p>Next, let's try to download an image from Docker Hub. Download the <code>busybox</code> image:</p> <pre><code>docker pull busybox\n</code></pre> <p>which is a very small (1-5Mb) containerized application that contains the most essential GNU file utilities, shell utilities, etc.</p> </li> <li> <p>After pulling the image, write</p> <pre><code>docker images\n</code></pre> <p>which should show you all available images. You should see the <code>busybox</code> image that we just downloaded.</p> </li> <li> <p>Let's try to run this image.</p> <pre><code>docker run busybox\n</code></pre> <p>You will see that nothing happens! The reason for that is we did not provide any commands to <code>docker run</code>. We essentially just ask it to start the <code>busybox</code> virtual machine, do nothing, and then close it again. Now, try again, this time with</p> <pre><code>docker run busybox echo \"hello from busybox\"\n</code></pre> <p>Note how fast this process is. In just a few seconds, Docker is able to start a virtual machine, execute a command, and kill it afterward.</p> </li> <li> <p>Try running:</p> <pre><code>docker ps\n</code></pre> <p>What does this command do? What if you add <code>-a</code> to the end?</p> </li> <li> <p>If we want to run multiple commands within the virtual machine, we can start it in interactive mode.</p> <pre><code>docker run -it busybox\n</code></pre> <p>This can be a great way to investigate what the filesystem of our virtual machine looks like.</p> </li> <li> <p>As you may have already noticed by now, each time we execute <code>docker run</code>, we can still see small remnants of the     containers using <code>docker ps -a</code>. These stray containers can end up taking up a lot of disk space. To remove them,     use <code>docker rm</code> where you provide the container ID that you want to delete.</p> <pre><code>docker rm &lt;container_id&gt;\n</code></pre> <p>In general we recommend using the <code>--rm</code> flag when running a container, e.g.</p> <pre><code>docker run --rm &lt;image&gt;\n</code></pre> <p>which will automatically remove the container after it has finished running.</p> </li> <li> <p>Let's now move on to trying to construct a Dockerfile ourselves for our MNIST project. Create a file called     <code>train.dockerfile</code>. The intention is that we want to develop one Dockerfile for running our training script and     one for making predictions.</p> </li> <li> <p>Instead of starting from scratch, we nearly always want to start from some base image. For this exercise, we are     going to start from a simple <code>python</code> image. Add the following to your <code>Dockerfile</code>:</p> <pre><code># Base image\nFROM python:3.11-slim\n</code></pre> </li> <li> <p>Next, we are going to install some essentials in our image. The essentials more or less consist of a Python     installation. These instructions may seem familiar if you are using Linux:</p> <pre><code># Install Python\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> </li> <li> <p>The previous two steps are common for any Docker application where you want to run Python. All the remaining steps     are application-specific (to some degree):</p> <ol> <li> <p>Let's copy over our application (the essential parts) from our computer to the container:</p> <pre><code>COPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY src/ src/\nCOPY data/ data/\n</code></pre> <p>Remember that we only want the essential parts to keep our Docker image as small as possible. Why do we need each of these files/folders to run training in our Docker container?</p> </li> <li> <p>Let's set the working directory in our container and add commands that install the dependencies (1):</p> <ol> <li> <p> We split the installation into two steps so that Docker can cache our project dependencies     separately from our application code. This means that if we change our application code, we do not need to     reinstall all the dependencies. This is a common strategy for Docker images.</p> <p> As an alternative, you can use <code>RUN make requirements</code> if you have a <code>Makefile</code> that installs the dependencies. Just remember to also copy over the <code>Makefile</code> into the Docker image.</p> </li> </ol> <pre><code>WORKDIR /\nRUN pip install -r requirements.txt --no-cache-dir\nRUN pip install . --no-deps --no-cache-dir\n</code></pre> <p>The <code>--no-cache-dir</code> is quite important. Can you explain what it does and why it is important in relation to Docker?</p> </li> <li> <p>Finally, we are going to name our training script as the entrypoint for our Docker image. The entrypoint is     the application that we want to run when the image is executed:</p> <pre><code>ENTRYPOINT [\"python\", \"-u\", \"src/&lt;project-name&gt;/train.py\"]\n</code></pre> <p>The <code>\"u\"</code> here makes sure that any output from our script, e.g., any <code>print(...)</code> statements, gets redirected to our terminal. If not included, you would need to use <code>docker logs</code> to inspect your run.</p> </li> </ol> </li> <li> <p>We are now ready to build our Dockerfile into a Docker image.</p> <pre><code>docker build -f train.dockerfile . -t train:latest\n</code></pre> MAC M1/M2 users <p>In general, Docker images are built for a specific platform. For example, if you are using a Mac with an M1/M2 chip, then you are running on an ARM architecture. If you are using a Windows or Linux machine, then you are running on an AMD64 architecture. This is important to know when building Docker images. Thus, Docker images you build may not work on platforms different than the ones you build on. You can specify which platform you want to build for by adding the <code>--platform</code> argument to the <code>docker build</code> command:</p> <pre><code>docker build --platform linux/amd64 -f train.dockerfile . -t train:latest\n</code></pre> <p>and also when running the image:</p> <pre><code>docker run --platform linux/amd64 train:latest\n</code></pre> <p>Note that this will significantly increase the build and run time of your Docker image when running locally, because Docker will need to emulate the other platform. In general, for the exercises today, you should not need to specify the platform, but be aware of this if you are building Docker images on your own.</p> <p>Please note that here we are providing two extra arguments to <code>docker build</code>. The <code>-f train.dockerfile .</code> (the dot is important to remember) indicates which Dockerfile we want to run (except if you just named it <code>Dockerfile</code>) and the <code>-t train:latest</code> is the respective name and tag that we see afterward when running <code>docker images</code> (see image below). Please note that building a Docker image can take a couple of minutes.</p> <p> </p> Docker images and space <p>Docker images can take up a lot of space on your computer, especially the Docker images we are trying to build because PyTorch is a huge dependency. If you are running low on space, you can try to</p> <pre><code>docker system prune\n</code></pre> <p>Alternatively, you can manually delete images using <code>docker rm {image_name}:{image_tag}</code>.</p> </li> <li> <p>Try running <code>docker images</code> and confirm that you get output similar to the above. If you succeed with this,     then try running the docker image.</p> <pre><code>docker run --name experiment1 train:latest\n</code></pre> <p>You should hopefully see your training starting. Please note that we can start as many containers as we want at the same time by giving them all different names using the <code>--name</code> tag.</p> <ol> <li> <p>You are most likely going to rebuild your Docker image multiple times, either due to an implementation error     or the addition of new functionality. Therefore, instead of watching pip suffer through downloading <code>torch</code> for     the 20th time, you can reuse the cache from the last time the Docker image was built. To do this, replace the line     in your Dockerfile that installs your requirements with:</p> <pre><code>RUN --mount=type=cache,target=/root/.cache/pip pip install -r requirements.txt\n</code></pre> <p>which mounts your local pip cache to the Docker image. For building the image, you need to have enabled the BuildKit feature. If you have Docker version v23.0 or later (you can check this by running <code>docker version</code>), then this is enabled by default. Otherwise, you need to enable it by setting the environment variable <code>DOCKER_BUILDKIT=1</code> before building the image.</p> <p>Try changing your Dockerfile and rebuilding the image. You should see that the build process is much faster.</p> </li> </ol> </li> <li> <p>Remember, if you are ever in doubt about how files are organized inside a Docker image, you always have the option     of starting the image in interactive mode:</p> <pre><code>docker run --rm -it --entrypoint sh {image_name}:{image_tag}\n</code></pre> </li> <li> <p>When your training has completed you will notice that any files that are created when running your training script     are not present on your laptop (for example if your script is saving the trained model to a file). This is because     the files were created inside your container (which is a separate little machine). To get the files you have two     options:</p> <ol> <li> <p>If you already have a completed run then you can use</p> <pre><code>docker cp\n</code></pre> <p>to copy the files between your container and laptop. For example to copy a file called <code>trained_model.pt</code> from a folder you would do:</p> <pre><code>docker cp {container_name}:{dir_path}/{file_name} {local_dir_path}/{local_file_name}\n</code></pre> <p>Try this out.</p> </li> <li> <p>A much more efficient strategy is to mount a volume that is shared between the host (your laptop) and the     container. This can be done with the <code>-v</code> option for the <code>docker run</code> command. For example, if we want to     automatically get the <code>trained_model.pt</code> file after running our training script we could simply execute the     container as</p> <pre><code>docker run --name {container_name} -v %cd%/models:/models/ train:latest\n</code></pre> <p>This command mounts our local <code>models</code> folder as a corresponding <code>models</code> folder in the container. Any file saved by the container to this folder will be synchronized back to our host machine. Try this out! Note if you have multiple files/folders that you want to mount (if in doubt about file organization in the container try to do the next exercise first). Also note that the <code>%cd%</code> needs to change depending on your OS, see this page for help.</p> </li> </ol> </li> <li> <p>With training done we also need to write an application for prediction. Create a new docker image called     <code>evaluate.dockerfile</code>. This file should call your <code>src/&lt;project-name&gt;/evaluate.py</code> script instead. This image     will need some trained model weights to work. Feel free to either include these during the build process or mount     them afterwards. When you create the file try to <code>build</code> and <code>run</code> it to confirm that it works. Hint: if     you are passing in the model checkpoint and evaluation data as arguments to your script, your <code>docker run</code> probably     needs to look something like</p> <pre><code>docker run --name evaluate --rm \\\n    -v %cd%/trained_model.pt:/models/trained_model.pt \\  # mount trained model file\n    -v %cd%/data/test_images.pt:/test_images.pt \\  # mount data we want to evaluate on\n    -v %cd%/data/test_targets.pt:/test_targets.pt \\\n    evaluate:latest \\\n    ../../models/trained_model.pt \\  # argument to script, path relative to script location in container\n</code></pre> </li> <li> <p>(Optional, requires GPU support) By default, a virtual machine created by docker only has access to your <code>cpu</code> and     not your <code>gpu</code>. While you do not necessarily have a laptop with a GPU that supports the training of neural networks     (e.g. one from Nvidia) it is beneficial that you understand how to construct a docker image that can take advantage     of a GPU if you were to run this on a machine in the future that has a GPU (e.g. in the cloud). It does take a bit     more work, but many of the steps will be similar to building a normal docker image.</p> <ol> <li> <p>There are three prerequisites for working with Nvidia GPU-accelerated docker containers. First, you need to have     the Docker Engine installed (already taken care of), have an Nvidia GPU with updated GPU drivers and finally have     the Nvidia container toolkit     installed. The last part you likely have not installed and need to do. Some distros of Linux have known     problems with the installation process, so you may have to search through known issues in     nvidia-docker repository to find a solution.</p> </li> <li> <p>To test that everything is working start by pulling a relevant Nvidia docker image. In my case this is     the correct image:</p> <pre><code>docker pull nvidia/cuda:11.0.3-base-ubuntu20.04\n</code></pre> <p>but it may differ based on what Cuda version you have. You can find all the different official Nvidia images here. After pulling the image, try running the <code>nvidia-smi</code> command inside a container based on the image you just pulled. It should look something like this:</p> <pre><code>docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi\n</code></pre> <p>and should show an image like below:</p> <p> </p> <p>If it does not work, try redoing the steps.</p> </li> <li> <p>We should hopefully have a working setup now for running Nvidia accelerated docker containers. The next step is     to get PyTorch inside our container, such that our PyTorch implementation also correctly identifies the GPU.     Luckily for us, Nvidia provides a set of docker images for GPU-optimized software for AI, HPC and visualizations     through their NGC Catalog.     The containers that have to do with PyTorch can be seen     here.     Try pulling the latest one:</p> <pre><code>docker pull nvcr.io/nvidia/pytorch:22.07-py3\n</code></pre> <p>It may take some time because the NGC images include a lot of other software for optimizing PyTorch applications. It may be possible for you to find other images for running GPU-accelerated applications that have a smaller memory footprint, but NGC is the recommended and supported way.</p> </li> <li> <p>Let's test that this container works:</p> <pre><code>docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:22.07-py3\n</code></pre> <p>This should run the container in interactive mode attached to your current terminal. Try opening <code>python</code> in the container and try writing:</p> <pre><code>import torch\nprint(torch.cuda.is_available())\n</code></pre> <p>which should hopefully return <code>True</code>.</p> </li> <li> <p>Finally, we need to incorporate all this into our already developed docker files for our application. This is     also fairly easy as we just need to change our <code>FROM</code> statement at the beginning of our docker file:</p> <pre><code>FROM python:3.11-slim\n</code></pre> <p>change to</p> <pre><code>FROM  nvcr.io/nvidia/pytorch:22.07-py3\n</code></pre> <p>try doing this to one of your dockerfiles, build the image and run the container. Remember to check that your application is using GPU by printing <code>torch.cuda.is_available()</code>.</p> </li> </ol> </li> <li> <p>(Optional) Another way you can use Dockerfiles in your day-to-day work is for Dev-containers. Developer containers     allow you to develop code directly inside a container, making sure that your code is running in the same     environment as it will when deployed. This is especially useful if you are working on a project that has a lot of     dependencies that are hard to install on your local machine. Setup instructions for VS Code and PyCharm can be found     here (should be simple since we have already installed Docker):</p> <ul> <li>VS Code</li> <li>PyCharm</li> </ul> <p>We will focus on the VS Code setup here.</p> <ol> <li> <p>First, install the     Remote - Containers     extension.</p> </li> <li> <p>Create a <code>.devcontainer</code> folder in your project root and create a <code>Dockerfile</code> inside it. We will keep this file very     barebones for now, so let's just define a base installation of Python:</p> <pre><code>FROM python:3.11-slim\n\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n</code></pre> </li> <li> <p>Create a <code>devcontainer.json</code> file in the <code>.devcontainer</code> folder. This file should look something like this:</p> <pre><code>{\n    \"name\": \"my_working_env\",\n    \"dockerFile\": \"Dockerfile\",\n    \"postCreateCommand\": \"pip install -r requirements.txt\"\n}\n</code></pre> <p>This file tells VS Code that we want to use the <code>Dockerfile</code> that we just created and that we want to install our Python dependencies after the container has been created.</p> </li> <li> <p>After creating these files, you should be able to open the command palette in VS Code (F1) and search for the     option <code>Remote-Containers: Reopen in Container</code> or <code>Remote-Containers: Rebuild and Reopen in Container</code>. Choose     either of these options.</p> <p> </p> <p>This will start a new VS Code instance inside a Docker container. You should be able to see this in the bottom left corner of your VS Code window. You should also be able to see that the Python interpreter has changed to the one inside the container.</p> <p>You are now ready to start developing inside the container. Try opening a terminal and run <code>python</code> and <code>import torch</code> to confirm that everything is working.</p> </li> </ol> </li> <li> <p>(Optional) In M8 on Data version control you learned about the     framework <code>dvc</code> for version controlling data. A natural question at this point would then be how to incorporate     <code>dvc</code> into our docker image. We need to do two things:</p> <ul> <li>Make sure that <code>dvc</code> has all the correct files to pull data from our remote storage</li> <li>Make sure that <code>dvc</code> has the correct credentials to pull data from our remote storage</li> </ul> <p>We are going to assume that <code>dvc</code> (and any <code>dvc</code> extension needed) is part of your <code>requirements.txt</code> file and that it is already being installed in a <code>RUN pip install -r requirements.txt</code> command in your Dockerfile. If not, then you need to add it.</p> <ol> <li> <p>Add the following lines to your Dockerfile</p> <pre><code>RUN dvc init --no-scm\nCOPY .dvc/config .dvc/config\nCOPY *.dvc .dvc/\nRUN dvc config core.no_scm true\nRUN dvc pull\n</code></pre> <p>The first line initializes <code>dvc</code> in the Docker image. The <code>--no-scm</code> option is needed because normally <code>dvc</code> can only be initialized inside a git repository, but this option allows initializing <code>dvc</code> without being in one. The second and third lines copy over the <code>dvc</code> config file and the <code>dvc</code> metadata files that are needed to pull data from your remote storage. The last line pulls the data.</p> </li> <li> <p>If your data is not public, we need to provide credentials in some way to pull the data. We are for now going to     do it in a not-so-secure way. When <code>dvc</code> first connected to your drive, a credential file was created. This file     is located in <code>$CACHE_HOME/pydrive2fs/{gdrive_client_id}/default.json</code> where <code>$CACHE_HOME</code>.</p> macOSLinuxWindows <p><code>~/Library/Caches</code></p> <p><code>~/.cache</code>  This is the typical location, but it may vary depending on what distro you are running.</p> <p><code>{user}/AppData/Local</code></p> <p>Find the file. The content should look similar to this (only some fields are shown):</p> <pre><code>{\n    \"access_token\": ...,\n    \"client_id\": ...,\n    \"client_secret\": ...,\n    \"refresh_token\": ...,\n    ...\n}\n</code></pre> <p>We are going to copy the file into our Docker image. This, of course, is not a secure way of doing it, but it is the easiest way to get started. As long as you are not sharing your Docker image with anyone else, then it is fine. Add the following lines to your Dockerfile before the <code>RUN dvc pull</code> command:</p> <pre><code>COPY &lt;path_to_default.json&gt; default.json\ndvc remote modify myremote --local gdrive_service_account_json_file_path default.json\n</code></pre> <p>where <code>&lt;path_to_default.json&gt;</code> is the path to the <code>default.json</code> file that you just found. The last line tells <code>dvc</code> to use the <code>default.json</code> file as the credentials for pulling data from your remote storage. You can confirm that this works by running <code>dvc pull</code> in your Docker image.</p> </li> </ol> </li> </ol>"},{"location":"s3_reproducibility/docker/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>What is the difference between a docker image and a docker container?</p> Solution <p>A Docker image is a template for a Docker container. A Docker container is a running instance of a Docker image. A Docker image is a static file, while a Docker container is a running process.</p> </li> <li> <p>What are the 3 steps involved in containerizing an application?</p> Solution <ol> <li>Write a Dockerfile that includes your app (including the commands to run it) and its dependencies.</li> <li>Build the image using the Dockerfile you wrote.</li> <li>Run the container using the image you've built.</li> </ol> </li> <li> <p>What advantage is there to running your application inside a Docker container instead of running the application     directly on your machine?</p> Solution <p>Running inside a Docker container gives you a consistent and independent environment for your application. This means that you can be sure that your application will run the same way on your machine as it will on another machine. Thus, Docker gives the ability to abstract away the differences between different machines.</p> </li> <li> <p>A Docker container is built from a series of layers that are stacked on top of each other. This should be clear if     you look at the output when building a Docker image. What is the advantage of this?</p> Solution <p>The advantage is efficiency and reusability. When a change is made to a Docker image, only the layer(s) that are changed need to be updated. For example, if you update the application code in your Docker image, which usually is the last layer, then only that layer needs to be rebuilt, making the process much faster. Additionally, if you have multiple Docker images that share the same base image, then the base image only needs to be downloaded once.</p> </li> </ol> <p>This covers the absolute minimum you should know about Docker to get a working image and container. If you want to really deep dive into this topic, you can find a copy of the Docker Cookbook by S\u00e9bastien Goasguen in the literature folder.</p> <p>If you are actively going to be using Docker in the future, one thing to consider is the image size. Even these simple images that we have built still take up GB in size. Several optimization steps can be taken to reduce the image size for you or your end user. If you have time, you can read this article on different approaches to reducing image size. Additionally, you can take a look at the dive-in extension for Docker Desktop that lets you explore your Docker images in depth.</p>"},{"location":"s4_debugging_and_logging/","title":"Debugging, Profiling, Logging and Boilerplate","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn how to use the debugger in your editor to find bugs in your code.</p> <p> M12: Debugging</p> </li> <li> <p></p> <p>Learn how to use a profiler to identify bottlenecks in your code and from those profiles optimize the runtime of your programs.</p> <p> M13: Profiling</p> </li> <li> <p></p> <p>Learn how to systematically log experiments and hyperparameters to make your code reproducible.</p> <p> M14: Logging</p> </li> <li> <p></p> <p>Learn how to use the <code>pytorch-lightning</code> framework to minimize boilerplate code and structure deep learning models.</p> <p> M15: Boilerplate</p> </li> </ul> <p>Today we are initially going to go over three different topics that are all fundamentally necessary for any data scientist or DevOps engineer:</p> <ul> <li>Debugging</li> <li>Profiling</li> <li>Logging</li> </ul> <p>All three topics can be characterized by something you are probably already familiar with. Since you started programming, you have done debugging, since nobody can write perfect code on the first try. Similarly, while you have not directly profiled your code, I bet that you at some point have had some very slow code and optimized it to run faster. Identifying and improving are the fundamentals of profiling code. Finally, logging is a very broad term and refers to any kind of output from your applications that helps you at a later point identify the \"performance\" of your application.</p> <p>However, while we expect you to already be familiar with these topics, we do not expect all of you to be experts, as these topics are rarely focused on. Today we are going to introduce some best practices and tools to help you overcome every one of these three important topics. As the final topic for today, we are going to learn about how we can minimize boilerplate and focus on coding what matters for our project instead of all the boilerplate to get it working.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the basics of debugging and how to use a debugger to find bugs in your code</li> <li>Be able to use a profiler to identify bottlenecks in your code and from those profiles optimize the runtime of     your programs</li> <li>Be familiar with an experiment logging framework for tracking experiments and hyperparameters of your code to     make it reproducible</li> <li>Be able to use the <code>pytorch-lightning</code> framework to minimize boilerplate code and structure deep learning models</li> </ul>"},{"location":"s4_debugging_and_logging/boilerplate/","title":"M15 - Boilerplate","text":""},{"location":"s4_debugging_and_logging/boilerplate/#minimizing-boilerplate","title":"Minimizing boilerplate","text":"<p>Boilerplate is a general term that describes any standardized text, copy, documents, methods, or procedures that may be used over and over again without making major changes to the original. But how does this relate to doing machine learning projects? If you have already tried doing a couple of projects within machine learning you will probably have seen a pattern: every project usually consists of these three aspects of code:</p> <ul> <li>a model implementation</li> <li>some training code</li> <li>a collection of utilities for saving models, logging images, etc.</li> </ul> <p>While the latter two certainly seem important, in most cases the actual development or research often revolves around defining the model. In this sense, both the training code and the utilities become boilerplate that should just carry over from one project to another. But the problem is that we usually have not generalized our training code to take care of the small adjustments that may be required in future projects and we therefore end up implementing it over and over again every time we start a new project. This is of course a waste of our time that we should try to find a solution to.</p> <p>This is where high-level frameworks come into play. High-level frameworks are built on top of another framework (PyTorch in this case) and try to abstract/standardize how to do particular tasks such as training. At first it may seem irritating that you need to comply with someone else's code structure, however there is a good reason for that. The idea is that you can focus on what really matters (your task, model architecture etc.) and do not have to worry about the actual boilerplate that comes with it.</p> <p>The most popular high-level (training) frameworks within the <code>PyTorch</code> ecosystem are:</p> <ul> <li>fast.ai</li> <li>Ignite</li> <li>skorch</li> <li>Catalyst</li> <li>Composer</li> <li>PyTorch Lightning</li> </ul> <p>They all offer many of the same features, so choosing one over the other for most projects should not matter that much. We are here going to use <code>PyTorch Lightning</code>, as it offers all the functionality that we are going to need later in the course.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#pytorch-lightning","title":"PyTorch Lightning","text":"<p>In general refer to the documentation from PyTorch lightning if in doubt about how to format your code for doing specific tasks. We are here going to explain the key concepts of the API that you need to understand to use the framework, starting with the <code>LightningModule</code> and the <code>Trainer</code>.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#lightningmodule","title":"LightningModule","text":"<p>The <code>LightningModule</code> is a subclass of a standard <code>nn.Module</code> that basically adds additional structure. In addition to the standard <code>__init__</code> and <code>forward</code> methods that need to be implemented in a <code>nn.Module</code>, a <code>LightningModule</code> further requires two more methods implemented:</p> <ul> <li> <p><code>training_step</code>: should contain your actual training code e.g. given a batch of data this should return the loss     that you want to optimize</p> </li> <li> <p><code>configure_optimizers</code>: should return the optimizer that you want to use</p> </li> </ul> <p>Below these two methods are shown added to the standard MNIST classifier</p> <p></p> <p>Compared to a standard <code>nn.Module</code>, the additional methods in the <code>LightningModule</code> basically specify exactly how you want to optimize your model.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#trainer","title":"Trainer","text":"<p>The second component to lightning is the <code>Trainer</code> object. As the name suggests, the <code>Trainer</code> object takes care of the actual training, automating everything that you do not want to worry about.</p> <pre><code>from pytorch_lightning import Trainer\nmodel = MyAwesomeModel()  # this is our LightningModule\ntrainer = Trainer()\ntraier.fit(model)\n</code></pre> <p>That's essentially all that you need to specify in lightning to have a working model. The trainer object does not have methods that you need to implement yourself, but it has a bunch of arguments that can be used to control how many epochs you want to train, if you want to run on gpu, etc. To get the training of our model to work we just need to specify how our data should be fed into the lightning framework.</p>"},{"location":"s4_debugging_and_logging/boilerplate/#data","title":"Data","text":"<p>For organizing our code that has to do with data in <code>Lightning</code> we essentially have three different options. However, all three assume that we are using <code>torch.utils.data.DataLoader</code> for the data loading.</p> <ol> <li> <p>If we already have a <code>train_dataloader</code> and possibly also a <code>val_dataloader</code> and <code>test_dataloader</code> defined we can     simply add them to our <code>LightningModule</code> using the similarly named methods:</p> <pre><code>def train_dataloader(self):\n    return DataLoader(...)\n\ndef val_dataloader(self):\n    return DataLoader(...)\n\ndef test_dataloader(self):\n    return DataLoader(...)\n</code></pre> </li> <li> <p>Maybe even simpler, we can directly feed such dataloaders into the <code>fit</code> method of the <code>Trainer</code> object:</p> <pre><code>trainer.fit(model, train_dataloader, val_dataloader)\ntrainer.test(model, test_dataloader)\n</code></pre> </li> <li> <p>Finally, <code>Lightning</code> also has the <code>LightningDataModule</code> that organizes data loading into a single structure, see     this page for more info. Putting     data loading into a <code>DataModule</code> makes sense as it can then be reused between projects.</p> </li> </ol>"},{"location":"s4_debugging_and_logging/boilerplate/#callbacks","title":"Callbacks","text":"<p>Callbacks are one way to add additional functionality to your model, that strictly speaking is not already part of your model. Callbacks should therefore be seen as a self-contained feature that can be reused between projects. You have the option of implementing callbacks yourself (by inheriting from the <code>pytorch_lightning.callbacks.Callback</code> base class) or using one of the built-in callbacks. Of particular interest are the <code>ModelCheckpoint</code> and <code>EarlyStopping</code> callbacks:</p> <ul> <li> <p>The <code>ModelCheckpoint</code> makes sure to save checkpoints of your model. This is in principal not hard to do yourself, but     the <code>ModelCheckpoint</code> callback offers additional functionality by saving checkpoints only when some metric improves,     or only save the best <code>K</code> performing models, etc.</p> <pre><code>model = MyModel()\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"./models\", monitor=\"val_loss\", mode=\"min\"\n)\ntrainer = Trainer(callbacks=[checkpoint_callbacks])\ntrainer.fit(model)\n</code></pre> </li> <li> <p>The <code>EarlyStopping</code> callback can help you prevent overfitting by automatically stopping the training if a certain     value is not improving anymore:</p> <pre><code>model = MyModel()\nearly_stopping_callback = EarlyStopping(\n    monitor=\"val_loss\", patience=3, verbose=True, mode=\"min\"\n)\ntrainer = Trainer(callbacks=[early_stopping_callback])\ntrainer.fit(model)\n</code></pre> </li> </ul> <p>Multiple callbacks can be used by passing them all in a list e.g.</p> <pre><code>trainer = Trainer(callbacks=[checkpoint_callbacks, early_stopping_callback])\n</code></pre>"},{"location":"s4_debugging_and_logging/boilerplate/#exercises","title":"\u2754 Exercises","text":"<p>Please note that in the following exercise we will basically ask you to reformat all your MNIST code to follow the lightning standard, such that we can take advantage of all the tricks the framework has to offer. The reason we did not implement our model in <code>lightning</code> to begin with is that to truly understand why it is beneficial to use a high-level framework to do some of the heavy lifting you need to have gone through some implementation troubles yourself.</p> <ol> <li> <p>Install pytorch lightning:</p> <pre><code>pip install pytorch-lightning # (1)!\n</code></pre> <ol> <li> You may also install it as <code>pip install lightning</code> which includes more than just the     <code>PyTorch Lightning</code> package. This also includes <code>Lightning Fabric</code> and <code>Lightning Apps</code> which you can read more     about here and here.</li> </ol> </li> <li> <p>Convert your corrupted MNIST model into a <code>LightningModule</code>. You can either choose to completely overwrite your old     model or implement it in a new file. The bare minimum that you need to add while converting to get it working with     the rest of lightning:</p> <ul> <li> <p>The <code>training_step</code> method. This function should contain essentially what goes into a single     training step and should return the loss at the end</p> </li> <li> <p>The <code>configure_optimizers</code> method</p> </li> </ul> <p>Please read the documentation for more info.</p> Solution lightning.py<pre><code>import pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\n\nclass MyAwesomeModel(pl.LightningModule):\n    \"\"\"My awesome model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(128, 10)\n\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\"\"\"\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.relu(self.conv3(x))\n        x = torch.max_pool2d(x, 2, 2)\n        x = torch.flatten(x, 1)\n        x = self.dropout(x)\n        return self.fc1(x)\n\n    def training_step(self, batch):\n        \"\"\"Training step.\"\"\"\n        img, target = batch\n        y_pred = self(img)\n        return self.loss_fn(y_pred, target)\n\n    def configure_optimizers(self):\n        \"\"\"Configure optimizer.\"\"\"\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\n\nif __name__ == \"__main__\":\n    model = MyAwesomeModel()\n    print(f\"Model architecture: {model}\")\n    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n\n    dummy_input = torch.randn(1, 1, 28, 28)\n    output = model(dummy_input)\n    print(f\"Output shape: {output.shape}\")\n</code></pre> </li> <li> <p>Make sure your data is formatted such that it can be loaded using the <code>torch.utils.data.DataLoader</code> object.</p> </li> <li> <p>Instantiate a <code>Trainer</code> object. It is recommended to take a look at the     trainer arguments (there     are many of them) and maybe adjust some of them:</p> <ol> <li> <p>Investigate what the <code>default_root_dir</code> flag does.</p> </li> <li> <p>As default lightning will run for 1000 epochs. This may be too much (for now). Change this by     changing the appropriate flag. Additionally, there is also a flag to set the maximum number of steps that we     should train for.</p> Solution <p>Setting the <code>max_epochs</code> will accomplish this.</p> <pre><code>trainer = Trainer(max_epochs=10)\n</code></pre> <p>Additionally, you may consider instead setting the <code>max_steps</code> flag to limit based on the number of steps or <code>max_time</code> to limit based on time. Similarly, the flags <code>min_epochs</code>, <code>min_steps</code> and <code>min_time</code> can be used to set the minimum number of epochs, steps or time.</p> </li> <li> <p>To start with we also want to limit the amount of training data to 20% of its original size. Which     trainer flag do you need to set for this to work?</p> Solution <p>Setting the <code>limit_train_batches</code> flag will accomplish this.</p> <pre><code>trainer = Trainer(limit_train_batches=0.2)\n</code></pre> <p>Similarly, you can also set the <code>limit_val_batches</code> and <code>limit_test_batches</code> flags to limit the validation and test data.</p> </li> </ol> </li> <li> <p>Try fitting your model: <code>trainer.fit(model)</code></p> </li> <li> <p>Now try adding some <code>callbacks</code> to your trainer.</p> Solution <pre><code>early_stopping_callback = EarlyStopping(\n    monitor=\"val_loss\", patience=3, verbose=True, mode=\"min\"\n)\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"./models\", monitor=\"val_loss\", mode=\"min\"\n)\ntrainer = Trainer(callbacks=[early_stopping_callback, checkpoint_callback])\n</code></pre> </li> <li> <p>The previous module was all about logging in <code>wandb</code>, so the question is naturally how does <code>lightning</code> support this.     Lightning does not only support <code>wandb</code>, but also many     others. Common to all of them is that     logging just needs to happen through the <code>self.log</code> method in your <code>LightningModule</code>:</p> <ol> <li> <p>Add <code>self.log</code> to your `LightningModule. It should look something like this:</p> <pre><code>def training_step(self, batch, batch_idx):\n    data, target = batch\n    preds = self(data)\n    loss = self.criterion(preds, target)\n    acc = (target == preds.argmax(dim=-1)).float().mean()\n    self.log('train_loss', loss)\n    self.log('train_acc', acc)\n    return loss\n</code></pre> </li> <li> <p>Add the <code>wandb</code> logger to your trainer</p> <pre><code>trainer = Trainer(logger=pl.loggers.WandbLogger(project=\"dtu_mlops\"))\n</code></pre> <p>and try to train the model. Confirm that you are seeing the scalars appearing in your <code>wandb</code> portal.</p> </li> <li> <p><code>self.log</code> does sadly only support logging scalar tensors. Luckily, for logging other quantities we     can still access the standard <code>wandb.log</code> through our model.</p> <pre><code>def training_step(self, batch, batch_idx):\n    ...\n    # self.logger.experiment is the same as wandb.log\n    self.logger.experiment.log({'logits': wandb.Histrogram(preds)})\n</code></pre> <p>Try doing this by logging something other than scalar tensors.</p> </li> </ol> </li> <li> <p>Finally, we maybe also want to do some validation or testing. In lightning we just need to add the <code>validation_step</code>     and <code>test_step</code> to our lightning module and supply the respective data in the form of a separate dataloader. Try to at     least implement one of them.</p> Solution <p>Both the validation and test steps can be implemented in the same way as the training step:</p> <pre><code>def validation_step(self, batch) -&gt; None:\n    data, target = batch\n    preds = self(data)\n    loss = self.criterion(preds, target)\n    acc = (target == preds.argmax(dim=-1)).float().mean()\n    self.log('val_loss', loss, on_epoch=True)\n    self.log('val_acc', acc, on_epoch=True)\n</code></pre> <p>Two things to note here are that we are setting the <code>on_epoch</code> flag to <code>True</code> in the <code>self.log</code> method. This is because we want to log the validation loss and accuracy only once per epoch. Additionally, we are not returning anything from the <code>validation_step</code> method because we do not optimize over the loss.</p> </li> <li> <p>(Optional, requires GPU) One of the big advantages of using <code>lightning</code> is that you do not need to deal with device     placement, i.e. calling <code>.to('cuda')</code> everywhere. If you have a GPU, try to set the <code>gpus</code> flag in the trainer. If you     do not have one, do not worry, we are going to return to this when we run training in the cloud.</p> Solution <p>The two arguments <code>accelerator</code> and <code>devices</code> can be used to specify which devices to run on and how many to run on. For example, to run on a single GPU you can do</p> <pre><code>trainer = Trainer(accelerator=\"gpu\", devices=1)\n</code></pre> <p>As an alternative the accelerator can just be set to <code>accelerator=\"auto\"</code> to automatically detect the best available device.</p> </li> <li> <p>(Optional) As default PyTorch uses <code>float32</code> for representing floating point numbers. However, research has shown     that neural network training is very robust towards a decrease in precision. The great benefit of going from <code>float32</code>     to <code>float16</code> is that we get approximately half the     memory consumption. Try out half-precision     training in PyTorch lightning. You can enable this by setting the     precision flag in the <code>Trainer</code>.</p> Solution <p>Lightning supports four different types of mixed precision training (16-bit and 16-bit bfloat) and two types of:</p> <pre><code># 16-bit mixed precision (model weights remain in torch.float32)\ntrainer = Trainer(precision=\"16-mixed\", devices=1)\n\n# 16-bit bfloat mixed precision (model weights remain in torch.float32)\ntrainer = Trainer(precision=\"bf16-mixed\", devices=1)\n\n# 16-bit precision (model weights get cast to torch.float16)\ntrainer = Trainer(precision=\"16-true\", devices=1)\n\n# 16-bit bfloat precision (model weights get cast to torch.bfloat16)\ntrainer = Trainer(precision=\"bf16-true\", devices=1)\n</code></pre> </li> <li> <p>(Optional) Lightning also has built-in support for profiling. Check out how to do this using the     profiler argument in     the <code>Trainer</code> object.</p> </li> <li> <p>(Optional) Another great feature of Lightning is that it allows for easily defining command line interfaces through     the Lightning CLI feature. The     Lightning CLI is essentially a drop in replacement for defining command line interfaces (covered in     this module) and can also replace the need for config files     (covered in this module) for securing reproducibility when working inside     the Lightning framework. We highly recommend checking out the feature and that you try to refactor your code such     that you do not need to call <code>trainer.fit</code> anymore but it is instead directly controlled from the Lightning CLI.</p> </li> <li> <p>Free exercise: Experiment with what the lightning framework is capable of. Either try out more of the trainer flags,     some of the other callbacks, or maybe look into some of the other methods that can be implemented in your lightning     module. Only your imagination is the limit!</p> </li> </ol> <p>That covers everything for today. It has been a mix of topics that should all help you write \"better\" code (by some objective measure). If you want to deep dive more into the PyTorch lightning framework, we highly recommend looking at the different tutorials in the documentation that cover more advanced models and training cases. Additionally, we also want to highlight other frameworks in the lightning ecosystem:</p> <ul> <li>Torchmetrics: collection of machine learning metrics written     in PyTorch</li> <li>lightning flash: high-level framework for fast prototyping,     baselining, finetuning with an even simpler interface than lightning</li> <li>lightning-bolts: Collection of SOTA pretrained models, model     components, callbacks, losses and datasets for testing out ideas as fast as possible</li> </ul>"},{"location":"s4_debugging_and_logging/debugging/","title":"M12 - Debugging","text":""},{"location":"s4_debugging_and_logging/debugging/#debugging","title":"Debugging","text":"<p>Debugging is very hard to teach and is one of the skills that just comes with experience. That said, there are good and bad ways to debug a program. We are all probably familiar with just inserting <code>print(...)</code> statements everywhere in our code. It is easy and can many times help narrow down where the problem happens. That said, this is not a great way of debugging when dealing with a very large codebase. You should therefore familiarize yourself with the built-in python debugger as it may come in handy during the course.</p> <p></p> <p>To invoke the built-in Python debugger you can either:</p> <ul> <li> <p>Set a trace directly with the Python debugger by calling</p> <pre><code>import pdb\npdb.set_trace()\n</code></pre> <p>anywhere you want to stop the code. Then you can use different commands (see the <code>python_debugger_cheatsheet.pdf</code>) to step through the code.</p> </li> <li> <p>If you are using an editor, then you can insert inline breakpoints (in VS Code this can be done by pressing <code>F9</code>)     and then execute the script in debug mode (inline breakpoints can often be seen as small red dots to the left of     your code). The editor should then offer some interface to allow you step through your code. Here is a guide to     using the built-in debugger in VScode.</p> </li> <li> <p>Additionally, if your program is stopping on an error and you automatically want to start the debugger where it     happens, then you can simply launch the program like this from the terminal</p> <pre><code>python -m pdb -c continue my_script.py\n</code></pre> </li> </ul>"},{"location":"s4_debugging_and_logging/debugging/#exercises","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>We here provide a script <code>vae_mnist_bugs.py</code> which contains a number of bugs that need to be fixed to get it running. Start by going over the script and try to understand what is going on. Afterwards, try to get it running by fixing the bugs. The following bugs exist in the script:</p> <ul> <li>One device bug (will only show if running on gpu, but try to find it anyways)</li> <li>One shape bug</li> <li>One math bug</li> <li>One training bug</li> </ul> <p>Some of the bugs prevent the script from running at all, while some of them influence the training dynamics. Try to find them all. We also provide a working version called <code>vae_mnist_working.py</code> (but please try to find the bugs before looking at the script). Successfully debugging and running the script should produce three files:</p> <ul> <li><code>orig_data.png</code> containing images from the standard MNIST training set</li> <li><code>reconstructions.png</code> reconstructions from the model</li> <li><code>generated_samples.png</code> samples from the model</li> </ul> <p>Again, we cannot stress enough that the exercise is not actually about finding the bugs but using a proper debugger to find them.</p> Solution for device bug <p>If you look at the reparameterization function in the <code>Encoder</code> class you can see that we initialize a noise tensor</p> <pre><code>def reparameterization(self, mean, var):\n    \"\"\"Reparameterization trick to sample z values.\"\"\"\n    epsilon = torch.randn(*var.shape)\n    return mean + var * epsilon\n</code></pre> <p>this will fail with a <code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</code> if you are running on GPU, because the noise tensor is initialized on the CPU. You can fix this by initializing the noise tensor on the same device as the mean and var tensors.</p> <pre><code>def reparameterization(self, mean, var):\n    \"\"\"Reparameterization trick to sample z values.\"\"\"\n    epsilon = torch.randn(*var.shape, device=mean.device)\n    return mean + var * epsilon\n</code></pre> Solution for shape bug <p>In the <code>Decoder</code> class we initialize the following fully connected layers</p> <pre><code>self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\nself.FC_output = nn.Linear(latent_dim, output_dim)\n</code></pre> <p>which are used in the forward pass as</p> <pre><code>def forward(self, x):\n    \"\"\"Forward pass of the decoder module.\"\"\"\n    h = torch.relu(self.FC_hidden(x))\n    return torch.sigmoid(self.FC_output(h))\n</code></pre> <p>This means that <code>h</code> should be a tensor of shape <code>[bs, hidden_dim]</code>, but since we initialize the <code>FC_output</code> layer with <code>latent_dim</code> output dimensions, the forward pass will fail with a <code>RuntimeError: size mismatch, m1: [bs, hidden_dim], m2: [bs, latent_dim]</code> if <code>hidden_dim != latent_dim</code>. You can fix this by initializing the <code>FC_output</code> layer with <code>hidden_dim</code> output dimensions.</p> <pre><code>self.FC_output = nn.Linear(hidden_dim, output_dim)\n</code></pre> Solution for math bug <p>In the Encoder class you have the following code:</p> <pre><code>def forward(self, x):\n    \"\"\"Forward pass of the encoder module.\"\"\"\n    h_ = torch.relu(self.FC_input(x))\n    mean = self.FC_mean(h_)\n    log_var = self.FC_var(h_)\n    z = self.reparameterization(mean, log_var)\n    return z, mean, log_var\n\ndef reparameterization(self, mean, var):\n    \"\"\"Reparameterization trick to sample z values.\"\"\"\n    epsilon = torch.randn(*var.shape)\n    return mean + var * epsilon\n</code></pre> <p>From just the naming of the variables you can see that <code>log_var</code> is the log of the variance and not the variance itself. This means that you should exponentiate <code>log_var</code> before using it in the <code>reparameterization</code> function.</p> <pre><code>z = self.reparameterization(mean, torch.exp(log_var))\n</code></pre> <p>Alternatively, we can convert to using the standard deviation instead of the variance</p> <pre><code>z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n</code></pre> <p>and</p> <pre><code>epsilon = torch.randn_like(std)\n</code></pre> Solution for training bug <p>Any training loop in PyTorch should have the following structure:</p> <pre><code>for epoch in range(num_epochs):\n    for batch in dataloader:\n        optimizer.zero_grad()\n        loss = model(batch)\n        loss.backward()\n        optimizer.step()\n</code></pre> <p>If you look at the code for the training loop in the <code>vae_mnist_bugs.py</code> script you can see that the optimizer is not zeroed before the backward pass. This means that the gradients will accumulate over the batches and will explode. You can fix this by adding the line</p> <pre><code>optimizer.zero_grad()\n</code></pre> <p>to the beginning of the inner training loop.</p>"},{"location":"s4_debugging_and_logging/logging/","title":"M14 - Logging","text":""},{"location":"s4_debugging_and_logging/logging/#logging","title":"Logging","text":"<p>Core Module</p> <p>Logging in general refers to the practice of recording events over time. Having proper logging in your applications can be extremely beneficial for a few reasons:</p> <ul> <li> <p>Debugging becomes easier because we in a more structured way can output information about the state of our program,     variables, values, etc. to help identify and fix bugs or unexpected behavior.</p> </li> <li> <p>When we move into a more production environment, proper logging is essential for monitoring the health and     performance of our application.</p> </li> <li> <p>It can help in auditing, as logging info about specific activities, etc. can help keeping a record of who did what     and when.</p> </li> <li> <p>Having proper logging means that info is saved for later, which can be analysed to gain insight into the behavior of     our application, such as trends.</p> </li> </ul> <p>We are in this course going to divide the kind of logging we can do into categories: application logging and experiment logging. In general application logging is important regardless of the kind of application you are developing, whereas experiment logging is important in machine learning-based projects where we do experiments.</p>"},{"location":"s4_debugging_and_logging/logging/#application-logging","title":"Application logging","text":"<p>The most basic form of logging in Python applications is the good old <code>print</code> statement:</p> <pre><code>for batch_idx, batch in enumerate(dataloader):\n    print(f\"Processing batch {batch_idx} out of {len(dataloader)}\")\n    ...\n</code></pre> <p>This will keep a \"record\" of the events happening in our script, in this case how far we have progressed. We could even change the print to include something like <code>batch.shape</code> to also have information about the current data being processed. Using <code>print</code> statements is fine for small applications, but to have proper logging we need a bit more functionality than what <code>print</code> can offer. Python actually comes with a great logging module that defines functions for flexible logging. However, it does require a bit more setup than just using <code>print</code>:</p> <pre><code>import logging\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nlogger.info(\"This is an info message\")\n</code></pre> <p>For this reason we are instead going to look at the loguru package which makes Python logging (stupidly) easy. Logging with loguru is essentially as easy as using <code>print</code>:</p> <pre><code>from loguru import logger\nlogger.info(\"This is an info message\")\n</code></pre> <p>The only core different is that we need to understand what log levels are and how to use them. Levels essentially allow us to get rid of statements like this:</p> <pre><code>if debug:\n    print(x.shape)\n</code></pre> <p>where the logging is conditional on the variable <code>debug</code> which we can set at runtime. Thus, it is something we can disable for users of our application (<code>debug=False</code>) but have enabled when we develop the application (<code>debug=True</code>). And it makes sense that not all things logged should be available to all stakeholders of a codebase. We as developers probably always want the highest level of logging, whereas users of our code need less info, and we may want to differentiate this based on users.</p> <p></p> <p>It is also important to understand the difference between logging and error handling. Error handling in Python is done using <code>raise</code> statements and <code>try/catch</code> like:</p> <pre><code>def f(x: int):\n    if not isinstance(x, int):\n        raise ValueError(\"Expected an integer\")\n    return 2 * x\n\ntry:\n    f(5):\nexcept ValueError:\n    print(\"I failed to do a thing, but continuing.\")\n</code></pre> <p>Why would we ever need log <code>warning</code>, <code>error</code>, <code>critical</code> levels of information, if we are just going to handle it? The reason is that raising exceptions are meant to change the program flow at runtime e.g. things we do not want the user to do, but we can deal with in some way. Logging is always for after a program has run, to inspect what went wrong. Sometimes you need one, sometimes the other, sometimes both.</p>"},{"location":"s4_debugging_and_logging/logging/#exercises","title":"\u2754 Exercises","text":"<p>If in doubt, always refer to the documentation for help.</p> <ol> <li> <p>Start by installing <code>loguru</code>:</p> <pre><code>pip install loguru\n</code></pre> <p>and remember to add it to your requirements file.</p> </li> <li> <p>Create a script called <code>my_logger.py</code> and try logging a few messages using <code>loguru</code>. Make sure to log at least one     message of each level: <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code> and <code>critical</code>.</p> Solution my_logger.py<pre><code>from loguru import logger\n\nlogger.debug(\"Used for debugging your code.\")\nlogger.info(\"Informative messages from your code.\")\nlogger.warning(\"Everything works but there is something to be aware of.\")\nlogger.error(\"There's been a mistake with the process.\")\nlogger.critical(\"There is something terribly wrong and process may terminate.\")\n</code></pre> </li> <li> <p>Configure the logging level such that only messages of level <code>warning</code> and higher are shown. Confirm by rerunning     the script.</p> Solution <pre><code>import sys\nfrom loguru import logger\nlogger.remove()  # Remove the default logger\nlogger.add(sys.stdout, level=\"WARNING\")  # Add a new logger with WARNING level\n</code></pre> <p>As an alternative you can set the <code>LOGURU_LEVEL</code> environment variable to <code>WARNING</code> before running the script.</p> </li> <li> <p>Instead of sending logs to the terminal, let's instead log to a file. This can be beneficial, such that only     <code>warning</code> level logs and higher are available to the user, but <code>debug</code> and <code>info</code> are still saved when the     application is running. Change the code such that logs are saved to a file called <code>my_log.log</code>.</p> Solution <pre><code>from loguru import logger\nlogger.add(\"my_log.log\", level=\"DEBUG\")\n</code></pre> <ol> <li> <p>A common problem with logging to a file is that the file can grow very large over time. Luckily, loguru has     a built-in feature for rotating logs. Add this feature to the logger, such that the log file is rotated when     it reaches 100 MB.</p> Solution <pre><code>from loguru import logger\nlogger.add(\"my_log.log\", level=\"DEBUG\", rotation=\"100 MB\")\n</code></pre> </li> </ol> </li> <li> <p>(Optional) Play around with some of the other features that loguru provides such as: <code>logger.catch</code> for catching     errors and sending them to the logger, the <code>format</code> argument for changing the format (and color) of the logs and the     <code>opt</code> method for lazy evaluation of logs.</p> </li> <li> <p>(Optional) We already briefly touched on logging during the     module on config files using hydra. Officially, <code>loguru</code> and <code>hydra</code> do     not really integrate because <code>hydra</code> uses the standard Python logging module. However, you can still use <code>loguru</code>     with <code>hydra</code> by configuring <code>loguru</code> to save a log file to the same location as hydra is saving its logs. Try     implementing this in your hydra project.</p> Solution my_logger_hydra.py<pre><code>import os\n\nimport hydra\nfrom loguru import logger\n\n\n@hydra.main(version_base=\"1.1\", config_path=\".\", config_name=\"config\")\ndef main(cfg):\n    \"\"\"Function that shows how to use loguru with hydra.\"\"\"\n    # Get the path to the hydra output directory\n    hydra_path = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n    # Add a log file to the logger\n    logger.add(os.path.join(hydra_path, \"my_logger_hydra.log\"))\n    logger.info(cfg)\n\n    logger.debug(\"Used for debugging your code.\")\n    logger.info(\"Informative messages from your code.\")\n    logger.warning(\"Everything works but there is something to be aware of.\")\n    logger.error(\"There's been a mistake with the process.\")\n    logger.critical(\"There is something terribly wrong and process may terminate.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </li> </ol>"},{"location":"s4_debugging_and_logging/logging/#experiment-logging","title":"Experiment logging","text":"<p>When most people think machine learning, we think about the training phase. Being able to track and log experiments is an important part of understanding what is going on with your model while you are training. It can help you debug your model and help tweak your models to perfection. Without proper logging of experiments, it can be really hard to iterate on the model because you do not know what changes led to increase or decrease in performance.</p> <p>The most basic logging we can do when running experiments is writing the metrics that our model is producing, e.g. the loss or the accuracy to the terminal or a file for later inspection. We can then also use tools such as matplotlib for plotting the progression of our metrics over time. This kind of workflow may be enough when doing smaller experiments or working alone on a project, but there is no way around using a proper experiment tracker and visualizer when doing large-scale experiments in collaboration with others. It especially becomes important when you want to compare performance between different runs.</p> <p>There exist many tools for logging your experiments, with some of them being:</p> <ul> <li>Tensorboard</li> <li>Comet</li> <li>MLFlow</li> <li>Neptune</li> <li>Weights and Bias</li> </ul> <p>All of the frameworks offer many of the same functionalities, you can see a (biased) review here. We are going to use Weights and Bias (wandb), as it supports everything we need in this course. Additionally, it is an excellent tool for collaboration and sharing of results.</p> <p></p>  Using the Weights and Bias (wandb) dashboard we can quickly get an overview and compare many runs over different metrics. This allows for better iteration of models and training procedure."},{"location":"s4_debugging_and_logging/logging/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by creating an account at wandb. I recommend using your GitHub account but feel     free to choose what you want. When you are logged in you should get an API key of length 40. Copy this for later     use (HINT: if you forgot to copy the API key, you can find it under settings), but make sure that you do not share     it with anyone or leak it in any way.</p> .env file <p>A good place to store not only your wandb API key but also other sensitive information is in a <code>.env</code> file. This file should be added to your <code>.gitignore</code> file to make sure that it is not uploaded to your repository. You can then load the variables in the <code>.env</code> file using the <code>python-dotenv</code> package. For more information see this page.</p> <p>.env<pre><code>WANDB_API_KEY=your-api-key\nWANDB_PROJECT=my_project\nWANDB_ENTITY=my_entity\n...\n</code></pre> load_from_env_file.py<pre><code>from dotenv import load_dotenv\nload_dotenv()\nimport os\napi_key = os.getenv(\"WANDB_API_KEY\")\n</code></pre></p> </li> <li> <p>Next install wandb on your laptop.</p> <pre><code>pip install wandb\n</code></pre> </li> <li> <p>Now connect to your wandb account.</p> <pre><code>wandb login\n</code></pre> <p>You will be asked to provide the length-40 API key. The connection should remain open to the wandb server even when you close the terminal, such that you do not have to log in each time. If using <code>wandb</code> in a notebook you need to manually close the connection using <code>wandb.finish()</code>.</p> </li> <li> <p>We are now ready to incorporate <code>wandb</code> into our code. We are going to continue development on our corrupt MNIST     codebase from the previous sessions. For help, we recommend looking at this     quickstart and this guide     for PyTorch applications. You first job is to alter your training script to include <code>wandb</code> logging, at least for     the training loss.</p> Solution train.py<pre><code>import torch\nimport typer\nimport wandb\nfrom my_project.data import corrupt_mnist\nfrom my_project.model import MyAwesomeModel\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\n\ndef train(lr: float = 0.001, batch_size: int = 32, epochs: int = 5) -&gt; None:\n    \"\"\"Train a model on MNIST.\"\"\"\n    print(\"Training day and night\")\n    print(f\"{lr=}, {batch_size=}, {epochs=}\")\n    wandb.init(\n        project=\"corrupt_mnist\",\n        config={\"lr\": lr, \"batch_size\": batch_size, \"epochs\": epochs},\n    )\n\n    model = MyAwesomeModel().to(DEVICE)\n    train_set, _ = corrupt_mnist()\n\n    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        model.train()\n        for i, (img, target) in enumerate(train_dataloader):\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(img)\n            loss = loss_fn(y_pred, target)\n            loss.backward()\n            optimizer.step()\n            accuracy = (y_pred.argmax(dim=1) == target).float().mean().item()\n            wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": accuracy})\n\n            if i % 100 == 0:\n                print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(train)\n</code></pre> <ol> <li> <p>After running your model, check out the webpage. Hopefully you should be able to see at least one run with     something logged.</p> </li> <li> <p>Now log something other than scalar values. This could be an image, a histogram or a matplotlib figure. In all     cases the logging is still going to use <code>wandb.log</code> but you need extra calls to <code>wandb.Image</code>, etc. depending     on what you choose to log.</p> Solution <p>In this solution we log the input images to the model every 100 steps. Additionally, we also log a histogram of the gradients to inspect if the model is converging. Finally, we create an ROC curve which is a matplotlib figure and log that as well.</p> train.py<pre><code>import matplotlib.pyplot as plt\nimport torch\nimport typer\nimport wandb\nfrom my_project.data import corrupt_mnist\nfrom my_project.model import MyAwesomeModel\nfrom sklearn.metrics import RocCurveDisplay\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\n\ndef train(lr: float = 0.001, batch_size: int = 32, epochs: int = 5) -&gt; None:\n    \"\"\"Train a model on MNIST.\"\"\"\n    print(\"Training day and night\")\n    print(f\"{lr=}, {batch_size=}, {epochs=}\")\n    wandb.init(\n        project=\"corrupt_mnist\",\n        config={\"lr\": lr, \"batch_size\": batch_size, \"epochs\": epochs},\n    )\n\n    model = MyAwesomeModel().to(DEVICE)\n    train_set, _ = corrupt_mnist()\n\n    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        model.train()\n\n        preds, targets = [], []\n        for i, (img, target) in enumerate(train_dataloader):\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(img)\n            loss = loss_fn(y_pred, target)\n            loss.backward()\n            optimizer.step()\n            accuracy = (y_pred.argmax(dim=1) == target).float().mean().item()\n            wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": accuracy})\n\n            preds.append(y_pred.detach().cpu())\n            targets.append(target.detach().cpu())\n\n            if i % 100 == 0:\n                print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n\n                # add a plot of the input images\n                images = wandb.Image(img[:5].detach().cpu(), caption=\"Input images\")\n                wandb.log({\"images\": images})\n\n                # add a plot of histogram of the gradients\n                grads = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None], 0)\n                wandb.log({\"gradients\": wandb.Histogram(grads)})\n\n        # add a custom matplotlib plot of the ROC curves\n        preds = torch.cat(preds, 0)\n        targets = torch.cat(targets, 0)\n\n        for class_id in range(10):\n            one_hot = torch.zeros_like(targets)\n            one_hot[targets == class_id] = 1\n            _ = RocCurveDisplay.from_predictions(\n                one_hot,\n                preds[:, class_id],\n                name=f\"ROC curve for {class_id}\",\n                plot_chance_level=(class_id == 2),\n            )\n\n        # alternatively use wandb.log({\"roc\": wandb.Image(plt)}\n        wandb.plot({\"roc\": plt})\n        plt.close()  # close the plot to avoid memory leaks and overlapping figures\n\n\nif __name__ == \"__main__\":\n    typer.run(train)\n</code></pre> </li> <li> <p>Finally, we want to log the model itself. This is done by saving the model as an artifact and then logging the     artifact. You can read much more about what artifacts are here, but     they are essentially one or more files logged together with runs that can be versioned and equipped with     metadata. Log the model after training and see if you can find it in the wandb dashboard.</p> Solution <p>In this solution we have added the calculating of final training metrics and when we then log the model we add these as metadata to the artifact.</p> train.py<pre><code>import matplotlib.pyplot as plt\nimport torch\nimport typer\nimport wandb\nfrom my_project.data import corrupt_mnist\nfrom my_project.model import MyAwesomeModel\nfrom sklearn.metrics import RocCurveDisplay, accuracy_score, f1_score, precision_score, recall_score\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n\n\ndef train(lr: float = 0.001, batch_size: int = 32, epochs: int = 5) -&gt; None:\n    \"\"\"Train a model on MNIST.\"\"\"\n    print(\"Training day and night\")\n    print(f\"{lr=}, {batch_size=}, {epochs=}\")\n    run = wandb.init(\n        project=\"corrupt_mnist\",\n        config={\"lr\": lr, \"batch_size\": batch_size, \"epochs\": epochs},\n    )\n\n    model = MyAwesomeModel().to(DEVICE)\n    train_set, _ = corrupt_mnist()\n\n    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        model.train()\n\n        preds, targets = [], []\n        for i, (img, target) in enumerate(train_dataloader):\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(img)\n            loss = loss_fn(y_pred, target)\n            loss.backward()\n            optimizer.step()\n            accuracy = (y_pred.argmax(dim=1) == target).float().mean().item()\n            wandb.log({\"train_loss\": loss.item(), \"train_accuracy\": accuracy})\n\n            preds.append(y_pred.detach().cpu())\n            targets.append(target.detach().cpu())\n\n            if i % 100 == 0:\n                print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")\n\n                # add a plot of the input images\n                images = wandb.Image(img[:5].detach().cpu(), caption=\"Input images\")\n                wandb.log({\"images\": images})\n\n                # add a plot of histogram of the gradients\n                grads = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None], 0)\n                wandb.log({\"gradients\": wandb.Histogram(grads)})\n\n        # add a custom matplotlib plot of the ROC curves\n        preds = torch.cat(preds, 0)\n        targets = torch.cat(targets, 0)\n\n        for class_id in range(10):\n            one_hot = torch.zeros_like(targets)\n            one_hot[targets == class_id] = 1\n            _ = RocCurveDisplay.from_predictions(\n                one_hot,\n                preds[:, class_id],\n                name=f\"ROC curve for {class_id}\",\n                plot_chance_level=(class_id == 2),\n            )\n\n        # alternatively use wandb.log({\"roc\": wandb.Image(plt)}\n        wandb.plot({\"roc\": plt})\n        plt.close()  # close the plot to avoid memory leaks and overlapping figures\n\n    final_accuracy = accuracy_score(targets, preds.argmax(dim=1))\n    final_precision = precision_score(targets, preds.argmax(dim=1), average=\"weighted\")\n    final_recall = recall_score(targets, preds.argmax(dim=1), average=\"weighted\")\n    final_f1 = f1_score(targets, preds.argmax(dim=1), average=\"weighted\")\n\n    # first we save the model to a file then log it as an artifact\n    torch.save(model.state_dict(), \"model.pth\")\n    artifact = wandb.Artifact(\n        name=\"corrupt_mnist_model\",\n        type=\"model\",\n        description=\"A model trained to classify corrupt MNIST images\",\n        metadata={\"accuracy\": final_accuracy, \"precision\": final_precision, \"recall\": final_recall, \"f1\": final_f1},\n    )\n    artifact.add_file(\"model.pth\")\n    run.log_artifact(artifact)\n\n\nif __name__ == \"__main__\":\n    typer.run(train)\n</code></pre> <p>After running the script you should be able to see the logged artifact in the wandb dashboard.</p> <p> </p> </li> </ol> </li> <li> <p>Weights and bias was created with collaboration in mind and let's therefore share our results with others.</p> <ol> <li> <p>Let's create a report that you can share. Click the Create report button (upper right corner when you are in     a project workspace) and include some of the graphs/plots/images that you have generated in the report.</p> </li> <li> <p>Make the report shareable by clicking the Share button and create a view-only-link. Send a link to your report     to a group member, fellow student or a friend. In the worst case that you have no one else to share with you can     send a link to my email <code>nsde@dtu.dk</code>, so I can check out your awesome work \ud83d\ude03</p> </li> </ol> </li> <li> <p>When calling <code>wandb.init</code> you can provide many additional arguments. Some of the most important are</p> <ul> <li><code>project</code></li> <li><code>entity</code></li> <li><code>job_type</code></li> </ul> <p>Make sure you understand what these arguments do and try them out. It will come in handy for your group work as they essentially allow multiple users to upload their own runs to the same project in <code>wandb</code>.</p> Solution <p>Relevant documentation can be found here. The <code>project</code> indicates what project all experiments and artifacts are logged to. We want to keep this the same for all group members. The <code>entity</code> is the username of the person or team who owns the project, which should also be the same for all group members. The job type is important if you have different jobs that log to the same project. A common example is one script that trains a model and another that evaluates it. By setting the job type you can easily filter the runs in the wandb dashboard.</p> <p> </p> </li> <li> <p>Wandb also comes with a built-in feature for doing hyperparameter sweeping     which can be beneficial to get a better working model. Look through the documentation on how to do a hyperparameter     sweep in Wandb. You at least need to create a new file called <code>sweep.yaml</code> and make sure that you call <code>wandb.log</code>     in your code on an appropriate value.</p> <ol> <li> <p>Start by creating a <code>sweep.yaml</code> file. Relevant documentation can be found     here. We recommend placing the file in a     <code>configs</code> folder in your project.</p> Solution <p>The <code>sweep.yaml</code> file will depend on the kind of hyperparameters your model accepts as arguments and how they are passed to the model. For this solution we assume that the model accepts the hyperparameters <code>lr</code>, <code>batch_size</code> and <code>epochs</code> and that they are passed as <code>--args</code> (with hyphens) (1) e.g. this would be how we run the script:</p> <ol> <li> If the script you want to run hyperparameter sweeping is configured using     hydra then you will need to change the default <code>command</code> config     in your <code>sweep.yaml</code> file. This is because <code>wandb</code> uses <code>--args</code> to pass hyperparameters to the script,     whereas <code>hydra</code> uses <code>args</code> (without the hyphen). See this     page for more information.</li> </ol> <pre><code>python train.py --lr=0.01 --batch_size=32 --epochs=10\n</code></pre> <p>The <code>sweep.yaml</code> could then look like this:</p> <pre><code>program: train.py\nname: sweepdemo\nproject: my_project  # change this\nentity: my_entity  # change this\nmetric:\n    goal: minimize\n    name: validation_loss\nparameters:\n    learning_rate:\n        min: 0.0001\n        max: 0.1\n        distribution: log_uniform\n    batch_size:\n        values: [16, 32, 64]\n    epochs:\n        values: [5, 10, 15]\nrun_cap: 10\n</code></pre> </li> <li> <p>Afterwards, you need to create a sweep using the <code>wandb sweep</code> command:</p> <pre><code>wandb sweep configs/sweep.yaml\n</code></pre> <p>This will output a sweep id that you need to use in the next step.</p> </li> <li> <p>Finally, you need to run the sweep using the <code>wandb agent</code> command:</p> <pre><code>wandb agent &lt;sweep_id&gt;\n</code></pre> <p>where <code>&lt;sweep_id&gt;</code> is the id of the sweep you just created. You can find the id in the output of the <code>wandb sweep</code> command. The reason that we first lunch the sweep and then the agent is that we can have multiple agents running at the same time, parallelizing the search for the best hyperparameters. Try this out by opening a new terminal and running the <code>wandb agent</code> command again (with the same <code>&lt;sweep_id&gt;</code>).</p> </li> <li> <p>Inspect the sweep results in the wandb dashboard. You should see multiple new runs under the project you are     logging the sweep to, corresponding to the different hyperparameters you tried. Make sure you understand the     results and can answer what hyperparameters gave the best results and what hyperparameters had the largest     impact on the results.</p> Solution <p>In the sweep dashboard you should see something like this:</p> <p> </p> <p>Importantly you can:</p> <ol> <li>Sort the runs based on what metric you are interested in, thereby quickly finding the best runs.</li> <li>Look at the parallel coordinates plot to see if there are any tendencies in the hyperparameters that     give the best results.</li> <li>Look at the importance/correlation plot to see what hyperparameters have the largest impact on the     results.</li> </ol> </li> </ol> </li> <li> <p>Next we need to understand the model registry, which will be very important later on when we get to the deployment     of our models. The model registry is a centralized place for storing and versioning models. Importantly, any model     in the registry is immutable, meaning that once a model is uploaded it cannot be changed. This is important for     reproducibility and traceability of models.</p> <p>  The model registry is in general a repository of a team's trained models where ML practitioners publish candidates for production and share them with others. Figure from wandb.  <ol> <li> <p>The model registry builds on the artifact registry in wandb. Any model that is uploaded to the model registry is     stored as an artifact. This means that we first need to log our trained models as artifacts before we can     register them in the model registry. Make sure you have logged at least one model as an artifact before     continuing.</p> </li> <li> <p>Next let's create a registry. Go to the model registry tab (left pane, visible from your homepage) and then click     the <code>New Registered Model</code> button. Fill out the form and create the registry.</p> <p> </p> </li> <li> <p>When then need to link our artifact to the model registry we just created. We can do this in two ways: either     through the web interface or through the <code>wandb</code> API. In the web interface, go to the artifact you want to link     to the model registry and click the <code>Link to registry</code> button (upper right corner). If you want to use the     API you need to call the link method on an artifact object.</p> Solution <p>To use the API, create a new script called <code>link_to_registry.py</code> and add the following code:</p> link_to_registry.py<pre><code>import wandb\napi = wandb.Api()\nartifact_path = \"&lt;entity&gt;/&lt;project&gt;/&lt;artifact_name&gt;:&lt;version&gt;\"\nartifact = api.artifact(artifact_path)\nartifact.link(target_path=\"&lt;entity&gt;/model-registry/&lt;my_registry_name&gt;\")\nartifact.save()\n</code></pre> <p>In the code <code>&lt;entity&gt;</code>, <code>&lt;project&gt;</code>, <code>&lt;artifact_name&gt;</code>, <code>&lt;version&gt;</code> and <code>&lt;my_registry_name&gt;</code> should be replaced with the appropriate values.</p> </li> <li> <p>We are now ready to consume our model, which can be done by downloading the artifact from the model registry. In     this case we use the wandb API to download the artifact.</p> <pre><code>import wandb\nrun = wandb.init()\nartifact = run.use_artifact('&lt;entity&gt;/model-registry/&lt;my_registry_name&gt;:&lt;version&gt;', type='model')\nartifact_dir = artifact.download(\"&lt;artifact_dir&gt;\")\nmodel = MyModel()\nmodel.load_state_dict(torch.load(\"&lt;artifact_dir&gt;/model.ckpt\"))\n</code></pre> <p>Try running this code with the appropriate values for <code>&lt;entity&gt;</code>, <code>&lt;my_registry_name&gt;</code>, <code>&lt;version&gt;</code> and <code>&lt;artifact_dir&gt;</code>. Make sure that you can load the model and that it is the same as the one you trained.</p> </li> <li> <p>Each model in the registry has at least one alias, which is the version of the model. The most recently added     model also receives the alias <code>latest</code>. Aliases are great for indicating where in the workflow a model is, e.g. if     it is a candidate for production or if it is a model that is still being developed. Try adding an alias to one     of your models in the registry.</p> </li> <li> <p>(Optional) A model always corresponds to an artifact, and artifacts can contain metadata that we can use to     automate the process of registering models. We could for example imagine that we at the end of each week run     a script that registers the best model from the week. Try creating a small script using the <code>wandb</code> API that     goes over a collection of artifacts and registers the best one.</p> Solution auto_register_best_model.py<pre><code>import logging\nimport operator\nimport os\n\nimport typer\nimport wandb\nfrom dotenv import load_dotenv\n\nlogger = logging.getLogger(__name__)\nload_dotenv()\n\n\ndef stage_best_model_to_registry(model_name: str, metric_name: str = \"accuracy\", higher_is_better: bool = True) -&gt; None:\n    \"\"\"\n    Stage the best model to the model registry.\n\n    Args:\n        model_name: Name of the model to be registered.\n        metric_name: Name of the metric to choose the best model from.\n        higher_is_better: Whether higher metric values are better.\n\n    \"\"\"\n    api = wandb.Api(\n        api_key=os.getenv(\"WANDB_API_KEY\"),\n        overrides={\"entity\": os.getenv(\"WANDB_ENTITY\"), \"project\": os.getenv(\"WANDB_PROJECT\")},\n    )\n    artifact_collection = api.artifact_collection(type_name=\"model\", name=model_name)\n\n    best_metric = float(\"-inf\") if higher_is_better else float(\"inf\")\n    compare_op = operator.gt if higher_is_better else operator.lt\n    best_artifact = None\n    for artifact in list(artifact_collection.artifacts()):\n        if metric_name in artifact.metadata and compare_op(artifact.metadata[metric_name], best_metric):\n            best_metric = artifact.metadata[metric_name]\n            best_artifact = artifact\n\n    if best_artifact is None:\n        logger.error(\"No model found in registry.\")\n        return\n\n    logger.info(f\"Best model found in registry: {best_artifact.name} with {metric_name}={best_metric}\")\n    best_artifact.link(\n        target_path=f\"{os.getenv('WANDB_ENTITY')}/model-registry/{model_name}\",\n        aliases=[\"best\", \"staging\"],\n    )\n    best_artifact.save()\n    logger.info(\"Model staged to registry.\")\n\n\nif __name__ == \"__main__\":\n    typer.run(stage_best_model_to_registry)\n</code></pre> </li> </ol> <li> <p>In the future it will be important for us to be able to run Wandb inside a docker container (together with whatever     training or inference we specify). The problem here is that we cannot authenticate Wandb in the same way as the     previous exercise; it needs to happen automatically. Let's therefore look into how we can do that.</p> <ol> <li> <p>First we need to generate an authentication key, or more precisely an API key. This is in general the way any     service (like a docker container) can authenticate. Start by going to https://wandb.ai/home, click your profile     icon in the upper right corner and then go to <code>User settings</code>. Scroll down to the danger zone and generate a     new API key (if you do not already have one) and finally copy it.</p> </li> <li> <p>Next create a new dockerfile called <code>wandb.docker</code> and add the following code</p> <pre><code>FROM python:3.11-slim\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\nRUN pip install wandb\nCOPY wandb_tester.py wandb_tester.py\nENTRYPOINT [\"python\", \"-u\", \"wandb_tester.py\"]\n</code></pre> <p>and a new script called <code>wandb_tester.py</code> that contains the following code</p> <pre><code>import random\n\nimport wandb\n\nwandb.init(project=\"wandb-test\")\nfor _ in range(100):\n    wandb.log({\"test_metric\": random.random()})\n</code></pre> <p>and then build the docker image. These two files are just a very minimal setup to test that we can authenticate a docker container with Wandb.</p> </li> <li> <p>When we want to run the image, what we need to do is include an environment variable that contains the API key     we generated. This will then authenticate the docker container with the wandb server:</p> <pre><code>docker run -e WANDB_API_KEY=&lt;your-api-key&gt; wandb:latest\n</code></pre> <p>Try running it and confirm that the results are uploaded to the wandb server (1).</p> <ol> <li> If you have stored the API key in a <code>.env</code> file you can use the <code>--env-file</code> flag instead     of <code>-e</code> to load the environment variables from the file e.g. <code>docker run --env-file .env wandb:latest</code>.</li> </ol> </li> </ol> </li> <li> <p>Feel free to experiment more with <code>wandb</code> as it is a great tool for logging, organizing and sharing experiments.</p> </li> <p>That is the module on logging. Please note that at this point in the course you will begin to see some overlap between the different frameworks. While we mainly used <code>hydra</code> for configuring our Python scripts it can also be used to save metrics and hyperparameters similar to how <code>wandb</code> can. Similar arguments hold for <code>dvc</code> which can also be used to log metrics. In our opinion <code>wandb</code> just offers a better experience when interacting with the results after logging. We want to stress that the combination of tools presented in this course may not be the best for all your future projects, and we recommend finding a setup that works for you. That said, each framework provides specific features that the others do not.</p> <p>Finally, we want to note that while during the course we really try to showcase a lot of open-source frameworks, Wandb is not one. It is free to use for personal usage (with a few restrictions) but for enterprises it does require a license. If you are eager to only work with open-source tools we highly recommend trying out MLFlow, which offers the same overall functionalities as Wandb.</p>"},{"location":"s4_debugging_and_logging/profiling/","title":"M13 - Profiling","text":""},{"location":"s4_debugging_and_logging/profiling/#profilers","title":"Profilers","text":"<p>Core Module</p>"},{"location":"s4_debugging_and_logging/profiling/#profilers_1","title":"Profilers","text":"<p>In general profiling code is about improving the performance of your code. In this session we are going to take a somewhat narrow approach to what \"performance\" is: runtime, meaning the time it takes to execute your program.</p> <p>At the bare minimum, the two questions a proper profiling of your program should be able to answer is:</p> <ul> <li>\u201c How many times is each method in my code called?\u201d</li> <li>\u201c How long do each of these methods take?\u201d</li> </ul> <p>The first question can help us prioritize what to optimize. If two methods <code>A</code> and <code>B</code> have approximately the same runtime, but <code>A</code> is called 1000 more times than <code>B</code> we should probably spend time optimizing <code>A</code> over <code>B</code> if we want to speed up our code. The second question directly tells us which methods are expensive to call.</p> <p>Using profilers can help you find bottlenecks in your code. In this exercise we will look at two different profilers, with the first one being the cProfile. <code>cProfile</code> is python's built-in profiler that can help give you an overview runtime of all the functions and methods involved in your programs.</p>"},{"location":"s4_debugging_and_logging/profiling/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Run <code>cProfile</code> on the <code>vae_mnist_working.py</code> script. Hint: you can directly call the profiler on a     script using the <code>-m</code> arg:</p> <pre><code>python -m cProfile -s &lt;sort_order&gt; myscript.py\n</code></pre> <p>To write the output to a file you can use the <code>-o</code> argument:</p> <pre><code>python -m cProfile -s &lt;sort_order&gt; -o profile.txt myscript.py\n</code></pre> Script to debug vae_mnist_working.py<pre><code>\"\"\"Adapted from https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/01_Variational_AutoEncoder.ipynb.\n\nA simple implementation of Gaussian MLP Encoder and Decoder trained on MNIST\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import save_image\n\n# Model Hyperparameters\ndataset_path = \"datasets\"\ndevice_name = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nDEVICE = torch.device(device_name)\nbatch_size = 100\nx_dim = 784\nhidden_dim = 400\nlatent_dim = 20\nlr = 1e-3\nepochs = 5\n\n\n# Data loading\nmnist_transform = transforms.Compose([transforms.ToTensor()])\n\ntrain_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\ntest_dataset = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n\nclass Encoder(nn.Module):\n    \"\"\"Gaussian MLP Encoder.\"\"\"\n\n    def __init__(self, input_dim, hidden_dim, latent_dim) -&gt; None:\n        super().__init__()\n\n        self.FC_input = nn.Linear(input_dim, hidden_dim)\n        self.FC_mean = nn.Linear(hidden_dim, latent_dim)\n        self.FC_var = nn.Linear(hidden_dim, latent_dim)\n        self.training = True\n\n    def forward(self, x):\n        \"\"\"Forward pass.\"\"\"\n        h_ = torch.relu(self.FC_input(x))\n        mean = self.FC_mean(h_)\n        log_var = self.FC_var(h_)\n\n        std = torch.exp(0.5 * log_var)\n        z = self.reparameterization(mean, std)\n\n        return z, mean, log_var\n\n    def reparameterization(self, mean, std):\n        \"\"\"Reparameterization trick.\"\"\"\n        epsilon = torch.randn_like(std)\n        return mean + std * epsilon\n\n\nclass Decoder(nn.Module):\n    \"\"\"Bernoulli MLP Decoder.\"\"\"\n\n    def __init__(self, latent_dim, hidden_dim, output_dim) -&gt; None:\n        super().__init__()\n        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n        self.FC_output = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        \"\"\"Forward pass.\"\"\"\n        h = torch.relu(self.FC_hidden(x))\n        return torch.sigmoid(self.FC_output(h))\n\n\nclass Model(nn.Module):\n    \"\"\"VAE Model.\"\"\"\n\n    def __init__(self, encoder, decoder) -&gt; None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, x):\n        \"\"\"Forward pass.\"\"\"\n        z, mean, log_var = self.encoder(x)\n        x_hat = self.decoder(z)\n\n        return x_hat, mean, log_var\n\n\nencoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\ndecoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n\nmodel = Model(encoder=encoder, decoder=decoder).to(DEVICE)\n\n\nBCE_loss = nn.BCELoss()\n\n\ndef loss_function(x, x_hat, mean, log_var):\n    \"\"\"Reconstruction + KL divergence losses summed over all elements and batch.\"\"\"\n    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n    kld = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n    return reproduction_loss + kld\n\n\noptimizer = Adam(model.parameters(), lr=lr)\n\n\nprint(\"Start training VAE...\")\nmodel.train()\nfor epoch in range(epochs):\n    overall_loss = 0\n    for batch_idx, (x, _) in enumerate(train_loader):\n        if batch_idx % 100 == 0:\n            print(batch_idx)\n        x = x.view(batch_size, x_dim)\n        x = x.to(DEVICE)\n\n        optimizer.zero_grad()\n\n        x_hat, mean, log_var = model(x)\n        loss = loss_function(x, x_hat, mean, log_var)\n\n        overall_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n    print(\n        \"\\tEpoch\",\n        epoch + 1,\n        \"complete!\",\n        \"\\tAverage Loss: \",\n        overall_loss / (batch_idx * batch_size),\n    )\nprint(\"Finish!!\")\n\n# Generate reconstructions\nmodel.eval()\nwith torch.no_grad():\n    for batch_idx, (x, _) in enumerate(test_loader):\n        if batch_idx % 100 == 0:\n            print(batch_idx)\n        x = x.view(batch_size, x_dim)\n        x = x.to(DEVICE)\n        x_hat, _, _ = model(x)\n        break\n\nsave_image(x.view(batch_size, 1, 28, 28), \"orig_data.png\")\nsave_image(x_hat.view(batch_size, 1, 28, 28), \"reconstructions.png\")\n\n# Generate samples\nwith torch.no_grad():\n    noise = torch.randn(batch_size, latent_dim).to(DEVICE)\n    generated_images = decoder(noise)\n\nsave_image(generated_images.view(batch_size, 1, 28, 28), \"generated_sample.png\")\n</code></pre> </li> <li> <p>Try looking at the output of the profiling. Can you figure out which function took the longest to run? How do you     show the content of the <code>profile.txt</code> file?</p> Solution <p>If you try to open <code>profile.txt</code> in a text editor you will see that it is not very human-readable. To get a better overview of the profiling you can use the <code>pstats</code> module to read the file and print the results in a more readable format. For example, to print the 10 functions that took the longest time to run you can use the following code:</p> <pre><code>import pstats\np = pstats.Stats('profile.txt')\np.sort_stats('cumulative').print_stats(10)\n</code></pre> </li> <li> <p>Can you explain the difference between <code>tottime</code> and <code>cumtime</code>? Under what circumstances do these differ and     when are they equal?</p> Solution <p><code>tottime</code> is the total time spent in the function excluding time spent in subfunctions. <code>cumtime</code> is the total time spent in the function including time spent in subfunctions. Therefore, <code>cumtime</code> is always greater than <code>tottime</code>.</p> </li> <li> <p>To get a better feeling of the profiled result we can try to visualize it. Python does not     provide a native solution, but open-source solutions such as snakeviz     exist. Try installing <code>snakeviz</code> and load a profiled run into it (HINT: snakeviz expects the run to have the file     format <code>.prof</code>).</p> </li> <li> <p>Try optimizing the run! (Hint: The data is not stored as a torch tensor). After optimizing the code make sure     (using <code>cProfile</code> and <code>snakeviz</code>) that the code actually runs faster.</p> Solution <p>For consistency reasons, even though the data in the <code>MNIST</code> dataset class from <code>torchvision</code> is stored as tensors, they are converted to PIL images before being returned. This is the reason the solution is to initialize the dataclass with the transform</p> <pre><code>mnist_transform = transforms.Compose([transforms.ToTensor()])\n</code></pre> <p>such that the data is returned as tensors. However, since the data is already stored as tensors, calling this transform every time you want to access the data is redundant and can be removed. The easiest way to do this is to create a <code>TensorDataset</code> from the internal data and labels (which are already tensors).</p> <pre><code>from torchvision.datasets import MNIST\nfrom torch.utils.data import TensorDataset\n# the class also internally normalize to [0,1] domain so we need to divide by 255\ntrain_dataset = MNIST(dataset_path, train=True, download=True)\ntrain_dataset = TensorDataset(train_dataset.data.float() / 255.0, train_dataset.targets)\ntest_dataset = MNIST(dataset_path, train=False, download=True)\ntest_dataset = TensorDataset(test_dataset.data.float() / 255.0, test_dataset.targets)\n</code></pre> </li> </ol>"},{"location":"s4_debugging_and_logging/profiling/#pytorch-profiling","title":"PyTorch profiling","text":"<p>Profiling machine learning code can become much more complex because we are suddenly beginning to mix different devices (CPU+GPU), which can (and should) overlap in some of their computations. When profiling this kind of machine learning code we are often looking for bottlenecks. A bottleneck is simply a place in your code that is preventing other processes from performing their best. This is the reason that all major deep learning frameworks also include their own profilers that can help profiling more complex applications.</p> <p>The image below show a typical report using the built-in profiler in pytorch. As the image shows, the profiler looks both at the <code>kernel</code> time (this is the time spent doing actual computations) and also transfer times such as <code>memcpy</code> (where we are copying data between devices). It can even analyze your code and give recommendations.</p> <p></p> <p>Using the profiler can be as simple as wrapping the code that you want to profile with the <code>torch.profiler.profile</code> decorator:</p> <pre><code>with torch.profiler.profile(...) as prof:\n    # code that I want to profile\n    output = model(data)\n</code></pre>"},{"location":"s4_debugging_and_logging/profiling/#exercises_1","title":"\u2754 Exercises","text":"<p>Exercise files</p> <p>In these investigate the profiler that is build into PyTorch already. Note that these exercises require that you have PyTorch v1.8.1 installed (or higher). You can always check which version you currently have installed by writing (in a python interpreter):</p> <pre><code>import torch\nprint(torch.__version__)\n</code></pre> <p>But we always recommend updating to the latest PyTorch version for the best experience. Additionally, to display the result nicely (like <code>snakeviz</code> for <code>cProfile</code>) we are also going to use the tensorboard profiler extension.</p> <pre><code>pip install torch_tb_profiler\n</code></pre> <ol> <li> <p>A good starting point is to look at the API for the profiler. Here     the important class to look at is the <code>torch.profiler.profile</code> class.</p> </li> <li> <p>Let's try out a simple example (taken from     here):</p> <ol> <li> <p>Try to run the following code:</p> <pre><code>import torch\nimport torchvision.models as models\nfrom torch.profiler import profile, ProfilerActivity\n\nmodel = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nwith profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n    model(inputs)\n</code></pre> <p>This will profile the <code>forward</code> pass of a Resnet 18 model.</p> </li> <li> <p>Running this code will produce a <code>prof</code> object that contains all the relevant information about the profiling.     Try writing the following code:</p> <pre><code>print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n</code></pre> <p>What operation is using most of the cpu?</p> </li> <li> <p>Try running</p> <pre><code>print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=30))\n</code></pre> <p>Can you see any correlation between the shape of the input and the cost of the operation?</p> </li> <li> <p>(Optional) If you have a GPU you can also profile the operations on that device:</p> <pre><code>with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n    model(inputs)\n</code></pre> </li> <li> <p>(Optional) As an alternative to using <code>profile</code> as a     context-manager, we can also use its <code>.start</code> and     <code>.stop</code> methods:</p> <pre><code>prof = profile(...)\nprof.start()\n...  # code I want to profile\nprof.stop()\n</code></pre> <p>Try doing this on the above example.</p> </li> </ol> </li> <li> <p>The <code>torch.profiler.profile</code> function takes some additional arguments. What argument would you need to     set to also profile the memory usage? (Hint: this page)     Try doing it on the simple example above and make sure to sort the samples by <code>self_cpu_memory_usage</code>.</p> </li> <li> <p>As mentioned we can also get a graphical output for better inspection. After having done profiling     try to export the results with:</p> <pre><code>prof.export_chrome_trace(\"trace.json\")\n</code></pre> <p>You should be able to visualize the file by going to <code>chrome://tracing</code> in any chromium-based web browser. Can you still identify the information printed in the previous exercises from the visualizations?</p> </li> <li> <p>Running profiling on a single forward step can produce misleading results, as it only provides a single sample that     may depend on what background processes are running on your computer. Therefore it is recommended to profile     multiple iterations of your model. If this is the case then we need to include <code>prof.step()</code> to tell the profiler     when we are doing a new iteration.</p> <pre><code>with profile(...) as prof:\n    for i in range(10):\n        model(inputs)\n        prof.step()\n</code></pre> <p>Try doing this. Is the conclusion the same on what operations are taking up most of the time? Have the percentages changed significantly?</p> </li> <li> <p>Additionally, we can also visualize the profiling results using the profiling viewer in tensorboard.</p> <ol> <li> <p>Start by initializing the <code>profile</code> class with an additional argument:</p> <pre><code>from torch.profiler import profile, tensorboard_trace_handler\nwith profile(..., on_trace_ready=tensorboard_trace_handler(\"./log/resnet18\")) as prof:\n    ...\n</code></pre> <p>Try running profiling (using a couple of iterations) and make sure that a file with the <code>.pt.trace.json</code> is produced in the <code>log/resnet18</code> folder.</p> </li> <li> <p>Now try launching tensorboard</p> <pre><code>tensorboard --logdir=./log\n</code></pre> <p>and open the page http://localhost:6006/#pytorch_profiler, where you should hopefully see an image similar to the one below:</p> <p>  Image credit  </p> <p>Try poking around in the interface.</p> </li> <li> <p>Tensorboard has a nice feature for comparing runs under the <code>diff</code> tab. Try redoing a profiling run but use     <code>model = models.resnet34()</code> instead. Load up both runs and try to look at the <code>diff</code> between them.</p> </li> </ol> </li> <li> <p>As a final exercise, try to use the profiler on the <code>vae_mnist_working.py</code> file from the previous module on     debugging, where you profile a whole training run (not only the forward pass). What is the bottleneck during the     training? Is it still the forward pass or is it something else? Can you improve the code somehow based on the     information from the profiler?</p> </li> </ol> <p>This ends the module on profiling. If you want to go into more details on this topic we recommend looking into line_profiler and kernprof. A downside of using python's <code>cProfile</code> is that it can only profile at a functional/modular level, which is great for identifying hotspots in your code. However, sometimes the cause of a computational hotspot is a single line of code in a function, which will not be caught by <code>cProfile</code>. An example would be a simple index operation such as <code>a[idx] = b</code>, which for large arrays and non-sequential indexes is really expensive. For these cases line_profiler and kernprof are excellent tools to have in your toolbox. Additionally, if you do not like cProfile we also recommend py-spy, which is another open-source profiling tool for Python programs.</p>"},{"location":"s5_continuous_integration/","title":"Continuous Integration","text":"<p>Slides</p> <ul> <li> <p>     Learn how to write unit tests that cover both data and models in your ML pipeline.</p> <p> M16: Unit testing</p> </li> <li> <p>     Learn how to implement continuous integration using GitHub actions such that tests are automatically executed upon     code changes.</p> <p> M17: GitHub Actions</p> </li> <li> <p>     Learn how to use pre-commit to ensure that code that is not up to standard does not get committed.</p> <p> M18: Pre-commit</p> </li> <li> <p></p> <p>Learn how to implement continuous machine learning pipelines in GitHub actions.</p> <p> M19: Continuous Machine Learning</p> </li> </ul> <p>Continuous integration is a sub-discipline of the general field of Continuous X. Continuous X is one of the core elements of modern DevOps, and by extension MLOps. Continuous X assumes that we have a (long) developer pipeline (see image below) where we want to make some changes to our code, e.g:</p> <ul> <li>Update our training data or data processing</li> <li>Update our model architecture</li> <li>Something else...</li> </ul> <p>Basically, any code change we expect will have an influence on the final result. The problem with making changes to the start of our pipeline is that we want the change to propagate all the way through to the end of the pipeline.</p> <p></p>  Image credit  <p>This is where continuous X comes into play. The word continuous here refers to the fact that the pipeline should continuously be updated as we make code changes. You can also choose to think of this as the automatization of processes. The X then covers that the process we need to go through to automate steps in the pipeline depends on where we are in the pipeline, e.g. the tools needed to do continuous integration are different from the tools needed to do continuous delivery.</p> <p>In this session, we are going to focus on continuous integration (CI). As indicated in the image above, continuous integration usually takes care of the first part of the developer pipeline which has to do with the code base, code building and code testing. This is paramount to automatization as we would rather catch bugs at the beginning of our pipeline than at the end.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Be able to write unit tests that cover both data and models in your ML pipeline</li> <li>Know how to implement continuous integration using GitHub actions such that tests are automatically executed on     code changes</li> <li>Be able to use pre-commit to ensure that code that is not up to standard does not get committed</li> <li>Know how to implement continuous integration for continuous building of containers</li> <li>Basic knowledge of how machine learning processes can be implemented in a continuous way</li> </ul>"},{"location":"s5_continuous_integration/cml/","title":"M19 - Continuous Machine Learning","text":""},{"location":"s5_continuous_integration/cml/#continuous-machine-learning","title":"Continuous Machine Learning","text":"<p>The continuous integration we have looked at until now is what we can consider \"classical\" continuous integration, which has its roots in DevOps and not MLOps. While the tests that we have written and the containers we have developed in the previous session have been about machine learning, everything we have done translates completely to how it would be done if we had developed any other application that did not include machine learning.</p> <p>In this session, we are now going to change gears and look at continuous machine learning (CML). As the name may suggest we are now focusing on automating actual machine learning processes. The reason for doing this is the same as with continuous integration, namely that we often have a bunch of checks that we want our newly trained model to pass before we trust it to be ready for deployment. Writing unit tests ensures that the code that we use for training our model is not broken, but there exist other failure modes of a machine learning pipeline:</p> <ul> <li>Did I train on the correct data?</li> <li>Did my model converge at all?</li> <li>Did a metric that I care about improve?</li> <li>Did I overfit?</li> <li>Did I underfit?</li> <li>...</li> </ul> <p>All these questions are questions that we can answer by writing tests that are specific to machine learning. In this session, we are going to look at how we can begin to use GitHub Actions to automate these tests.</p>"},{"location":"s5_continuous_integration/cml/#mlops-maturity-model","title":"MLOps maturity model","text":"<p>Before getting started with the exercises, let's first take a side step and look at what is called the MLOps maturity model. The reason here is to get a better understanding of when continuous machine learning is relevant. The main idea behind the MLOps maturity model is to help organizations understand where they are in their machine learning operations journey and what the next logical steps are. The model is divided into five stages:</p> <p></p>  Image credit  <code>Level 0</code> <p>At this level, organizations are doing machine learning in an ad-hoc manner. There is no standardization, no version control, no testing, and no monitoring.</p> <code>Level 1</code> <p>At this level, organizations have started to implement DevOps practices in their machine learning workflows. They have started to use version control and maybe basic continuous integration practices.</p> <code>Level 2</code> <p>At this level, organizations have started to standardize the training process and tackle the problem of creating reproducible experiments. Centralization of model artifacts and metadata is common at this level. They have started to implement model versioning and model registry practices.</p> <code>Level 3</code> <p>At this level, organizations have started to implement continuous integration and continuous deployment practices. They have started to automate the testing of their models and have started to monitor their models in production.</p> <code>Level 4</code> <p>At this level, organizations have started to implement continuous machine learning practices. They have started to automate the training, evaluation, and deployment of their models. They have started to implement automated retraining and model updates.</p> <p>The MLOps maturity model tells us that continuous machine learning is the highest form of maturity in MLOps. It is the stage where we have automated the entire machine learning pipeline and the cases we will be going through in the exercises are therefore some of the last steps in the MLOps maturity model.</p>"},{"location":"s5_continuous_integration/cml/#exercises","title":"\u2754 Exercises","text":"<p>In the following exercises, we are going to look at two different cases where we can use continuous machine learning. The first one is a simple case where we are automatically going to trigger some workflow (like model training) whenever we make changes to our data. This is a very common use case in machine learning where we have a data pipeline that is continuously updating our data. The second case is connected to staging and deploying models. In this case, we are going to look at how we can automatically do further processing of our model whenever we push a new model to our repository.</p> <ol> <li> <p>For the first set of exercises, we are going to rely on the <code>cml</code> framework by iterative.ai,     which is a framework that is built on top of GitHub actions. The figure below describes the overall process using     the <code>cml</code> framework. It should be clear that it is the very same process that we go through in the other     continuous integration sessions: <code>push code</code> -&gt; <code>trigger GitHub actions</code> -&gt; <code>do stuff</code>. The new part in this     session is that we are only going to trigger whenever data changes.</p> <p>  Image credit  </p> <ol> <li> <p>If you have not already created a dataset class for the corrupted MNIST data, start by doing that. Essentially,     it is a class that should inherit from <code>torch.utils.data.Dataset</code> and should have <code>__getitem__</code> and <code>__len__</code>.</p> Solution dataset.py<pre><code>from __future__ import annotations\n\nimport os\nfrom typing import TYPE_CHECKING\n\nimport torch\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\n\nif TYPE_CHECKING:\n    import torchvision.transforms.v2 as transforms\n\n\nclass MnistDataset(Dataset):\n    \"\"\"MNIST dataset for PyTorch.\n\n    Args:\n        data_folder: Path to the data folder.\n        train: Whether to load training or test data.\n        img_transform: Image transformation to apply.\n        target_transform: Target transformation to apply.\n    \"\"\"\n\n    name: str = \"MNIST\"\n\n    def __init__(\n        self,\n        data_folder: str = \"data\",\n        train: bool = True,\n        img_transform: transforms.Transform | None = None,\n        target_transform: transforms.Transform | None = None,\n    ) -&gt; None:\n        super().__init__()\n        self.data_folder = data_folder\n        self.train = train\n        self.img_transform = img_transform\n        self.target_transform = target_transform\n        self.load_data()\n\n    def load_data(self) -&gt; None:\n        \"\"\"Load images and targets from disk.\"\"\"\n        images, target = [], []\n        if self.train:\n            nb_files = len([f for f in os.listdir(self.data_folder) if f.startswith(\"train_images\")])\n            for i in range(nb_files):\n                images.append(torch.load(f\"{self.data_folder}/train_images_{i}.pt\"))\n                target.append(torch.load(f\"{self.data_folder}/train_target_{i}.pt\"))\n        else:\n            images.append(torch.load(f\"{self.data_folder}/test_images.pt\"))\n            target.append(torch.load(f\"{self.data_folder}/test_target.pt\"))\n        self.images = torch.cat(images, 0)\n        self.target = torch.cat(target, 0)\n\n    def __getitem__(self, idx: int) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"Return image and target tensor.\"\"\"\n        img, target = self.images[idx], self.target[idx]\n        if self.img_transform:\n            img = self.img_transform(img)\n        if self.target_transform:\n            target = self.target_transform(target)\n        return img, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of images in the dataset.\"\"\"\n        return self.images.shape[0]\n</code></pre> </li> <li> <p>Then let's create a function that can report basic statistics such as the number of training samples, number     of test samples and generate figures of sample images in the dataset and distribution of classes in the     dataset. This function should be called <code>dataset_statistics</code> and should take a path to the dataset as input.</p> Solution dataset.py<pre><code>import matplotlib.pyplot as plt\nimport torch\nimport typer\nfrom mnist_dataset import MnistDataset\nfrom utils import show_image_and_target\n\n\ndef dataset_statistics(datadir: str = \"data\") -&gt; None:\n    \"\"\"Compute dataset statistics.\"\"\"\n    train_dataset = MnistDataset(data_folder=datadir, train=True)\n    test_dataset = MnistDataset(data_folder=datadir, train=False)\n    print(f\"Train dataset: {train_dataset.name}\")\n    print(f\"Number of images: {len(train_dataset)}\")\n    print(f\"Image shape: {train_dataset[0][0].shape}\")\n    print(\"\\n\")\n    print(f\"Test dataset: {test_dataset.name}\")\n    print(f\"Number of images: {len(test_dataset)}\")\n    print(f\"Image shape: {test_dataset[0][0].shape}\")\n\n    show_image_and_target(train_dataset.images[:25], train_dataset.target[:25], show=False)\n    plt.savefig(\"mnist_images.png\")\n    plt.close()\n\n    train_label_distribution = torch.bincount(train_dataset.target)\n    test_label_distribution = torch.bincount(test_dataset.target)\n\n    plt.bar(torch.arange(10), train_label_distribution)\n    plt.title(\"Train label distribution\")\n    plt.xlabel(\"Label\")\n    plt.ylabel(\"Count\")\n    plt.savefig(\"train_label_distribution.png\")\n    plt.close()\n\n    plt.bar(torch.arange(10), test_label_distribution)\n    plt.title(\"Test label distribution\")\n    plt.xlabel(\"Label\")\n    plt.ylabel(\"Count\")\n    plt.savefig(\"test_label_distribution.png\")\n    plt.close()\n\n\nif __name__ == \"__main__\":\n    typer.run(dataset_statistics)\n</code></pre> </li> <li> <p>Next, we are going to implement a GitHub actions workflow that only activates when we make changes to our data.     Create a new workflow file (call it <code>cml_data.yaml</code>) and make sure it only activates on push/pull-request events     when <code>data/</code> changes. Relevant     documentation</p> Solution <p>The secret is to use the <code>paths</code> keyword in the workflow file. We here specify that the workflow should only trigger when the <code>.dvc</code> folder or any file with the <code>.dvc</code> extension changes, which is the case when we update our data and call <code>dvc add data/</code>.</p> <pre><code>name: DVC Workflow\n\non:\n  pull_request:\n    branches:\n    - main\n    paths:\n    - '**/*.dvc'\n    - '.dvc/**'\n</code></pre> </li> <li> <p>The next step is to implement steps in our workflow that do something when data changes. This is the reason     why we created the <code>dataset_statistics</code> function. Implement a workflow that:</p> <ol> <li>Checks out the code</li> <li>Sets up Python</li> <li>Installs dependencies</li> <li>Downloads the data</li> <li>Runs the <code>dataset_statistics</code> function on the data</li> </ol> Solution <p>This solution assumes that data is stored in a GCP bucket and that the credentials are stored in a secret called <code>GCP_SA_KEY</code>. If this is not the case for you, you need to adjust the workflow accordingly with the correct way to pull the data.</p> <pre><code>jobs:\n  run_data_checker:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: 3.11\n        cache: 'pip'\n        cache-dependency-path: setup.py\n\n    - name: Install dependencies\n      run: |\n        make dev_requirements\n        pip list\n\n    - name: Auth with GCP\n      uses: google-github-actions/auth@v2\n      with:\n        credentials_json: ${{ secrets.GCP_SA_KEY }}\n\n    - name: Pull data\n      run: |\n        dvc pull --no-run-cache\n\n    - name: Check data statistics\n      run: |\n        python dataset_statistics.py\n</code></pre> </li> <li> <p>Let's make sure that the workflow works as expected for now. Create a new branch and either add or remove a file     in the <code>data/</code> folder. Then run</p> <pre><code>dvc add data/\ngit add data.dvc\ngit commit -m \"Update data\"\ngit push\ndvc push\n</code></pre> <p>to commit the changes to data. Open a pull request with the branch and make sure that the workflow activates and runs as expected.</p> </li> <li> <p>Let's now add the <code>cml</code> framework such that we can comment the results of the <code>dataset_statistics</code> function in     the pull request automatically. Look at the     getting started guide for help on how to do this. You will     need to write all the content of the <code>dataset_statistics</code> function to a file called <code>report.md</code> and then use the     <code>cml comment create</code> command to create a comment in the pull request with the content of the file.</p> Solution <pre><code>jobs:\n  dataset_statistics:\n    runs-on: ubuntu-latest\n    steps:\n    # ...all the previous steps\n    - name: Check data statistics &amp; generate report\n    run: |\n      python src/example_mlops/data.py &gt; data_statistics.md\n      echo '![](./mnist_images.png \"MNIST images\")' &gt;&gt; data_statistics.md\n      echo '![](./train_label_distribution.png \"Train label distribution\")' &gt;&gt; data_statistics.md\n      echo '![](./test_label_distribution.png \"Test label distribution\")' &gt;&gt; data_statistics.md\n\n    - name: Setup cml\n      uses: iterative/setup-cml@v2\n\n    - name: Comment on PR\n      env:\n        REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n      run: |\n        cml comment create data_statistics.md --watermark-title=\"Data Checker\" # (1)!\n</code></pre> <ol> <li> The <code>--watermark-title</code> flag is used to watermark the comment created by <code>cml</code>. It is     to make sure that no new comments are created every time the workflow runs.</li> </ol> </li> <li> <p>Make sure that the workflow works as expected. You should see a comment created by <code>github-actions (bot)</code> like     this if you have done everything correctly:</p> <p> </p> </li> <li> <p>(Optional) Feel free to add more checks to the workflow. For example, you could add a check that runs a small     baseline model on the updated data and checks that the model converges. This is a very common sanity check that     is done in machine learning pipelines.</p> </li> </ol> </li> <li> <p>For the second set of exercises, we are going to look at how to automatically run further testing of our models     whenever we add them to our model registry. For that reason, do not continue with this set of exercises before you     have completed the exercises on the model registry in this module.</p> <p>  The model registry is in general a repository of a team's trained models where ML practitioners publish candidates for production and share them with others. Figure from wandb.  <ol> <li> <p>The first step is in our weights and bias account to create a team. Some of these more advanced features are only     available for teams, however every user is allowed to create one team for free. Go to your weights and bias     account and create a team (the option should be on the left side of the UI). Give a team name and select W&amp;B     cloud storage.</p> </li> <li> <p>Now we need to generate a personal access token that can link our weights and bias account to our GitHub account.     Go to this page and generate a new token. You can also     find the page by clicking your profile icon in the upper right corner of GitHub and selecting     <code>Settings</code>, then <code>Developer settings</code>, then <code>Personal access tokens</code> and finally choose either     <code>Tokens (classic)</code> or <code>Fine-grained tokens</code> (which is the safer option, which is also what the link points to).</p> <p> </p> <p>Give it a name, set what repositories it should have access to and select the permissions you want it to have. In our case if you choose to create a <code>Fine-grained token</code> then it needs access to the <code>contents:write</code> permission. If you choose <code>Tokens (classic)</code> then it needs access to the <code>repo</code> permission. After you have created the token, copy it and save it somewhere safe.</p> </li> <li> <p>Go to the settings of your newly created team: https://wandb.ai/teamname/settings and scroll down to the     <code>Team secrets</code> section. Here add the token you just created as a secret with the name <code>GITHUB_ACTIONS_TOKEN</code>.     WANDB will now be able to use this token to trigger actions in your repository.</p> </li> <li> <p>On the same settings page, scroll down to the <code>Webhooks</code> settings. Click the <code>New webhook</code> button in fill in the     following information:</p> <ul> <li>Name: <code>github_actions_dispatch</code></li> <li>URL: <code>https://api.github.com/repos/&lt;owner&gt;/&lt;repo&gt;/dispatches</code></li> <li>Access token: <code>GITHUB_ACTIONS_TOKEN</code></li> <li>Secret: leave empty</li> </ul> <p>Here you need to replace <code>&lt;owner&gt;</code> and <code>&lt;repo&gt;</code> with your own information. The <code>/dispatches</code> endpoint is a special endpoint that all GitHub action workflows can listen to. Thus, if you ever want to set up a webhook in some other framework that should trigger a GitHub action, you can use this endpoint.</p> </li> <li> <p>Next, navigate to your model registry. It should hopefully contain at least one registry with at least one model     registered. If not, go back to the previous module and do that.</p> </li> <li> <p>When you have a model in your registry, click on the <code>View details</code> button. Then click the <code>New automation</code>     button. On the first page, select that you want to trigger the automation when an alias is added to a model     version, set that alias to <code>staging</code> and select the action type to be <code>Webhook</code>. On the next page, select the     <code>github_actions_dispatch</code> webhook that you just created and add this as the payload:</p> <pre><code>{\n    \"event_type\": \"staged_model\",\n    \"client_payload\":\n    {\n        \"event_author\": \"${event_author}\",\n        \"artifact_version\": \"${artifact_version}\",\n        \"artifact_version_string\": \"${artifact_version_string}\",\n        \"artifact_collection_name\": \"${artifact_collection_name}\",\n        \"project_name\": \"${project_name}\",\n        \"entity_name\": \"${entity_name}\"\n    }\n}\n</code></pre> <p>Finally, on the next page give the automation a name and click <code>Create automation</code>.</p> <p> </p> <p>Make sure you understand overall what is happening here.</p> Solution <p>The automation is set up to trigger a webhook whenever the alias <code>staging</code> is added to a model version. The webhook is set up to trigger a GitHub action workflow that listens to the <code>/dispatches</code> endpoint and has the event type <code>staged_model</code>. The payload that is sent to the webhook contains information about the model that was staged.</p> </li> <li> <p>We are now ready to create the <code>GitHub actions workflow</code> that listens to the <code>/dispatches</code> endpoint and triggers     whenever a model is staged. Create a new workflow file (called <code>stage_model.yaml</code>) and make sure it only     activates on the <code>staged_model</code> event. Hint: relevant     documentation</p> Solution <pre><code>name: Check staged model\n\non:\n  repository_dispatch:\n    types: staged_model\n</code></pre> <p>Do note that the <code>repository_dispatch</code> event will only trigger a workflow run if the workflow file exists on the default branch. Therefore, if you are testing this on a branch, you need to push the workflow file to the default branch.</p> </li> <li> <p>Next, we need to implement the steps in our workflow that do something when a model is staged. The payload that     is sent to the webhook contains information about the model that was staged. Implement a workflow that:</p> <ol> <li>Identifies the model that was staged</li> <li>Sets an environment variable with the corresponding artifact path</li> <li>Outputs the model name</li> </ol> Solution <pre><code>jobs:\n  identify_event:\n    runs-on: ubuntu-latest\n    outputs:\n      model_name: ${{ steps.set_output.outputs.model_name }}\n    steps:\n      - name: Check event type\n        run: |\n          echo \"Event type: repository_dispatch\"\n          echo \"Payload Data: ${{ toJson(github.event.client_payload) }}\"\n\n      - name: Setting model environment variable and output\n        id: set_output\n        run: |\n          echo \"model_name=${{ github.event.client_payload.artifact_version_string }}\" &gt;&gt; $GITHUB_OUTPUT\n</code></pre> </li> <li> <p>We now need to write a script that can be executed on our staged model. In this case, we are going to run some     performance tests on it to check that it is fast enough for deployment. Therefore, do the following:</p> <ol> <li> <p>In a <code>tests/performancetests</code> folder, create a new file called <code>test_model.py</code>.</p> </li> <li> <p>Implement a test that loads the model from a wandb artifact path e.g.     //: and runs it on a random input. Importantly, the     artifact path should be read from an environment variable called <code>MODEL_NAME</code>. <li> <p>The test should assert that the model can do 100 predictions in less than X amount of time.</p> </li> Solution <p>In this solution we assume that 4 environment variables are set: <code>WANDB_API</code>, <code>WANDB_ENTITY</code>, <code>WANDB_PROJECT</code> and <code>MODEL_NAME</code>.</p> test_model.py<pre><code>import wandb\nimport os\nimport time\nfrom my_project.models import MyModel\n\ndef load_model(artifact):\n    api = wandb.Api(\n        api_key=os.getenv(\"WANDB_API_KEY\"),\n        overrides={\"entity\": os.getenv(\"WANDB_ENTITY\"), \"project\": os.getenv(\"WANDB_PROJECT\")},\n    )\n    artifact = api.artifact(model_checkpoint)\n    artifact.download(root=logdir)\n    file_name = artifact.files()[0].name\n    return MyModel.load_from_checkpoint(f\"{logdir}/{file_name}\")\n\ndef test_model_speed():\n    model = load_model(os.getenv(\"MODEL_NAME\"))\n    start = time.time()\n    for _ in range(100):\n        model(torch.rand(1, 1, 28, 28))\n    end = time.time()\n    assert end - start &lt; 1\n</code></pre> <li> <p>Let's now add another job that calls the script we just wrote. It needs to:</p> <ul> <li>Set the correct environment variables</li> <li>Checkout the code</li> <li>Setup Python</li> <li>Install dependencies</li> <li>Run the test</li> </ul> <p>which is very similar to the kind of jobs we have written before.</p> Solution <pre><code>jobs:\n  identify_event:\n    ...\n  test_model:\n    runs-on: ubuntu-latest\n    needs: identify_event\n    env:\n      WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}\n      WANDB_ENTITY: ${{ secrets.WANDB_ENTITY }}\n      WANDB_PROJECT: ${{ secrets.WANDB_PROJECT }}\n      MODEL_NAME: ${{ needs.identify_event.outputs.model_name }}\n    steps:\n    - name: Echo model name\n      run: |\n        echo \"Model name: $MODEL_NAME\"\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: 3.11\n        cache: 'pip'\n        cache-dependency-path: setup.py\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip list\n\n    - name: Test model\n      run: |\n        pytest tests/performancetests/test_model.py\n</code></pre> </li> <li> <p>Finally, we are going to assume in this setup that if the model gets this far then it is ready for deployment.     We are therefore going to add a final job that will add a new alias to the model called <code>production</code>. Here is     some relevant Python code that can be used to add the alias:</p> <pre><code>import typer\nimport os\nimport wandb\n\n\ndef link_model(artifact_path: str, aliases: list[str] = [\"staging\"]) -&gt; None:\n    \"\"\"\n    Stage a specific model to the model registry.\n\n    Args:\n        artifact_path: Path to the artifact to stage.\n            Should be of the format \"entity/project/artifact_name:version\".\n        aliases: List of aliases to link the artifact with.\n\n    Example:\n        model_management link-model entity/project/artifact_name:version -a staging -a best\n\n    \"\"\"\n    if artifact_path == \"\":\n        typer.echo(\"No artifact path provided. Exiting.\")\n        return\n\n    api = wandb.Api(\n        api_key=os.getenv(\"WANDB_API_KEY\"),\n        overrides={\"entity\": os.getenv(\"WANDB_ENTITY\"), \"project\": os.getenv(\"WANDB_PROJECT\")},\n    )\n    _, _, artifact_name_version = artifact_path.split(\"/\")\n    artifact_name, _ = artifact_name_version.split(\":\")\n\n    artifact = api.artifact(artifact_path)\n    artifact.link(target_path=f\"{os.getenv('WANDB_ENTITY')}/model-registry/{artifact_name}\", aliases=aliases)\n    artifact.save()\n    typer.echo(f\"Artifact {artifact_path} linked to {aliases}\")\n\nif __name__ == \"__main__\":\n    typer.run(link_model)\n</code></pre> <p>For example, you can run this script with the following command:</p> <pre><code>python link_model.py entity/project/artifact_name:version -a staging -a production\n</code></pre> <p>Implement a final job that calls this script and adds the <code>production</code> alias to the model.</p> Solution <pre><code>jobs:\n  identify_event:\n    ...\n  test_model:\n    ...\n  add_production_alias:\n    runs-on: ubuntu-latest\n    needs: identify_event\n    env:\n      WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}\n      WANDB_ENTITY: ${{ secrets.WANDB_ENTITY }}\n      WANDB_PROJECT: ${{ secrets.WANDB_PROJECT }}\n      MODEL_NAME: ${{ needs.identify_event.outputs.model_name }}\n    steps:\n    - name: Echo model name\n      run: |\n        echo \"Model name: $MODEL_NAME\"\n\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: 3.11\n        cache: 'pip'\n        cache-dependency-path: setup.py\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip list\n\n    - name: Add production alias\n      run: |\n        python link_model.py $MODEL_NAME -a production\n</code></pre> </li> <li> <p>Finally, make sure the workflow works as expected. To try it out again and again for testing purposes, you can     just manually add and then delete the <code>staging</code> alias to any model version in the model registry.</p> </li> <li> <p>(Optional) Consider adding more checks to the workflow. For example, you could add a step that checks if the     model is too large for deployment, runs some further evaluation scripts, or checks if the model is robust to     adversarial attacks. Only the imagination sets the limits here.</p> </li> <li> <p>(Optional) If you have gotten this far, consider combining principles from the two exercises. Here is an idea: we use     the workflow from the second exercise to trigger a workflow that checks a staged model for performance. We then     use the <code>cml</code> framework to automatically create a pull request e.g. use <code>cml pr create</code> instead of     <code>cml comment create</code> to create a pull request with the results of the performance test. Then if we are happy with     the performance, we can then approve that pull request and the production alias is added to the model. This is a     better workflow because it allows for human intervention before the model is deployed.</p> </li>"},{"location":"s5_continuous_integration/cml/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>What is the difference between continuous integration and continuous machine learning?</p> Solution <p>There are three key differences between continuous integration and continuous machine learning:</p> <ul> <li>Scope: CI focuses on integrating and testing software code, while CML encompasses the entire lifecycle of     machine learning models, including data handling, model training, evaluation, deployment, and monitoring.</li> <li>Automation Focus: CI automates code testing and integration, whereas CML automates the training, evaluation,     deployment, and monitoring of machine learning models.</li> <li>Feedback Mechanisms: CI primarily uses automated tests to provide feedback on code quality. CML uses     performance metrics from deployed models to provide feedback and trigger retraining or model updates.</li> </ul> </li> <li> <p>Imagine you get hired in the pharmaceutical industry and are asked to develop a machine learning pipeline that can     automatically determine which drugs are safe and which are not. What level of the MLOps maturity model would you     strive to reach?</p> Solution <p>There is really no right or wrong answer here, but in most cases we would actually not aim for level 4. The reason is that the consequences of a bad model in this case can be severe. Therefore, we would probably not want automated retraining and model updates, which is what level 4 is about. Instead, we would probably aim for level 3 where we have automated testing and monitoring of our models but there is still human oversight in the process.</p> </li> </ol> <p>This ends the module on continuous machine learning. As we have hopefully convinced you, it is only the imagination that sets the limits for what you can use GitHub actions for in your machine learning pipeline. However, we do want to stress that it is important that human oversight is always present in the process. Automation is great, but it should never replace human judgement. This is especially true in machine learning where the consequences of a bad model can be severe if it is used in critical decision making.</p> <p>Finally, if you have completed the exercises on using the cloud consider checking out the cml runner lunch command that allows you to run your workflows on cloud resources instead of the GitHub action runners.</p>"},{"location":"s5_continuous_integration/github_actions/","title":"M17 - GitHub Actions","text":""},{"location":"s5_continuous_integration/github_actions/#github-actions","title":"GitHub actions","text":"<p>Core Module</p> <p>With the tests established in the previous module, we are now ready to move on to implementing some continuous integration in our pipeline. As you probably have already realized, testing your code locally may be cumbersome to do, because</p> <ul> <li>You need to run it often to make sure to catch bugs early on</li> <li>If you want to have high code coverage of your code base, you will need many tests that take a long time to run</li> </ul> <p>For these reasons, we want to automate the testing, such that it is done every time we push to our repository. If we combine this with only pushing to branches and then only merging these branches whenever all automated testing has passed, our code should be fairly safe against unwanted bugs (assuming your tests cover your code well).</p>"},{"location":"s5_continuous_integration/github_actions/#github-actions_1","title":"GitHub actions","text":"<p>GitHub actions are the continuous integration solution that GitHub provides. Each of your repositories gets 2,000 minutes of free testing per month which should be more than enough for the scope of this course (and probably all personal projects you do). Getting GitHub actions set up in a repository may seem complicated at first, but workflow files that you create for one repository can more or less be reused for any other repository that you have.</p> <p>Let's take a look at how a GitHub workflow file is organized:</p> <ul> <li>Initially, we start by giving the workflow a <code>name</code>.</li> <li>Next, we specify what events the workflow should be triggered. This includes both the action     (pull request, push, etc.) and on what branches it should activate.</li> <li>Next, we list the jobs that we want to do. Jobs are by default executed in parallel but can     also be dependent on each other.</li> <li>In the <code>runs-on</code>, we can specify which operating system we want the workflow to run on.</li> <li>Finally, we have the <code>steps</code>. This is where we specify the actual commands that should be     run when the workflow is executed.</li> </ul> <p></p>  Image credit"},{"location":"s5_continuous_integration/github_actions/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by creating a <code>.github</code> folder in the root of your repository. Add a sub-folder to that called <code>workflows</code>.</p> </li> <li> <p>Go over this page that explains how to do     automated testing of Python code in GitHub actions. You do not have     to understand everything, but at least get a feeling of what a workflow file should look like.</p> </li> <li> <p>We have provided a workflow file called <code>tests.yaml</code> that should run your tests for you. Place     this file in the <code>.github/workflows/</code> folder. The workflow file consists of three steps:</p> <ul> <li> <p>First, a Python environment is initiated (in this case Python 3.8)</p> </li> <li> <p>Next, all dependencies required to run the test are installed</p> </li> <li> <p>Finally, <code>pytest</code> is called and our tests will be run</p> </li> </ul> <p>Go over the file and try to understand the overall structure and syntax of the file.</p> <code>tests.yaml</code> tests.yaml<pre><code>name: \"Run tests\"\n\non:\n  push:\n    branches: [ master, main ]\n  pull_request:\n    branches: [ master, main ]\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: 3.11\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n        pip install -r requirements_tests.txt\n    - name: Test with pytest\n      run: |\n        pytest -v\n</code></pre> </li> <li> <p>For the script to work you need to define the <code>requirements.txt</code> and <code>requirements_tests.txt</code>. The first file should     contain all the packages required to run your code. The second file contains all additional packages required to     run the tests. In your simple case, it may very well be that the second file is empty; however, sometimes additional     packages are used for testing that are not strictly required for the scripts to run.</p> </li> <li> <p>Finally, try pushing the changes to your repository. Hopefully, your tests should just start, and after some     time you will see a green check mark next to the hash of the commit. Also, try to inspect the Actions tab where     you can see the history of actions run.</p> <p> </p> </li> <li> <p>Normally we develop code on only one operating system and just hope that it will work on other operating systems.     However, continuous integration enables us to automatically test on systems different to the one we are using.</p> <ol> <li> <p>The provided <code>tests.yaml</code> only runs on one operating system. Which one?</p> </li> <li> <p>Alter the file such that it executes the test on the two other main operating systems that exist. You can find     information on available operating systems also called runners here</p> Solution <p>We can \"parametrize\" the script to run on different operating systems by using the <code>strategy</code> attribute. This attribute allows us to define a matrix of values that the workflow will run on. The following code will run the tests on <code>ubuntu-latest</code>, <code>windows-latest</code>, and <code>macos-latest</code>:</p> tests.yaml<pre><code>jobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [\"ubuntu-latest\", \"windows-latest\", \"macos-latest\"]\n</code></pre> </li> <li> <p>Can you also figure out how to run the tests using different Python versions?</p> Solution <p>Just add another line to the <code>strategy</code> attribute that specifies the Python version and use the value in the setup Python action. The following code will run the tests on Python versions:</p> tests.yaml<pre><code>jobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [\"ubuntu-latest\", \"windows-latest\", \"macos-latest\"]\n        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n    - uses: actions/checkout@v4\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python-version }}\n</code></pre> </li> <li> <p>If you push the above changes you will maybe see that whenever one of the tests in the matrix fails, it will     automatically cancel the other tests. This is for saving time and resources. However, sometimes you want all the     tests to run even if one fails. Can you figure out how to do that?</p> Solution <p>You can set the <code>fail-fast</code> attribute to <code>false</code> under the <code>strategy</code> attribute:</p> tests.yaml<pre><code>jobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [\"ubuntu-latest\", \"windows-latest\", \"macos-latest\"]\n        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n</code></pre> </li> </ol> </li> <li> <p>As the workflow is currently implemented, GitHub actions will destroy every downloaded package     when the workflow has been executed. To improve this we can take advantage of <code>caching</code>:</p> <ol> <li> <p>Figure out how to implement <code>caching</code> in your workflow file. You can find a guide     here and     here.</p> Solution tests.yaml<pre><code>steps:\n- uses: actions/checkout@v4\n- uses: actions/setup-python@v5\n  with:\n    python-version: 3.11\n    cache: 'pip' # caching pip dependencies\n- run: pip install -r requirements.txt\n</code></pre> </li> <li> <p>When you have implemented a caching system go to <code>Actions-&gt;Caches</code> in your repository and make sure that they     are correctly added. It should look something like the image below.</p> <p> </p> </li> <li> <p>Measure how long your workflow takes before and after adding <code>caching</code>. Did caching improve the     runtime of your workflow?</p> </li> </ol> </li> <li> <p>(Optional) Code coverage can also be added to the workflow file by uploading it as an artifact     after running the coverage. Follow the instructions in this     post     on how to do it.</p> </li> <li> <p>With different checks in place, it is a good time to learn about branch protection rules. A branch     protection rule is essentially some kind of guard that prevents you from merging code into a branch before     certain conditions are met. In this exercise, we will create a branch protection rule that requires all checks to     pass before merging code into the main branch.</p> <ol> <li> <p>Start by going into your <code>Settings -&gt; Rules -&gt; Rulesets</code> and create a new branch ruleset. See the image below.</p> <p> </p> </li> <li> <p>In the ruleset start by giving it a name and then set the target branches to be <code>Default branch</code>. This means that     the ruleset will be applied to your master/main branch. As shown in the image below, two rules may be     particularly beneficial when you later start working with other people:</p> <ul> <li> <p>The first rule to consider is Require a pull request before merging. As the name suggests, this rule     requires that changes that are to be merged into the main branch must be done through a pull request. This     is a good practice as it allows for code review and testing before the code is merged into the main branch.     Additionally, this opens the option to specify that the code must be reviewed (or at least approved) by     a certain number of people.</p> </li> <li> <p>The second rule to consider is Require status checks to pass. This rule makes sure that our workflows     are passing before we can merge code into the main branch. You can select which workflows are required, as     some may be nice to have passing but not strictly needed.</p> </li> </ul> <p> </p> <p>Finally, if you think the rules are a bit too restrictive you can always add that the repository admin, e.g. you, can bypass the rules by adding <code>Repository admin</code> to the bypass list. Implement the following rules:</p> <ul> <li>At least one person needs to approve any PR</li> <li>All your workflows need to pass</li> <li>All conversations need to be resolved</li> </ul> </li> <li> <p>If you have created the rules correctly you should see something like the image below when you try to merge a     pull request. In this case, all three checks are required to pass before the code can be merged. Additionally,     a single reviewer is required to approve the code. A bypass rule is also setup for the repository admin.</p> <p> </p> </li> </ol> </li> <li> <p>One problem you may have encountered is running tests that have to do with your data, with the core problem     being that your data is not stored on GitHub (assuming you have done module     M8 - DVC) and therefore cannot be tested. However, we can download     data while running our continuous integration. Let's try to create that.</p> <ol> <li> <p>The first problem is that we need our continuous integration pipeline to be able to authenticate with our storage     solution. We can take advantage of an authentication file that is created the first time we push with DVC. It is     located in <code>$CACHE_HOME/pydrive2fs/{gdrive_client_id}/default.json</code> where <code>$CACHE_HOME</code> depends on your     operating system:</p> macOSLinuxWindows <p><code>~/Library/Caches</code></p> <p><code>~/.cache</code>  This is the typical location, but it may vary depending on what distro you are running</p> <p><code>{user}/AppData/Local</code></p> <p>Find the file. The content should look similar to this (only some fields are shown):</p> <pre><code>{\n    \"access_token\": ...,\n    \"client_id\": ...,\n    \"client_secret\": ...,\n    \"refresh_token\": ...,\n    ...\n}\n</code></pre> </li> <li> <p>The content of that file should be treated as a password and not shared with the world and the relevant     question is therefore how to use this info in a public repository. The answer is GitHub secrets, where we     can store information and access it in our workflow files and it is still not public. Navigate to the secrets     option (as shown below) and create a secret with the name <code>GDRIVE_CREDENTIALS_DATA</code> that contains the content     of the file you found in the previous exercise.</p> <p> </p> </li> <li> <p>Afterward, add the following code to your workflow file:</p> <pre><code>- uses: iterative/setup-dvc@v1\n- name: Get data\n  run: dvc pull\n  env:\n    GDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}\n</code></pre> <p>which runs <code>dvc pull</code> using the secret authentication file. For help you can visit this small repository that implements the same workflow.</p> </li> <li> <p>Finally, add the changes, commit, push and confirm that everything works as expected. You should now be able to     run unit tests that depend on your input data.</p> </li> </ol> </li> <li> <p>In module M6 on good coding practices     (optional module) of the course you were introduced to a couple of good coding practices such as being consistent     with your coding style, how your Python packages are sorted and that your code follows certain standards. All this     was done using the <code>ruff</code> framework. In this set of exercises, we will create GitHub workflows that will     automatically test for this.</p> <ol> <li> <p>Create a new workflow file called <code>codecheck.yaml</code>, that implements the following three steps:</p> <ul> <li> <p>Sets up Python environment</p> </li> <li> <p>Installs <code>ruff</code></p> </li> <li> <p>Runs <code>ruff check</code> and <code>ruff format</code> on the repository</p> </li> </ul> <p>(HINT: You should be able to just change the last steps of the <code>tests.yaml</code> workflow file)</p> Solution codecheck.yaml<pre><code>name: Code formatting\n\non:\n  push:\n    branches:\n    - main\n  pull_request:\n    branches:\n    - main\n\njobs:\n  format:\n      runs-on: ubuntu-latest\n      steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.11\n          cache: 'pip'\n          cache-dependency-path: setup.py\n      - name: Install dependencies\n        run: |\n          pip install ruff\n          pip list\n      - name: Ruff check\n        run: ruff check .\n      - name: Ruff format\n        run: ruff format .\n</code></pre> </li> <li> <p>In addition to <code>ruff</code> we also used <code>mypy</code> in those sets of exercises for checking if the typing we added to our     code was good enough. Add another step to the <code>codecheck.yaml</code> file that runs <code>mypy</code> on your repository.</p> </li> <li> <p>Try to make sure that all steps are passed on the repository. Especially <code>mypy</code> can be hard to get passing, so this     exercise formally only requires you to get <code>ruff</code> passing.</p> </li> </ol> </li> <li> <p>(Optional) As you have probably already experienced in module M9 on docker, it can     be cumbersome to build docker images, sometimes taking a couple of minutes to build each time we make changes to our     code base. For this reason, we just want to build a new image every time we commit our code because that should mark     that we believe the code to be working at that point. Thus, let's automate the process of building our docker images     using GitHub actions. Do note that in a future module we will look at how to     build containers using cloud providers, and this exercise is therefore very much optional.</p> <ol> <li> <p>Start by making sure you have a dockerfile in your repository. If you do not have one, you can use the following     simple dockerfile:</p> <pre><code>FROM busybox\nCMD echo \"Howdy cowboy\"\n</code></pre> </li> <li> <p>Push the dockerfile to your repository</p> </li> <li> <p>Next, create a Docker Hub account</p> </li> <li> <p>Within Docker Hub create an access token by going to <code>Settings -&gt; Security</code>. Click the <code>New Access Token</code> button     and give it a name that you recognize.</p> </li> <li> <p>Copy the newly created access token and head over to your GitHub repository online. Go to     <code>Settings -&gt; Secrets -&gt; Actions</code> and click the <code>New repository secret</code>. Copy over the access token and give     it the name <code>DOCKER_HUB_TOKEN</code>. Additionally, add two other secrets <code>DOCKER_HUB_USERNAME</code> and     <code>DOCKER_HUB_REPOSITORY</code> that contain your docker username and docker repository name respectively.</p> </li> <li> <p>Next, we are going to construct the actual GitHub actions workflow file</p> <pre><code>name: Docker Image continuous integration\n\non:\n  push:\n    branches: [ master ]\n\njobs:\n    build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - name: Build the Docker image\n      run: |\n        echo \"${{ secrets.DOCKER_HUB_TOKEN }}\" | docker login \\\n          -u \"${{ secrets.DOCKER_HUB_USERNAME }}\" --password-stdin docker.io\n        docker build . --file Dockerfile \\\n          --tag docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/${{ secrets.DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA\n        docker push \\\n          docker.io/${{ secrets.DOCKER_HUB_USERNAME }}/${{ secrets.DOCKER_HUB_REPOSITORY }}:$GITHUB_SHA\n</code></pre> <p>The first part of the workflow file should look somewhat recognizable. However, the last three lines are where all the magic happens. Carefully go through them and figure out what they do. If you want some help you can look at the help page for <code>docker login</code>, <code>docker build</code> and <code>docker push</code>.</p> </li> <li> <p>Upload the workflow to your GitHub repository and check that it is being executed. If everything works you should     be able to see the built docker image in your container repository in the docker hub.</p> </li> <li> <p>Make sure that you can execute <code>docker pull</code> locally to pull down the image that you just continuously build.</p> </li> <li> <p>(Optional) To test that the container works directly in GitHub you can also try to include an additional     step that runs the container.</p> <pre><code>    - name: Run container\n      run: |\n        docker run ...\n</code></pre> </li> </ol> </li> </ol>"},{"location":"s5_continuous_integration/github_actions/#dependabot","title":"Dependabot","text":"<p>A great feature that GitHub provides is the ability to have bots help you with maintaining your repository. One of the most useful bots is called <code>Dependabot</code>. As the name suggests, <code>Dependabot</code> helps you keep your dependencies up to date. This is important because dependencies often either contain fixes for bugs or security vulnerabilities that you want to have in your code.</p>"},{"location":"s5_continuous_integration/github_actions/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>To get dependabot working in your repository, we need to add a single configuration file to your repository. Create     a file called <code>.github/dependabot.yaml</code>. Look through the     documentation for how to set up     the file such that it updates your Python dependencies on a weekly basis.</p> Solution <p>The following code will check for updates in the <code>pip</code> ecosystem every week, i.e. it automatically will look for <code>requirements.txt</code> files and update the packages in there.</p> <pre><code>version: 2\nupdates:\n  - package-ecosystem: \"pip\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n</code></pre> <ol> <li>Push the changes to your repository and check that the dependabot is working by going to the <code>Insights</code> tab and then the <code>Dependency graph</code> tab. From here under the <code>Dependabot</code> tab you should be able to see if the bot has correctly identified what files to track and if it has found any updates.</li> </ol> <p> </p> <p>Click the <code>Recent update jobs</code> to see the history of Dependabot checking for updates. If there are no updates you can try to click the <code>Check for updates</code> button to force Dependabot to check for updates.</p> </li> <li> <p>At this point the Dependabot should hopefully have found some updates and created one or more pull requests. If it     has not done so you most likely need to update your requirement file such that your dependencies are correctly     restricted/specified e.g.</p> <pre><code># lets assume pytorch v2.5 is the latest version\n\n# these different specifications will not trigger dependabot because\n# the latest version is included in the specification\ntorch\ntorch == 2.5\ntorch &gt;= 2.5\ntorch ~= 2.5\n\n# these specifications will trigger dependabot because the latest\n# version is not included\ntorch &lt; 2.5\ntorch == 2.4\ntorch &lt;= 2.4\n</code></pre> <p>If you have a pull request from Dependabot, check it out and see if it looks good. If it does, you can merge it.</p> <p> </p> </li> <li> <p>(Optional) Dependabot can also help keep our GitHub Actions pipelines up-to-date. As you may have realized     during this module, when we write statements like in our workflow files:</p> <pre><code>...\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n...\n</code></pre> <p>The <code>@v4</code> specifies that we are using version 4 of the <code>actions/checkout</code> action. This means that if a new version of the action is released, we will not automatically get the new version. Dependabot can help us with this. Try adding to the <code>dependabot.yaml</code> file that Dependabot should also check for updates in the GitHub Actions ecosystem.</p> Solution <pre><code>version: 2\nupdates:\n  - package-ecosystem: \"pip\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n  - package-ecosystem: \"github-actions\"\n    schedule:\n      interval: \"weekly\"\n</code></pre> </li> </ol>"},{"location":"s5_continuous_integration/github_actions/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>When working with GitHub actions you will often encounter the following 4 concepts:</p> <ul> <li>Workflow</li> <li>Runner</li> <li>Job</li> <li>Action</li> </ul> <p>Try to define them in your own words.</p> Solution <ul> <li>Workflow: A <code>yaml</code> file that defines the instructions to be executed on specific events. Needs to be placed in     the <code>.github/workflows</code> folder.</li> <li>Runner: Workflows need to run somewhere. The environment that the workflow is being executed on is called the     runner. Most commonly the runner is hosted by GitHub but can also hosted by yourself.</li> <li>Job: A series of steps that are executed on the same runner. A workflow must include at least one job but     often contains many.</li> <li>Action: An action is the smallest unit in a workflow. Jobs often consist of multiple actions that are     executed sequentially.</li> </ul> </li> <li> <p>The <code>on</code> attribute specifies upon which events the workflow will be triggered. Assume you have set the <code>on</code> attribute     to the following:</p> <pre><code>on:\n    push:\n      branches: [main]\n    pull_request:\n      branches: [main]\n    schedule:\n      - cron: \"0 0 * * *\"\n    workflow_dispatch: {}\n</code></pre> <p>What 4 events would trigger the execution of that action?</p> Solution <ol> <li>Direct push to branch <code>main</code> would trigger it</li> <li>Any pull request opened that will merge into <code>main</code> will trigger it</li> <li>At the end of the day the action would trigger, see cron for more info</li> <li> <p>The trigger can be executed by manually triggering it through the GitHub UI, for example, shown below</p> <p> </p> </li> </ol> </li> </ol> <p>This ends the module on GitHub workflows. If you are more interested in this topic you can check out module M31 on documentation which first includes locally building some documentation for your project and afterward using GitHub actions for deploying it to GitHub Pages. Additionally, GitHub also has a lot of templates already for running different continuous integration tasks. If you try to create a workflow file directly in GitHub you may encounter the following page:</p> <p></p> <p>We highly recommend checking this out if you want to write any other kind of continuous integration pipeline in GitHub actions. We can also recommend this repository that has a list of awesome actions, and check out the act repository which is a tool for running your GitHub Actions locally!</p>"},{"location":"s5_continuous_integration/pre_commit/","title":"M18 - Pre-commit","text":""},{"location":"s5_continuous_integration/pre_commit/#pre-commit","title":"Pre-commit","text":"<p>One of the cornerstones of working with git is remembering to commit your work often. Frequent committing ensures that it is easier to identify and revert unwanted changes that you have introduced, because the code changes become smaller per commit.</p> <p>However, as you have hopefully already seen in the course there are a lot of mental tasks to do before you actually write <code>git commit</code> in the terminal. The most basic thing is of course making sure that you have saved all your changes, and you are not committing a not up-to-date file. However, this also includes tasks such as styling, formatting, making sure all tests succeed, etc. All these mental to-do notes do not mix well with the principal of remembering to commit often because you in principal have to do them every time.</p> <p>The obvious solution to this problem is to automate all or some of our mental tasks every time that we make a commit. This is where pre-commit hooks come into play, as they can help us attach additional tasks that should be run every time that we do a <code>git commit</code>.</p>"},{"location":"s5_continuous_integration/pre_commit/#configuration","title":"Configuration","text":"<p>Pre-commit simply works by inserting whatever workflow we want to automate in between whenever we do a <code>git commit</code> and afterwards would do a <code>git push</code>.</p> <p></p>  Image credit  <p>The system works by looking for a file called <code>.pre-commit-config.yaml</code> that we can configure. If you execute</p> <pre><code>pre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\n</code></pre> <p>you should get a sample file that looks like</p> <pre><code># See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n</code></pre> <p>The file structure is very simple:</p> <ul> <li>It starts by listing the repositories where we want to get our pre-commits from, in this case   https://github.com/pre-commit/pre-commit-hooks. This repository contains a large collection of pre-commit hooks.</li> <li>Next we need to define what pre-commit hooks we want to get by specifying the <code>id</code> of the different hooks.   The <code>id</code> corresponds to an <code>id</code> in this file:   https://github.com/pre-commit/pre-commit-hooks/blob/master/.pre-commit-hooks.yaml</li> </ul> <p>When we are done defining our <code>.pre-commit-config.yaml</code> we just need to install it.</p> <pre><code>pre-commit install\n</code></pre> <p>This will make sure that the file is automatically executed whenever we run <code>git commit</code>.</p>"},{"location":"s5_continuous_integration/pre_commit/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Install pre-commit.</p> <pre><code>pip install pre-commit\n</code></pre> <p>Consider adding <code>pre-commit</code> to a <code>requirements_dev.txt</code> file, as it is a development tool.</p> </li> <li> <p>Next create the sample file:</p> <pre><code>pre-commit sample-config &gt; .pre-commit-config.yaml\n</code></pre> </li> <li> <p>The sample file already contains 4 hooks. Make sure you understand what each does and if you need them at all.</p> </li> <li> <p><code>pre-commit</code> works by hooking into the <code>git commit</code> command, running whenever that command is run. For this to work,     we need to install the hooks into <code>git commit</code>. Run</p> <pre><code>pre-commit install\n</code></pre> <p>to do this.</p> </li> <li> <p>Try to commit your recently created <code>.pre-commit-config.yaml</code> file. You will likely not do anything, because     <code>pre-commit</code> only checks files that are being committed. Instead try to run</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>which will check every file in your repository.</p> </li> <li> <p>Try adding at least another check from the base repository to your     <code>.pre-commit-config.yaml</code> file.</p> Solution <p>In this case we have added the <code>check-json</code> hook to our <code>.pre-commit-config.yaml</code> file, which will automatically check that all JSON files are valid.</p> <pre><code>repos:\n-   repo:\n    rev: v3.2.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n    -   id: check-json\n</code></pre> </li> <li> <p>If you have completed the optional module     M7 on good coding practice you will have learned     about the linter <code>ruff</code>. <code>ruff</code> comes with its own pre-commit hook.     Try adding that to your <code>.pre-commit-config.yaml</code> file and see what happens when you try to commit files.</p> Solution <p>This is one way to add the <code>ruff</code> pre-commit hook. We run both the <code>ruff</code> and <code>ruff-format</code> hooks, and we also add the <code>--fix</code> argument to the <code>ruff</code> hook to try to fix what is possible.</p> <pre><code>repos:\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  rev: v0.4.7\n  hooks:\n    # try to fix what is possible\n    - id: ruff\n        args: [\"--fix\"]\n    # perform formatting updates\n    - id: ruff-format\n    # validate if all is fine with preview mode\n    - id: ruff\n</code></pre> </li> <li> <p>(Optional) Add more hooks to your <code>.pre-commit-config.yaml</code>.</p> </li> <li> <p>Sometimes you are in a hurry, so make sure that you also can make commits without running <code>pre-commit</code> e.g.</p> <pre><code>git commit -m &lt;message&gt; --no-verify\n</code></pre> </li> <li> <p>Finally, figure out how to disable <code>pre-commit</code> again (if you get tired of it).</p> </li> <li> <p>Assuming you have completed the module on GitHub Actions, let's try to add a     <code>pre-commit</code> workflow that automatically runs your <code>pre-commit</code> checks every time you push to your repository and     then automatically commits those changes to your repository. We recommend that you make use of</p> <ul> <li>this pre-commit action for installing and running <code>pre-commit</code></li> <li>this commit action to automatically commit the   changes that <code>pre-commit</code> makes</li> </ul> <p>As an alternative you can configure the CI tool provided by the creators of <code>pre-commit</code>.</p> Solution <p>The workflow first uses the <code>pre-commit</code> action to install and run the <code>pre-commit</code> checks. Importantly we run it with <code>continue-on-error: true</code> to make sure that the workflow does not fail if the checks fail. Next, we use <code>git diff</code> to list the changes that <code>pre-commit</code> has made and then we use the <code>git-auto-commit-action</code> to commit those changes.</p> .github/workflows/pre_commit.yaml<pre><code>name: Pre-commit CI\n\non:\n  pull_request:\n  push:\n    branches: [main]\n\njobs:\n  pre-commit:\n    name: Check pre-commit\n    runs-on: ubuntu-latest\n\n    permissions:\n      contents: write\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: 3.11\n\n    - name: Install pre-commit\n      uses: pre-commit/action@v3.0.1\n      continue-on-error: true\n\n    - name: List modified files\n      run: |\n        git diff --name-only\n\n    - name: Commit changes\n      uses: stefanzweifel/git-auto-commit-action@v5\n      with:\n        commit_message: Pre-commit fixes\n        commit_options: '--no-verify'\n</code></pre> </li> <li> <p>(Optional) Another integration between pre-commit and GitHub Actions that you may want to consider, is using     GitHub Actions to automatically update your <code>.pre-commit-config.yaml</code> file. If you're done the module on     GitHub Actions you would think this could be implemented using     Dependabot, however this is currently not supported and there are currently no     plans to support it. Instead, let's create a custom     workflow that does this. Do the following:</p> <ul> <li>Create a new workflow file called <code>update_pre_commit.yaml</code> in the <code>.github/workflows</code> directory.</li> <li>The workflow should run on a schedule, e.g. every week.</li> <li>The workflow should check out your code, install a recent version of python and then install pre-commit</li> <li>The workflow should then run pre-commit autoupdate to update     the <code>.pre-commit-config.yaml</code> file.</li> <li>Finally, the workflow should then create a pull request with the changes. For this we recommend using this     action.</li> </ul> <p>Implement the workflow and run it to confirm that a PR is created with the changes.</p> Solution <pre><code># pre-commit auto-update workflow\n# Dependabot does not support updating pre-commit so this workflow will do that\nname: Pre-commit auto-update\n\non:\n  schedule:\n    - cron: '0 0 * * 0'\n  workflow_dispatch: {}  # Allows manual executions\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  auto-update:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: 3.11\n          cache: 'pip'\n\n      - name: Upgrade pip\n        run: python -m pip install --upgrade pip\n\n      - name: Install pre-commit\n        run: pip install pre-commit\n\n      - name: Run pre-commit autoupdate\n        run: pre-commit autoupdate\n\n      - name: Create Pull Request\n        uses: peter-evans/create-pull-request@v7\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          branch: update/pre-commit-autoupdate\n          title: Auto-update pre-commit hooks\n          commit-message: Auto-update pre-commit hooks\n          body: Update versions of tools in pre-commit configs to latest version\n          labels: dependencies\n</code></pre> </li> </ol> <p>That was all about how <code>pre-commit</code> can be used to automate tasks. If you want to deep dive more into the topic you can checkout this page on how to define your own <code>pre-commit</code> hooks.</p>"},{"location":"s5_continuous_integration/unittesting/","title":"M16 - Unit testing","text":""},{"location":"s5_continuous_integration/unittesting/#unit-testing","title":"Unit testing","text":"<p>Core Module</p> <p>What often comes to mind for many developers when discussing continuous integration (CI) is code testing. Continuous integration should ensure that whenever a codebase is updated it is automatically tested such that if bugs have been introduced in the codebase they will be caught early on. If you look at the MLOps cycle, continuous integration is one of the cornerstones of the operations part. However, it should be noted that applying continuous integration does not magically ensure that your code does not crash. Continuous integration is only as strong as the tests that are automatically executed. Continuous integration simply structures and automates this.</p> <p>Quote</p> <p>Continuous Integration doesn\u2019t get rid of bugs, but it does make them dramatically easier to find and remove.  Martin Fowler, Chief Scientist, ThoughtWorks</p> <p></p>  Image credit  <p>The kind of tests we are going to look at are called unit tests. Unit testing refers to the practice of writing tests that test individual parts of your code base to test for correctness. By unit, you can therefore think of a function, module or in general any object. By writing tests in this way it should be very easy to isolate which part of the code broke after an update to the code base. Another way to test your code base would be through integration testing which is equally important but we are not going to focus on it in this course.</p> <p>Unit tests (and integration tests) are not a unique concept to MLOps but are a core concept of DevOps. However, it is important to note that testing machine learning-based systems is much more difficult than traditional systems. The reason for this is that machine learning systems depend on data that influences the state of our system. For this reason, we not only need unit tests and integration tests of our code but also need data testing, infrastructure testing and more monitoring to check that we stay within the data distribution we are training on (more on this in module M25 on data drifting). This added complexity is illustrated in the figure below.</p> <p></p>"},{"location":"s5_continuous_integration/unittesting/#pytest","title":"Pytest","text":"<p>Before we can begin to automate testing of our code base we of course need to write the tests first. It is both a hard and tedious task to do but arguably the most important aspect of continuous integration. Python offers a couple of different libraries for writing tests. We are going to use <code>pytest</code>.</p>"},{"location":"s5_continuous_integration/unittesting/#exercises","title":"\u2754 Exercises","text":"<p>The following exercises should be applied to your MNIST repository</p> <ol> <li> <p>The first part of doing continuous integration is writing the unit tests. We do not expect you to cover every part     of the code you have developed but try to at least write tests that cover two files. Start by     creating a <code>tests</code> folder.</p> </li> <li> <p>Read the getting started guide for pytest     which is the testing framework that we are going to use.</p> </li> <li> <p>Install pytest:</p> <pre><code>pip install pytest\n</code></pre> </li> <li> <p>Write some tests. Below are some guidelines on some tests that should be implemented, but     you are of course free to implement more tests. You can at any point check if your tests are     passing by typing in a terminal:</p> <pre><code>pytest tests/\n</code></pre> <p>When you implement a test you need to follow two standards, for <code>pytest</code> to be able to find your tests. First, any files created (except <code>__init__.py</code>) should always start with <code>test_*.py</code>. Secondly, any test implemented needs to be wrapped into a function that again needs to start with <code>test_*</code>:</p> <pre><code># this will be found and executed by pytest\ndef test_something():\n    ...\n\n# this will not be found and executed by pytest\ndef something_to_test():\n    ...\n</code></pre> <ol> <li> <p>Start by creating a <code>tests/__init__.py</code> file and fill in the following:</p> <pre><code>import os\n_TEST_ROOT = os.path.dirname(__file__)  # root of test folder\n_PROJECT_ROOT = os.path.dirname(_TEST_ROOT)  # root of project\n_PATH_DATA = os.path.join(_PROJECT_ROOT, \"data\")  # root of data\n</code></pre> <p>These can help you refer to your data files during testing. For example, in another test file, I could write</p> <pre><code>from tests import _PATH_DATA\n</code></pre> <p>which then contains the root path to my data.</p> </li> <li> <p>Data testing: In a file called <code>tests/test_data.py</code> implement at least a test that checks that data gets     correctly loaded. By this, we mean that you should check</p> <pre><code>def test_data():\n    dataset = MNIST(...)\n    assert len(dataset) == N_train for training and N_test for test\n    assert that each datapoint has shape [1,28,28] or [784] depending on how you choose to format\n    assert that all labels are represented\n</code></pre> <p>where <code>N_train</code> should be either 30,000 or 50,000 depending on if you are just using the first subset of the corrupted MNIST data or also including the second subset. <code>N_test</code> should be 5,000.</p> Solution <pre><code>from my_project.data import corrupt_mnist\n\ndef test_data():\n    train, test = corrupt_mnist()\n    assert len(train) == 30000\n    assert len(test) == 5000\n    for dataset in [train, test]:\n        for x, y in dataset:\n            assert x.shape == (1, 28, 28)\n            assert y in range(10)\n    train_targets = torch.unique(train.tensors[1])\n    assert (train_targets == torch.arange(0,10)).all()\n    test_targets = torch.unique(test.tensors[1])\n    assert (test_targets == torch.arange(0,10)).all()\n</code></pre> </li> <li> <p>Model testing: In a file called <code>tests/test_model.py</code> implement at least a test that checks for a given input     with shape X that the output of the model has shape Y.</p> Solution <pre><code>from my_project.model import MyAwesomeModel\n\ndef test_model():\n    model = MyAwesomeModel()\n    x = torch.randn(1, 1, 28, 28)\n    y = model(x)\n    assert y.shape == (1, 10)\n</code></pre> </li> <li> <p>Training testing: In a file called <code>tests/test_training.py</code> implement at least one test that asserts something     about your training script. You are here given free rein as to what should be tested but try to test something     that risks being broken when developing the code.</p> </li> <li> <p>Good code raises errors and gives out warnings in appropriate places. This is often in     the case of some invalid combination of inputs to your script. For example, your model could check for the size     of the input given to it (see code below) to make sure it corresponds to what you are expecting. Not     implementing such errors would still result in PyTorch failing at a later point due to shape errors. However,     these custom errors will probably make more sense to the end user. Implement at least one raised error or     warning somewhere in your code and use either <code>pytest.raises</code> or <code>pytest.warns</code> to check that     they are correctly raised/warned. As inspiration, the following implements <code>ValueError</code> in code     belonging to the model:</p> <pre><code># src/models/model.py\ndef forward(self, x: Tensor):\n    if x.ndim != 4:\n        raise ValueError('Expected input to a 4D tensor')\n    if x.shape[1] != 1 or x.shape[2] != 28 or x.shape[3] != 28:\n        raise ValueError('Expected each sample to have shape [1, 28, 28]')\n</code></pre> Solution <p>The above example would be captured by a test looking something like this:</p> <pre><code># tests/test_model.py\nimport pytest\nfrom my_project.model import MyAwesomeModel\n\ndef test_error_on_wrong_shape():\n    model = MyAwesomeModel()\n    with pytest.raises(ValueError, match='Expected input to a 4D tensor'):\n        model(torch.randn(1,2,3))\n    with pytest.raises(ValueError, match='Expected each sample to have shape [1, 28, 28]'):\n        model(torch.randn(1,1,28,29))\n</code></pre> </li> <li> <p>A test is only as good as the error message it gives, and by default, <code>assert</code> will only report that the     check failed. However, we can help ourselves and others by adding strings after <code>assert</code> like</p> <pre><code>assert len(train_dataset) == N_train, \"Dataset did not have the correct number of samples\"\n</code></pre> <p>Add such comments to the assert statements you just wrote in the previous exercises.</p> </li> <li> <p>The tests that involve checking anything that has to do with our data will of course fail     if the data is not present. To future-proof our code, we can take advantage of the     <code>pytest.mark.skipif</code> decorator. Use this decorator to skip your data tests if the corresponding     data files do not exist. It should look something like this:</p> <pre><code>import os.path\n@pytest.mark.skipif(not os.path.exists(file_path), reason=\"Data files not found\")\ndef test_something_about_data():\n    ...\n</code></pre> <p>You can read more about skipping tests here</p> </li> </ol> </li> <li> <p>After writing the different tests, make sure that they are passing locally.</p> </li> <li> <p>We often want to check a function/module for various input arguments. In this case, you could write the same test     over and over again for different inputs, but <code>pytest</code> also has built-in support for this with the use of the     pytest.mark.parametrize decorator. Implement a parametrized     test and make sure that it runs for different inputs.</p> Solution <pre><code>@pytest.mark.parametrize(\"batch_size\", [32, 64])\ndef test_model(batch_size: int) -&gt; None:\n    model = MyModel()\n    x = torch.randn(batch_size, 1, 28, 28)\n    y = model(x)\n    assert y.shape == (batch_size, 10)\n</code></pre> </li> <li> <p>There is no way of measuring how good the test you have written is. However, what we can measure is the     code coverage. Code coverage refers to the percentage of your codebase that gets run when all your     tests are executed. Having a high coverage at least means that all your code will run when executed.</p> <ol> <li> <p>Install coverage.</p> <pre><code>pip install coverage\n</code></pre> </li> <li> <p>Instead of running your tests directly with <code>pytest</code>, now do:</p> <pre><code>coverage run -m pytest tests/\n</code></pre> </li> <li> <p>To get a simple coverage report simply type</p> <pre><code>coverage report\n</code></pre> <p>which will give you the percentage of cover in each of your files. You can also write</p> <pre><code>coverage report -m\n</code></pre> <p>to get the exact lines that were missed by your tests.</p> </li> <li> <p>Finally, try to increase the coverage by writing a new test that runs some of the lines in your codebase that     are not covered yet.</p> </li> <li> <p>Often <code>coverage</code> reports the code coverage on files that we do not want to get code coverage for, for example     your test file. Figure out how to configure <code>coverage</code> to exclude some files.</p> Solution <p>You need to set the <code>omit</code> option. This can either be done when running <code>coverage run</code> or <code>coverage report</code> such as:</p> <pre><code>coverage run --omit=\"tests/*\" -m pytest tests/\n# or\ncoverage report --omit=\"tests/*\"\n</code></pre> <p>As an alternative you can specify this in your <code>pyproject.toml</code> file:</p> <pre><code>[tool.coverage.run]\nomit = [\"tests/*\"]\n</code></pre> </li> </ol> </li> </ol>"},{"location":"s5_continuous_integration/unittesting/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>Assuming you have a code coverage of 100%, would you expect that no bugs are present in your code?</p> Solution <p>No, code coverage is not a guarantee that your code is bug-free. It is just a measure of how many lines of code are run when your tests are executed. Therefore, there may still be some corner case that is not covered by your tests and will result in a bug. However, having a high code coverage is a good indicator that you have tested your code.</p> </li> <li> <p>Consider the following code:</p> <pre><code>@pytest.mark.parametrize(\"network_size\", [10, 100, 1000])\n@pytest.mark.parametrize(\"device\", [\"cpu\", \"cuda\"])\nclass MyTestClass:\n    @pytest.mark.parametrize(\"network_type\", [\"alexnet\", \"squeezenet\", \"vgg\", \"resnet\"])\n    @pytest.mark.parametrize(\"precision\", [torch.half, torch.float, torch.double])\n    def test_network1(self, network_size, device, network_type, precision):\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"Test requires cuda\")\n        model = MyModelClass(network_size, network_type).to(device=device, dtype=precision)\n        ...\n\n    @pytest.mark.parametrize(\"add_dropout\", [True, False])\n    def test_network2(self, network_size, device, add_dropout):\n        if device == \"cuda\" and not torch.cuda.is_available():\n            pytest.skip(\"Test requires cuda\")\n        model = MyModelClass2(network_size, add_dropout).to(device)\n        ...\n</code></pre> <p>How many tests are executed when running the above code?</p> Solution <p>The answer depends on whether or not we are running on a GPU-enabled machine. The <code>test_network1</code> has 4 parameters, <code>network_size, device, network_type, precision</code>, that respectively can take on <code>3, 2, 4, 3</code> values, meaning that in total that test will run <code>3x2x4x3=72</code> times with different parameters on a GPU-enabled machine and 36 on a machine without a GPU. A similar calculation can be done for <code>test_network2</code>, which only has three factors <code>network_size, device, add_dropout</code> that result in <code>3x2x2=12</code> tests on a GPU-enabled machine and 6 on a machine without a GPU. In total, that means 84 tests would run on a machine with a GPU and 42 on a machine without a GPU.</p> </li> </ol> <p>That covers the basics of writing unit tests for Python code. We want to note that <code>pytest</code> of course is not the only framework for doing this. Python has a built-in framework called unittest for doing this also (but <code>pytest</code> offers slightly more features). Another open-source framework that you could choose to check out is hypothesis, which can help catch errors in corner cases of your code. In addition to writing unit tests it is also highly recommended to test code that you include in your docstrings belonging to your functions and modules to make sure that any code that is in your documentation is also correct. For such testing, we highly recommend using the built-in Python framework doctest.</p>"},{"location":"s6_the_cloud/","title":"Cloud computing","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn how to get started with Google Cloud Platform and how to interact with the SDK.</p> <p> M20: Cloud Setup</p> </li> <li> <p></p> <p>Learn how to use different GCP services to support your machine learning pipeline.</p> <p> M21: Cloud Services</p> </li> </ul> <p>Running computations locally is often sufficient when only playing around with code in the initial phase of development. However, to scale your experiments you will need more computing power than what your standard laptop/desktop can offer. You probably already have experience with running on a local cluster or similar but today's topic is about utilizing cloud computing.</p> <p></p>  Image credit  <p>There exist numerous amount of cloud computing providers with some of the biggest being:</p> <ul> <li>Azure</li> <li>AWS</li> <li>Google Cloud Platform (GCP)</li> <li>Alibaba Cloud</li> </ul> <p>They all have slight advantages and disadvantages over each other. In this course, we are going to focus on Google Cloud Platform, because they have been kind enough to sponsor $50 of cloud credit for each student. If you happen to run out of credit, you can also get some free credit for a limited amount of time when you sign up with a new account. What's important to note is that all these different cloud providers all have the same set of services and that learning how to use the services of one cloud provider in many cases translates to also knowing how to use the same services on another cloud provider. The services are called something different and can have a bit of a different interface/interaction pattern but in the end, it does not matter.</p> <p>Today's exercises are about getting to know how to work with the cloud. If you are in doubt about anything or want to deep dive into some topics, I recommend watching this series of videos or going through the general docs.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>In general being familiar with the Google SDK</li> <li>Being able to start different compute instances and work with them</li> <li>Know how to implement continuous integration workflows for the building of docker images</li> <li>Knowledge about how to store data and containers/artifacts in cloud buckets</li> <li>Being able to train simple deep-learning models using a combination of cloud services</li> </ul>"},{"location":"s6_the_cloud/cloud_setup/","title":"Cloud setup","text":"<p>Core Module</p> <p>Google Cloud Platform (GCP) is the cloud service provided by Google. The key concept, or selling point, of any cloud provider, is the idea of near-infinite resources. Without the cloud, it is simply not feasible to do many modern deep learning and machine learning tasks because they cannot be scaled locally.</p> <p>The image below shows all the different services that the Google Cloud platform offers. We are going to be working with around 10 of these services throughout the course. Therefore, if you finish the exercises early I highly recommend that you deep dive more into the Google Cloud Platform.</p> <p></p>  Image credit"},{"location":"s6_the_cloud/cloud_setup/#exercises","title":"\u2754 Exercises","text":"<p>As the first step, we are going to get you some Google Cloud credits.</p> <ol> <li> <p>Go to https://learn.inside.dtu.dk. Go to this course. Find the recent message where there should be a download     link and instructions on how to claim the $50 cloud credit. Please do not share the link anywhere as there is a     limited amount of coupons. If you are not officially taking this course at DTU, Google gives $300 cloud credits     whenever you sign up with a new account. NOTE that you need to provide a credit card for this so make     sure to closely monitor your credit usage so you do not end up spending more than the free credit.</p> </li> <li> <p>Log in to the homepage of GCP. It should look like this:</p> <p> </p> </li> <li> <p>Go to billing and make sure that your account is showing $50 of cloud credit.</p> <p> </p> <p>Make sure to also check out the <code>Reports</code> throughout the course. When you are starting to use some of the cloud services these tabs will update with info about how much time you have before your cloud credit runs out. Make sure that you monitor this page as you will not be given another coupon.</p> </li> <li> <p>One way to stay organized within GCP is to create projects.</p> <p> </p> <p>Create a new project called <code>dtumlops</code>. When you click <code>create</code> you should get a notification that the project is being created. The notification bell is a good way to make sure how the processes you are running are doing throughout the course.</p> </li> <li> <p>Next is local setup on your laptop. We are going to install <code>gcloud</code>, which is part of the Google Cloud SDK.     <code>gcloud</code> is the command line interface for working with our Google Cloud account. Nearly everything that we can do     through the web interface we can also do through the <code>gcloud</code> interface. Follow the installation instructions     here for your specific OS.</p> <ol> <li> <p>After installation, try in a terminal to type:</p> <pre><code>gcloud -h\n</code></pre> <p>The command should show the help page. If not, something went wrong in the installation (you may need to restart after installing).</p> </li> <li> <p>Now log in by typing</p> <pre><code>gcloud auth login #(1)!\n</code></pre> <ol> <li>If you are authenticating through WSL you most likely need to add the argument <code>--no-launch-browser</code> at the     end of the command to get the authentication link. Copy the link and paste it into your browser.</li> </ol> <p>You should be sent to a web page where you link your cloud account to the <code>gcloud</code> interface. Afterward, also run this command:</p> <pre><code>gcloud auth application-default login\n</code></pre> <p>If you at some point want to revoke the authentication you can type:</p> <pre><code>gcloud auth revoke\n</code></pre> </li> <li> <p>Next, you will need to set the project that we just created as the default project0. In your web browser under     project info, you should be able to see the <code>Project ID</code> belonging to your <code>dtumlops</code> project. Copy this and     type the following command in a terminal:</p> <pre><code>gcloud config set project &lt;project-id&gt;\n</code></pre> <p>You can also get the project info by running</p> <pre><code>gcloud projects list\n</code></pre> </li> <li> <p>Next, install the Google Cloud Python API:</p> <pre><code>pip install --upgrade google-api-python-client\n</code></pre> <p>Make sure that the Python interface is also installed. In a Python terminal type</p> <pre><code>import googleapiclient\n</code></pre> <p>which should work without any errors.</p> </li> <li> <p>(Optional) If you are using VSCode you can also download the relevant     extension     called <code>Cloud Code</code>. After installing it you should see a small <code>Cloud Code</code> button in the action bar.</p> </li> </ol> </li> <li> <p>Finally, we need to activate a couple of     developer APIs that are not activated     by default. In a terminal write</p> <pre><code>gcloud services enable apigateway.googleapis.com\ngcloud services enable servicemanagement.googleapis.com\ngcloud services enable servicecontrol.googleapis.com\n</code></pre> <p>You can always check which services are enabled by typing</p> <pre><code>gcloud services list\n</code></pre> </li> </ol> <p>After following these steps your laptop should hopefully be setup for using GCP locally. You are now ready to use their services, both locally on your laptop and in the cloud console.</p>"},{"location":"s6_the_cloud/cloud_setup/#iam-and-quotas","title":"IAM and Quotas","text":"<p>A big part of using the cloud in a bigger organization has to do with Admin and quotas. Admin here in general refers to the different roles that users of GCP can have and quotas refers to the amount of resources that a given user has access to. For example, one employee, let's say a data scientist, may only be granted access to certain GCP services that have to do with the development and training of machine learning models, with <code>X</code> amount of GPUs available to use to make sure that the employee does not spend too much money. Another employee, a DevOps engineer, probably does not need access to the same services and not necessarily the same resources.</p> <p>In this course, we are not going to focus too much on this aspect but it is important to know that it exists. One feature you are going to need for doing the project is how to share a project with other people. This is done through the IAM (Identities and Access Management) page. Simply click the <code>Grant Access</code> button, search for the email of the person you want to share the project with and give them either <code>Viewer</code>, <code>Editor</code> or <code>Owner</code> access, depending on what you want them to be able to do. The figure below shows how to do this.</p> <p></p> <p>What we are going to go through right now is how to increase the quotas for how many GPUs you have available for your project. By default, for any free accounts in GCP (or accounts using teaching credits) the default quota for GPUs that you can use is either 0 or 1 (their policies sometimes change). We will in the exercises below try to increase it.</p>"},{"location":"s6_the_cloud/cloud_setup/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by enabling the <code>Compute Engine</code> service. Simply search for it in the top search bar. It should bring you     to a page where you can enable the service (may take some time). We are going to look more into this service     in the next module.</p> </li> <li> <p>Next go to the <code>IAM &amp; Admin</code> page, again search for it in the top search bar. The remaining steps are illustrated     in the figure below.</p> <ol> <li> <p>Go to the <code>quotas page</code>.</p> </li> <li> <p>In the search field search for <code>GPUs (all regions)</code> (needs to match exactly, the search field is case-sensitive),     such that you get the same quota as in the image.</p> </li> <li> <p>In the limit, you can see what your current quota for the number of GPUs you can use is. Additionally, to the     right of the limit, you can see the current usage. It is worth checking if you are ever in doubt if a job     is running on GPU or not.</p> </li> <li> <p>Click the quota and afterward the <code>Edit</code> quotas button.</p> </li> <li> <p>In the pop-up window, increase your limit to either 1 or 2.</p> </li> <li> <p>After sending your request you can try clicking the <code>Increase requests</code> tab to see the status of your request.</p> <p> </p> </li> </ol> </li> </ol> <p>If you are ever running into errors when working in GPU that contain statements about <code>quotas</code> you can always try to go to this page and see what you are allowed to use currently and try to increase it. For example, when you get to training machine learning models using Vertex AI in the next module, you would most likely need to ask for a quota increase for that service as well.</p> <p>Note</p> <p>You can only request a quota increase for a service that you have enabled. After enabling a given service it may take 5-10 minutes before you can request a quota increase for that service.</p> <p></p> <p>Finally, we want to note that a quota increase is sometimes not allowed within 24 hours of creating an account. If your request gets rejected, we recommend waiting a day and trying again. If this does still not work, you may need to use their services some more to show you are not a bot that wants to mine crypto.</p>"},{"location":"s6_the_cloud/cloud_setup/#service-accounts","title":"Service accounts","text":"<p>At some point, you will most likely need to use a service account. A service account is a virtual account that is used to interact with the Google Cloud API. It it intended for non-human users, e.g. other machines, services, etc. For example, if you want to launch a training job from GitHub Actions, you will need to use a service account for authentication between GitHub and GCP. You can read more about how to create a service account here.</p>"},{"location":"s6_the_cloud/cloud_setup/#exercises_2","title":"\u2754 Exercises","text":"<ol> <li> <p>Go to the <code>IAM &amp; Admin</code> page and click on <code>Service accounts</code>. Alternatively, you can search for it in the top search     bar.</p> </li> <li> <p>Click the <code>Create Service Account</code> button. On the next page, you can give the service account a name and id     (automatically generated, but you can change it if you want). You can also give it a description. Leave the rest as     default and click <code>Create</code>.</p> </li> <li> <p>Next, let's give the service account some permissions. Your job now is to give the service account the lowest     possible permissions such that it can download files from a bucket. Copy the email of the service account and go to     the <code>IAM</code> page. Click on the <code>Grant Access</code> button and paste in the email address in the <code>Add principals</code> field.     Then click the <code>Select a role</code> dropdown and either write the name of the role you want to give the service account     or search for it in the list. Finally, click <code>Save</code>.  For help, you can look at this     page and try to find the role that fits the description.</p> Solution <p>The role you are looking for is <code>Storage Object Viewer</code>. This role allows the service account to list objects in a bucket and download objects, but nothing more. Thus, even if someone gets access to the service account they cannot delete objects in the bucket.</p> </li> <li> <p>To use the service account later we need to create a key for it. Click on the service account and then the <code>Keys</code>     tab. Click <code>Add key</code> and then <code>Create new key</code>. Choose the <code>JSON</code> key type and click <code>Create</code>. This will download     a JSON file to your computer. This file is the key to the service account and should be kept secret. If you lose     it you can always create a new one.</p> </li> <li> <p>Finally, everything we just did from creating the service account, giving it permissions, and creating a key can     also be done through the <code>gcloud</code> interface. Try to find the commands to do this in the     documentation.</p> Solution <p>The commands you are looking for are:</p> <pre><code>gcloud iam service-accounts create my-sa \\\n    --description=\"My first service account\" --display-name=\"my-sa\"\ngcloud projects add-iam-policy-binding $(GCP_PROJECT_NAME) \\\n    --member=\"serviceAccount:global-service-account@iam.gserviceaccount.com\" \\\n    --role=\"roles/storage.objectViewer\"\ngcloud iam service-accounts keys create service_account_key.json \\\n    --iam-account=global-service-account@$(GCP_PROJECT_NAME).iam.gserviceaccount.com\n</code></pre> <p>where <code>$(GCP_PROJECT_NAME)</code> is the name of your project. If you then want to delete the service account you can run</p> <pre><code>gcloud iam service-accounts delete global-service-account@$(GCP_PROJECT_NAME).iam.gserviceaccount.com\n</code></pre> </li> </ol> <p>In this course we recommend that you only use a single service account for doing exercises and your project. This is to simplify the process of managing permissions. In a real-world scenario, you would most likely have multiple service accounts, each with different permissions. The following roles are the most common ones that you will use in this course:</p> <ul> <li><code>Storage Object Viewer</code>: Allows the service account to list objects in a bucket and download objects</li> <li><code>Cloud Build Builder</code>: Allows the service account to run builds in Cloud Build</li> <li><code>Secret Manager Secret Accessor</code>: Allows the service account to access secrets in Secret Manager</li> <li><code>Cloud Run Developer</code>: Allows the service account to deploy services in Cloud Run</li> <li><code>AI Platform Developer</code>: Allows the service account to use the AI Platform</li> <li><code>Artifact Registry Writer</code>: Allows the service account to write to the Artifact Registry</li> </ul>"},{"location":"s6_the_cloud/cloud_setup/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>What considerations should you have when choosing a GCP region for running a new application?</p> Solution <p>A series of factors may influence your choice of region, including:</p> <ul> <li>Services availability in the region; not all services are available in all regions</li> <li>Resource availability: some regions have more     GPUs available than others</li> <li>Reduced latency: if your application is running in the same region as your users, the latency will be lower</li> <li>Compliance: some countries have strict rules that require user info to be stored inside a particular region     e.g., EU has GDPR rules that require all user data to be stored in the EU</li> <li>Pricing: some regions may have different pricing than others</li> </ul> </li> <li> <p>The three major cloud providers all have the same services, but they are called something different depending on the     provider. What are the corresponding names of these GCP services in AWS and Azure?</p> <ul> <li>Compute Engine</li> <li>Cloud storage</li> <li>Cloud functions</li> <li>Cloud run</li> <li>Cloud build</li> <li>Vertex AI</li> </ul> <p>It is important to know these correspondences to navigate blog posts, etc. about MLOps on the internet.</p> Solution GCP AWS Azure Compute Engine Elastic Compute Cloud (EC2) Virtual Machines Cloud storage Simple Storage Service (S3) Blob Storage Cloud functions Lambda Functions Serverless Compute Cloud run App Runner, Fargate, Lambda Container Apps, Container Instances Cloud build CodeBuild DevOps Vertex AI SageMaker AI Platform </li> <li> <p>Why is it always important to assign the lowest possible permissions to a service account?</p> Solution <p>The reason is that if someone gets access to the service account they can only do what the service account is allowed to do. If the service account has permission to delete objects in a bucket, the attacker can delete all the objects in the bucket. For this reason, in most cases multiple service accounts are used, each with different permissions. This setup is called the principle of least privilege.</p> </li> </ol>"},{"location":"s6_the_cloud/using_the_cloud/","title":"M21 - Using the Cloud","text":""},{"location":"s6_the_cloud/using_the_cloud/#using-the-cloud","title":"Using the cloud","text":"<p>Core Module</p> <p>In this set of exercises, we are going to get more familiar with using some of the resources that GCP offers.</p>"},{"location":"s6_the_cloud/using_the_cloud/#compute","title":"Compute","text":"<p>The most basic service of any cloud provider is the ability to create and run virtual machines. In GCP this service is called Compute Engine API. A virtual machine allows you to essentially run an operating system that behaves like a completely separate computer. There are many reasons for using virtual machines:</p> <ul> <li> <p>Virtual machines allow you to scale your operations, essentially giving you access to infinitely many individual     computers.</p> </li> <li> <p>Virtual machines allow you to use large-scale hardware. For example, if you are developing a deep learning model on     your laptop and want to know the inference time for a specific hardware configuration, you can just create a virtual     machine with those specs and run your model.</p> </li> <li> <p>Virtual machines allow you to run processes in the \"background\". If you want to train a model for a week or more, you     do not want to do this on your laptop as you cannot move it or do anything with it while it is training.     Virtual machines allow you to just launch a job and forget about it (at least until you run out of credit).</p> </li> </ul> <p></p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises","title":"\u2754 Exercises","text":"<p>We are now going to start using the cloud.</p> <ol> <li> <p>Click on the <code>Compute Engine</code> tab in the sidebar on the homepage of GCP.</p> </li> <li> <p>Click the <code>Create Instance</code> button. You will see the following image below.</p> <p> </p> <p>Give the virtual machine a meaningful name, and set the location to some location that is closer to where you are (to reduce latency, we recommend <code>europe-west-1</code>). Finally, try to adjust the configuration a bit. Can you find at least two settings that alter the price of the virtual machine?</p> Solution <p>In general, the price of a virtual machine is determined by the class of hardware attached to it. Higher class CPUs and GPUs mean higher prices. Additionally, the amount of memory and disk space also affects the price. Finally, the location of the virtual machine also affects the price.</p> </li> <li> <p>After figuring this out, create a <code>e2-medium</code> instance (leave the rest configured as default). Before clicking the     <code>Create</code> button make sure to check the <code>Equivalent code</code> button. You should see a very long command that you     could have typed in the terminal that would create a VM similar to configuring it through the UI.</p> </li> <li> <p>After creating the virtual machine, in a local terminal type:</p> <pre><code>gcloud compute instances list\n</code></pre> <p>You should hopefully see the instance you have just created.</p> </li> <li> <p>You can start a terminal directly by typing:</p> <pre><code>gcloud compute ssh --zone &lt;zone&gt; &lt;name&gt; --project &lt;project-id&gt;\n</code></pre> <p>You can always see the exact command that you need to run to <code>ssh</code> into a VM by selecting the <code>View gcloud command</code> option in the Compute Engine overview (see image below).</p> <p> </p> </li> <li> <p>While logged into the instance, check if Python and PyTorch are installed.     You should see that neither is installed. The VM we have only specified what     compute resources it should have, and not what software should be in it. We     can fix this by starting VMs based on specific docker images (it's all coming together).</p> <ol> <li> <p>GCP comes with several ready-to-go images for doing deep learning.     More info can be found here.     Try, running this line:</p> <pre><code>gcloud container images list --repository=\"gcr.io/deeplearning-platform-release\"\n</code></pre> <p>What does the output show?</p> Solution <p>The output should show a list of images that are available for you to use. The images are essentially docker images that contain a specific software stack. The software stack is often a specific version of Python, PyTorch, TensorFlow, etc. The images are maintained by Google and are updated regularly.</p> </li> <li> <p>Next, start (in the terminal) a new instance using a PyTorch image. The command for doing it should look     something like this:</p> <pre><code>gcloud compute instances create &lt;instance_name&gt; \\\n    --zone=&lt;zone&gt; \\\n    --image-family=&lt;image-family&gt; \\\n    --image-project=deeplearning-platform-release \\\n    # add these arguments if you want to run on GPU and have the quota to do so\n    --accelerator=\"type=nvidia-tesla-V100,count=1\" \\\n    --maintenance-policy TERMINATE \\\n    --metadata=\"install-nvidia-driver=True\" \\\n</code></pre> <p>You can find more info here on what <code>&lt;image-family&gt;</code> should be and what extra argument you need to add if you want to run on GPU (if you have access).</p> Solution <p>The command should look something like this:</p> CPUGPU <pre><code>gcloud compute instances create my-instance \\\n    --zone=europe-west1-b \\\n    --image-family=pytorch-latest-cpu \\\n    --image-project=deeplearning-platform-release\n</code></pre> <pre><code>gcloud compute instances create my-instance \\\n    --zone=europe-west1-b \\\n    --image-family=pytorch-latest-gpu \\\n    --image-project=deeplearning-platform-release \\\n    --accelerator=\"type=nvidia-tesla-V100,count=1\" \\\n    --maintenance-policy TERMINATE\n</code></pre> </li> <li> <p><code>ssh</code> into the VM as in one of the previous exercises. Confirm that the container indeed contains     both a Python installation and PyTorch is also installed. Hint: you also have the possibility     through the web page to start a browser session directly to the VMs you create:</p> <p> </p> </li> </ol> </li> <li> <p>Everything that you have done locally can also be achieved through the web terminal, which of course comes     pre-installed with the <code>gcloud</code> command etc.</p> <p> </p> <p>Try launching this and run some of the commands from the previous exercises.</p> </li> <li> <p>Finally, we want to make sure that we do not forget to stop our VMs. VMs are charged by the minute, so even if you     are not using them you are still paying for them. Therefore, you must remember to stop your VMs when you are not     using them. You can do this by either clicking the <code>Stop</code> button on the VM overview page or by running the following     command:</p> <pre><code>gcloud compute instances stop &lt;instance-name&gt;\n</code></pre> </li> </ol>"},{"location":"s6_the_cloud/using_the_cloud/#data-storage","title":"Data storage","text":"<p>Another big part of cloud computing is the storage of data. There are many reasons that you want to store your data in the cloud including:</p> <ul> <li>Easily being able to share</li> <li>Easily expand as you need more</li> <li>Data is stored in multiple locations, making sure that it is not lost in case of an emergency</li> </ul> <p>Cloud storage is luckily also very cheap. Google Cloud only takes around $0.026 per GB per month. This means that around 1 TB of data would cost you $26 which is more than what the same amount of data would cost on Google Drive, but the storage in Google Cloud is much more focused on enterprise usage such that you can access the data through code.</p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises_1","title":"\u2754 Exercises","text":"<p>When we did the exercise on data version control, we made <code>dvc</code> work together with our own Google Drive to store data. However, a big limitation of this is that we need to authenticate each time we try to either push or pull the data. The reason is that we need to use an API instead which is offered through GCP.</p> <p>We are going to follow the instructions from this page</p> <ol> <li> <p>Let's start by creating data storage. On the GCP start page, in the sidebar, click on <code>Cloud Storage</code>.     On the next page click <code>Create bucket</code>:</p> <p> </p> <p>Give the bucket a unique name, set it to a region close by and importantly remember to enable Object versioning under the last tab. Finally, click `Create``.</p> </li> <li> <p>After creating the storage, you should be able to see it online and you should be able to see it if you type in your     local terminal:</p> <pre><code>gsutil ls\n</code></pre> <p>gsutil is a command line tool that allows you to create, upload, download, list, move, rename and delete objects in cloud storage. For example, you can upload a file to cloud storage by running:</p> <pre><code>gsutil cp &lt;file&gt; gs://&lt;bucket-name&gt;\n</code></pre> </li> <li> <p>Next, we need the Google storage extension for <code>dvc</code>.</p> <pre><code>pip install dvc-gs\n</code></pre> </li> <li> <p>Now in your corrupt MNIST repository where you have already configured <code>dvc</code>, we are going to change the storage     from our Google Drive to our newly created Google Cloud storage.</p> <pre><code>dvc remote add -d remote_storage &lt;output-from-gsutils&gt;\n</code></pre> <p>In addition, we are also going to modify the remote to support object versioning (called <code>version_aware</code> in <code>dvc</code>):</p> <pre><code>dvc remote modify remote_storage version_aware true\n</code></pre> <p>This will change the default way that <code>dvc</code> handles data. Instead of just storing the latest version of the data as content-addressable storage, it will now store the data as it looks in our local repository, which allows us to not only use <code>dvc</code> to download our data.</p> </li> <li> <p>The above command will change the <code>.dvc/config</code> file. <code>git add</code> and <code>git commit</code> the changes to that file.     Finally, push data to the cloud.</p> <pre><code>dvc push --no-run-cache  # (1)!\n</code></pre> <ol> <li> The <code>--no-run-cache</code> flag is used to avoid pushing the cache file to the cloud, which is not     supported by the Google Cloud storage.</li> </ol> </li> <li> <p>Finally, make sure that you can pull without having to give your credentials. The easiest way to see this     is to delete the <code>.dvc/cache</code> folder that should be on your laptop and afterward do a</p> <pre><code>dvc pull --no-run-cache\n</code></pre> </li> </ol> <p>This setup should work when trying to access the data from your laptop, which we authenticated in the previous module. However, how can you access the data from a virtual machine, inside a docker container or from a different laptop? We in general recommend two ways:</p> <ul> <li> <p>You can make the bucket publicly accessible, i.e. no authentication is needed. That means that anyone with the URL     to the data can access it. This is the easiest way to do it, but also the least secure. You can read more about     how to make your buckets public here.</p> </li> <li> <p>You can use the service account that you created in the previous module to authenticate the VM. This is the most     secure way to do it, but also the most complicated. You first need to give the service account the correct     permissions. Then you need to authenticate using the service account. In <code>dvc</code> this is done by setting the     environment variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> to the path of</p> Linux/MacOSWindows <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/credentials.json\"\n</code></pre> <pre><code>set GOOGLE_APPLICATION_CREDENTIALS=\"C:\\path\\to\\your\\credentials.json\"\n</code></pre> </li> </ul>"},{"location":"s6_the_cloud/using_the_cloud/#artifact-registry","title":"Artifact registry","text":"<p>You should hopefully at this point have seen the strength of using containers to create reproducible environments. They allow us to specify exactly the software that we want to run inside our VMs. However, you should already have run into two problems with containers</p> <ul> <li>The building process can take a lot of time</li> <li>Docker images can be large</li> </ul> <p>For this reason, we want to move both the building process and the storage of images to the cloud. In GCP the two services that we are going to use for this are called Cloud Build for building the containers in the cloud and Artifact registry for storing the images afterward.</p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises_2","title":"\u2754 Exercises","text":"<p>In these exercises, I recommend that you start with a dummy version of some code to make sure that the building process does not take too long. Below is a simple Python script that does image classification using Sklearn, together with the corresponding <code>requirements.txt</code> file and <code>Dockerfile</code>.</p> Python script main.py<pre><code>from sklearn import datasets, metrics, svm\nfrom sklearn.model_selection import train_test_split\n\nif __name__ == \"__main__\":\n    digits = datasets.load_digits()\n\n    # flatten the images\n    n_samples = len(digits.images)\n    data = digits.images.reshape((n_samples, -1))\n\n    # Create a classifier: a support vector classifier\n    clf = svm.SVC(gamma=0.001)\n\n    # Split data into 50% train and 50% test subsets\n    X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, shuffle=False)\n\n    # Learn the digits on the train subset\n    clf.fit(X_train, y_train)\n\n    # Predict the value of the digit on the test subset\n    predicted = clf.predict(X_test)\n\n    print(f\"Classification report for classifier {clf}:\\n{metrics.classification_report(y_test, predicted)}\\n\")\n</code></pre> requirements.txt requirements.txt<pre><code>scikit-learn&gt;=1.0\n</code></pre> Dockerfile Dockerfile<pre><code>FROM python:3.11-slim\n\n# install python\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt requirements.txt\nCOPY main.py main.py\nWORKDIR /\nRUN pip install -r requirements.txt --no-cache-dir\n\nENTRYPOINT [\"python\", \"-u\", \"main.py\"]\n</code></pre> <p>The docker images for this application are therefore going to be substantially faster to build and smaller in size than the images we are used to that use PyTorch.</p> <ol> <li> <p>Start by enabling the services <code>Google Artifact Registry API</code> and <code>Google Cloud Build API</code>. This can be     done through the website (by searching for the services) or can also be enabled from the terminal:</p> <pre><code>gcloud services enable artifactregistry.googleapis.com\ngcloud services enable cloudbuild.googleapis.com\n</code></pre> </li> <li> <p>The first step is creating an artifact repository in the cloud. You can either do this through the UI or by using     <code>gcloud</code> in the command line.</p> UICommand line <p>Find the <code>Artifact Registry</code> service (search for it in the search bar) and click on it. From there click on the <code>Create repository</code> button. You should see the following page:</p> <p> </p> <p>Give the repository a name, make sure to set the format to <code>Docker</code> and specify the region. At the bottom of the page you can optionally add a cleanup policy. We recommend that you add one to keep costs down. Give the policy a name choose the <code>Keep most recent versions</code> option and set the keep count to <code>5</code>. Click <code>Create</code> and you should now see the repository in the list of repositories.</p> <pre><code>gcloud artifacts repositories create &lt;registry-name&gt; \\\n    --repository-format=docker \\\n    --location=europe-west1 \\\n    --description=\"My docker registry\"\n</code></pre> <p>where you need to replace <code>&lt;registry-name&gt;</code> with a name of your choice. You can read more about the command here. We recommend that after creating the repository you update it with a cleanup policy to keep costs down. You can do this by running:</p> <pre><code>gcloud artifacts repositories set-cleanup-policies REPOSITORY\n    --project=&lt;project-id&gt;\n    --location=&lt;region&gt;\n    --policy=policy.yaml\n</code></pre> <p>where the <code>policy.yaml</code> file should look something like this:</p> <p><pre><code>[\n    {\n        \"name\": \"keep-minimum-versions\",\n        \"action\": {\"type\": \"Keep\"},\n        \"mostRecentVersions\": {\n            \"keepCount\": 5\n        }\n    }\n]\n</code></pre> and you can read more about the command here.</p> <p>Whenever we in the future want to push or pull to this artifact repository we can refer to it using this URL:</p> <pre><code>&lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;\n</code></pre> <p>for example, <code>europe-west1-docker.pkg.dev/dtumlops-335110/container-registry</code> would be a valid URL (this is the one I created).</p> </li> <li> <p>We are now ready to build our containers in the cloud. In principle, GCP cloud build works out of the box with docker     files. However, the recommended way is to add specialized <code>cloudbuild.yaml</code> files. You can think of the     <code>cloudbuild.yaml</code> file as the corresponding file in GCP as workflow files are in GitHub actions, which you learned     about in module M16. It is essentially a file that specifies     a list of steps that should be executed to do something, but the syntax is different.</p> <p>Look at the documentation on how to write a <code>cloudbuild.yaml</code> file for building and pushing a docker image to the artifact registry. Try to implement such a file in your repository.</p> Solution <p>For building docker images the syntax is as follows:</p> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Build container image'\n  args: [\n    'build',\n    '.',\n    '-t',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/&lt;image-name&gt;',\n    '-f',\n    '&lt;path-to-dockerfile&gt;'\n  ]\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Push container image'\n  args: [\n    'push',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/&lt;image-name&gt;'\n  ]\noptions:\n  logging: CLOUD_LOGGING_ONLY\n</code></pre> <p>where you need to replace <code>&lt;registry-name&gt;</code>, <code>&lt;image-name&gt;</code> and <code>&lt;path-to-dockerfile&gt;</code> with your own values. You can hopefully recognize the syntax from the docker exercises. In this example, we are calling the <code>cloud-builders/docker</code> service with both the <code>build</code> and <code>push</code> arguments.</p> </li> <li> <p>You can now try to trigger the <code>cloudbuild.yaml</code> file from your local machine. What <code>gcloud</code> command would you use     to do this?</p> Solution <p>You can trigger a build by running the following command:</p> <pre><code>gcloud builds submit . --config=cloudbuild.yaml\n</code></pre> <p>This command will submit a build to the cloud build service using the configuration file <code>cloudbuild.yaml</code> in the current directory. The <code>.</code> specifies that the build context is the current directory. The build context is all the files that are uploaded to the cloud build service, thus if your Dockerfile has <code>COPY</code> commands that copy files from the local directory to the container, these files need to be in the build context. Do note that by default all files in your <code>.gitignore</code> file are excluded from the build context, but you can override this by using the <code>--ignore-file</code> flag, which you can read more about here.</p> </li> <li> <p>Instead of relying on manually submitting builds, we can setup the build process as continuous integration such     that it is triggered every time we push code to the repository. This is done by setting up a     trigger in the GCP console. From the GCP homepage, navigate to the     triggers panel:</p> <p> </p> <p>Click on manage repositories.</p> <ol> <li> <p>From there, click <code>Connect Repository</code> and go through the steps of authenticating your GitHub profile with     GCP and choose the repository that you want to set up build triggers for. For now, skip the     <code>Create a trigger (optional)</code> part by pressing <code>Done</code> at the end.</p> <p> </p> </li> <li> <p>Navigate back to the <code>Triggers</code> homepage and click <code>Create trigger</code>. Set the following:</p> <ul> <li>Give a name</li> <li>Event: choose <code>Push to branch</code></li> <li>Source: choose the repository you just connected</li> <li>Branch: choose <code>^main$</code></li> <li>Configuration: choose either <code>Autodetected</code> or <code>Cloud build configuration file</code></li> </ul> <p>Finally, click the <code>Create</code> button and the trigger should show up on the triggers page.</p> </li> <li> <p>To activate the trigger, push some code to the chosen repository.</p> </li> <li> <p>Go to the <code>Cloud Build</code> page and you should see the image being built and pushed.</p> <p> </p> <p>Try clicking on the build to check out the build process and build summary. As you can see from the image, if a build is failing you will often find valuable info by looking at the build summary. If your build is failing try to configure it to run in one of these regions: <code>us-central1, us-west2, europe-west1, asia-east1, australia-southeast1, southamerica-east1</code> as specified in the documentation.</p> </li> <li> <p>If/when your build is successful, navigate to the <code>Artifact Registry</code> page. You should hopefully find that the     image you just built was pushed here. Congrats!</p> </li> </ol> </li> <li> <p>Make sure that you can pull your image down to your laptop</p> <pre><code>docker pull &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/&lt;image-name&gt;:&lt;image-tag&gt;\n</code></pre> <p>you will need to authenticate <code>docker</code> with GCP first. Instructions can be found here, but the following command should hopefully be enough to make <code>docker</code> and GCP talk to each other:</p> <pre><code>gcloud auth configure-docker &lt;region&gt;-docker.pkg.dev\n</code></pre> <p>where you need to replace <code>&lt;region&gt;</code> with the region you are using. Do note you need to have <code>docker</code> actively running in the background, just like any other time you want to use <code>docker</code>.</p> </li> <li> <p>Automatization through the cloud is in general the way to go, but sometimes you may want to manually create images     and push them to the registry. Figure out how to push an image to your <code>Artifact Registry</code>. For simplicity, you can     just push the <code>busybox</code> image you downloaded during the initial docker exercises. This     page should help you with the exercise.</p> Solution <p>Pushing to a repository is similar to pulling. Assuming that you have already built an image called <code>busybox</code> you can push it to the repository by running:</p> <pre><code>docker tag busybox &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/busybox:latest\ndocker push &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/busybox:latest\n</code></pre> <p>where you need to replace <code>&lt;region&gt;</code>, <code>&lt;project-id&gt;</code> and <code>&lt;registry-name&gt;</code> with your own values.</p> </li> <li> <p>(Optional) Instead of using the built-in trigger in GCP, another way to activate the build-on code changes is to     integrate with GitHub Actions. This has the benefit that we can make the build process depend on other steps in the     pipeline. For example, in the image below we have conditioned the build to only run if tests are passing on     all operating systems. Lets try to implement this.</p> <p> </p> <ol> <li> <p>Start by adding a new secret to GitHub with the name <code>GCLOUD_SERVICE_KEY</code> and the value of the service account     key that you created in the previous module. This is needed to authenticate the GitHub action with GCP.</p> </li> <li> <p>We assume that you already have a workflow file that runs some unit tests:</p> <pre><code>name: Unit tests &amp; build\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n</code></pre> <p>We now want to add a job that triggers the build process in GCP. How can you make the <code>build</code> job depend on the <code>test</code> job? Hint: Relevant documentation.</p> Solution <p>You can make the <code>build</code> job depend on the <code>test</code> job by adding the <code>needs</code> keyword to the <code>build</code> job:</p> <pre><code>name: Unit tests &amp; build\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    ...\n  build:\n    needs: test\n    ...\n</code></pre> </li> <li> <p>Additionally, we probably only want to build the image if the job is running on our main branch, i.e. not part     of a pull request. How can you make the <code>build</code> job only run on the main branch?</p> Solution <pre><code>name: Unit tests &amp; build\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    ...\n  build:\n    needs: test\n    if: ${{ github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main' }}\n    ...\n</code></pre> </li> <li> <p>Finally, we need to add the steps to submit the build job to GCP. You need four steps:</p> <ul> <li>Check out the code</li> <li>Authenticate with GCP</li> <li>Set up gcloud</li> <li>Submit the build</li> </ul> <p>How can you do this? Hint: For the first two steps these two GitHub actions can be useful: auth and setup-gcloud.</p> Solution <pre><code>name: Unit tests &amp; build\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    ...\n  build:\n    needs: test\n    if: ${{ github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main' }}\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Auth with GCP\n      uses: google-github-actions/auth@v2\n      with:\n        credentials_json: ${{ secrets.GCLOUD_SERVICE_KEY }}\n\n    - name: Set up Cloud SDK\n      uses: google-github-actions/setup-gcloud@v2\n\n    - name: Submit build\n      run: gcloud builds submit . --config cloudbuild_containers.yaml\n</code></pre> </li> </ol> </li> <li> <p>(Optional) The <code>cloudbuild</code> specification format allows you to specify so-called     substitutions. A substitution     is simply a way to replace a variable in the <code>cloudbuild.yaml</code> file with a value that is known only at runtime. This     can be useful for using the same <code>cloudbuild.yaml</code> file for multiple builds. Try to implement a substitution in your     docker cloud build file such that the image name is a variable.</p> <p>Build in substitutions</p> <p>You have probably already encountered substitutions like <code>$PROJECT_ID</code> in the <code>cloudbuild.yaml</code> file. These are substitutions that are automatically replaced by GCP. Other commonly used are <code>$BUILD_ID</code>, <code>$PROJECT_NUMBER</code> and <code>$LOCATION</code>. You can find a full list of built-in substitutions here.</p> Solution <p>We just need to add the <code>substitutions</code> field to the <code>cloudbuild.yaml</code> file. For example, if we want to replace the image name with a variable called <code>_IMAGE_NAME</code> we can do the following:</p> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Build container image'\n  args: [\n    'build',\n    '.',\n    '-t',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/$_IMAGE_NAME',\n    '-f',\n    '&lt;path-to-dockerfile&gt;'\n  ]\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Push container image'\n  args: [\n    'push',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/$_IMAGE_NAME'\n  ]\noptions:\n  logging: CLOUD_LOGGING_ONLY\nsubstitutions:\n  _IMAGE_NAME: 'my_image'\n</code></pre> <p>Do note that user substitutions are prefixed with an underscore <code>_</code> to distinguish them from built-in ones. You can read more here.</p> <ol> <li> <p>How would you provide the value for the <code>_IMAGE_NAME</code> variable to the <code>gcloud builds submit</code> command?</p> Solution <p>You can provide the value for the <code>_IMAGE_NAME</code> variable by adding the <code>--substitutions</code> flag to the <code>gcloud builds submit</code> command:</p> <pre><code>gcloud builds submit . --config=cloudbuild.yaml --substitutions=_IMAGE_NAME=my_image\n</code></pre> <p>If you want to provide more than one substitution you can do so by separating them with a comma.</p> </li> </ol> </li> </ol>"},{"location":"s6_the_cloud/using_the_cloud/#training","title":"Training","text":"<p>As the final step in our journey through different GCP services in this module, we are going to look at the training of our models. This is one of the important tasks that GCP can help us with because we can always rent more hardware as long as we have credits, meaning that we can both scale horizontally (run more experiments) and vertically (run longer experiments).</p> <p>We are going to check out two ways of running our experiments. First, we are going to return to the Compute Engine service because it gives the most simple form of scaling of experiments. That is: we create a VM with an appropriate docker image, start it, log into the VM and run our experiments. Most people can run a couple of experiments in parallel this way. However, what if there was an abstract layer that automatically created a VM for us, launched our experiments and then closed the VM afterwards?</p> <p>This is where the Vertex AI service comes into play. This is a dedicated service for handling ML models in GCP in the cloud. Vertex AI is in principal an end-to-end service that can take care of everything machine learning-related in the cloud. In this course, we are primarily focused on just the training of our models, and then use other services for other parts of our pipeline.</p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises_3","title":"\u2754 Exercises","text":"<ol> <li> <p>Let's start by going through how we could train a model using PyTorch using the Compute Engine service:</p> <ol> <li> <p>Start by creating an appropriate VM. If you want to start a VM that has PyTorch pre-installed with only CPU     support you can run the following command:</p> <pre><code>gcloud compute instances create &lt;instance-name&gt; \\\n    --zone europe-west1-b \\\n    --image-family=pytorch-latest-cpu \\\n    --image-project=deeplearning-platform-release\n</code></pre> <p>Alternatively, if you have access to GPU in your GCP account you could start a VM in the following way:</p> <pre><code>gcloud compute instances create &lt;instance-name&gt; \\\n    --zone europe-west4-a \\\n    --image-family=pytorch-latest-gpu \\\n    --image-project=deeplearning-platform-release \\\n    --accelerator=\"type=nvidia-tesla-t4,count=1\" \\\n    --metadata=\"install-nvidia-driver=True\" \\\n    --maintenance-policy TERMINATE\n</code></pre> </li> <li> <p>Next log into your newly created VM. You can either open an <code>ssh</code> terminal in the cloud console or run the     following command:</p> <pre><code>gcloud compute ssh &lt;instance-name&gt;\n</code></pre> </li> <li> <p>It is recommended to always check that the VM we get is actually what we asked for. In this case, the VM should     have PyTorch pre-installed so let's check for that by running</p> <pre><code>python -c \"import torch; print(torch.__version__)\"\n</code></pre> <p>Additionally, if you have a VM with GPU support also try running the <code>nvidia-smi</code> command.</p> </li> <li> <p>When you have logged in to the VM, it works as your machine. Therefore to run some training code you would     need to do the same setup step you have done on your machine: clone your GitHub, install dependencies,     download data, and run code. Try doing this to make sure you can train a model.</p> </li> </ol> </li> <li> <p>The above exercises should hopefully have convinced you that it can be hard to scale experiments using the Compute     Engine service. The reason is that you need to manually start, set up and stop a separate VM for each experiment.     Instead, let's try to use the Vertex AI service to train our models.</p> <ol> <li> <p>Start by enabling it by searching for <code>Vertex AI</code> in the cloud console by going to the service or by running the     following command:</p> <pre><code>gcloud services enable aiplatform.googleapis.com\n</code></pre> </li> <li> <p>The way we are going to use Vertex AI is to create custom jobs because we have already developed docker     containers that contain everything to run our code. Thus the only command that we need to use is the     <code>gcloud ai custom-jobs create</code> command. An example here would be:</p> <pre><code>gcloud ai custom-jobs create \\\n    --region=europe-west1 \\\n    --display-name=test-run \\\n    --config=config.yaml \\\n    # these are the arguments that are passed to the container, only needed if you want to change defaults\n    --command 'python src/my_project/train.py' \\\n    --args=--epochs=10 --args=--batch-size=128\n</code></pre> <p>Essentially, this command combines everything into one command: it first creates a VM with the specs specified by a configuration file, then loads a container specified again in the configuration file, and finally it runs everything. An example of a config file could be:</p> CPUGPU <pre><code># config_cpu.yaml\nworkerPoolSpecs:\n    machineSpec:\n        machineType: n1-highmem-2\n    replicaCount: 1\n    containerSpec:\n        imageUri: &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/&lt;image-name&gt;:&lt;image-tag&gt;\n</code></pre> <pre><code># config_gpu.yaml\nworkerPoolSpecs:\n    machineSpec:\n        machineType: n1-standard-8\n        acceleratorType: NVIDIA_TESLA_T4\n        acceleratorCount: 1\n    replicaCount: 1\n    containerSpec:\n        imageUri: &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/&lt;image-name&gt;:&lt;image-tag&gt;\n</code></pre> <p>In this case we are requesting an Nvidia Tesla T4 GPU. This will only work if you have a quota for allocating this type of GPU in the Vertex AI service. To check your quota, go into quotas and search for <code>custom_model_training_nvidia_t4</code> in the <code>Filter</code> field to see the quota. If not, you can try to request a quota increase. Remember that it is not enough to just request a quota for the GPU; the request needs to be approved by Google before you can use it.</p> <p>You can read more about the configuration formatting here and the different types of machines here. Try to execute a job using the <code>gcloud ai custom-jobs create</code> command. For additional documentation you can look at the documentation on the command and this page and this page.</p> </li> <li> <p>Assuming you manage to launch a job, you should see an output like this:</p> <p> </p> <p>Try executing the commands that are outputted to look at both the status and the progress of your job.</p> </li> <li> <p>In addition, you can also visit the <code>Custom Jobs</code> tab in the <code>training</code> part of Vertex AI.</p> <p> </p> <p>You will need to select the specific region that you submitted your job to in order to see the job.</p> </li> <li> <p>During custom training, we do not necessarily need to use <code>dvc</code> for downloading our data. A more efficient way is     to use cloud storage as a     mounted file system.     This allows us to access data directly from the cloud storage without having to download it first. All our     training jobs are automatically mounted to a <code>gcs</code> folder in the root directory. Try to access the data from your     training script:</p> <pre><code># loading from a bucket using mounted file system\ndata = torch.load('/gcs/&lt;my-bucket-name&gt;/data.pt')\n# writing to a bucket using mounted file system\ntorch.save(data, '/gcs/&lt;my-bucket-name&gt;/data.pt')\n</code></pre> <p>should speed up the training process a bit.</p> </li> <li> <p>Your code may depend on environment variables for authenticating, for example with weights and biases during     training. These can also be specified in the configuration file. How would you do this?</p> Solution <p>You can specify environment variables in the configuration file by adding the <code>env</code> field to the <code>containerSpec</code> field. For example, if you want to specify the <code>WANDB_API_KEY</code> you can do it like this:</p> <pre><code>workerPoolSpecs:\n    machineSpec:\n        machineType: n1-highmem-2\n    replicaCount: 1\n    containerSpec:\n        imageUri: &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/&lt;image-name&gt;:&lt;image-tag&gt;\n        env:\n        - name: WANDB_API_KEY\n          value: &lt;your-wandb-api-key&gt;\n</code></pre> <p>You need to replace <code>&lt;your-wandb-api-key&gt;</code> with your actual key. Also, remember that this file now contains a secret and should be treated as such.</p> </li> <li> <p>Try to execute multiple jobs with different configurations, e.g., change the <code>--args</code> field in the <code>gcloud ai     custom-jobs create</code> command at the same time. This should hopefully show you how easy it is to scale experiments     using the Vertex AI service.</p> </li> </ol> </li> </ol>"},{"location":"s6_the_cloud/using_the_cloud/#secrets-management","title":"Secrets management","text":"<p>Similar to GitHub Actions, GCP also has secrets storage that can be used to keep secrets safe. This is called the Secret Manager in GCP. By using the Secret Manager, we have the option of injecting secrets into our code without having to store them in the code itself.</p>"},{"location":"s6_the_cloud/using_the_cloud/#exercises_4","title":"\u2754 Exercises","text":"<ol> <li> <p>Let's look at the example from before where we have a config file like this for custom Vertex AI jobs:</p> <pre><code>workerPoolSpecs:\n    machineSpec:\n        machineType: n1-highmem-2\n    replicaCount: 1\n    containerSpec:\n        imageUri: gcr.io/&lt;project-id&gt;/&lt;docker-img&gt;\n        env:\n        - name: WANDB_API_KEY\n          value: $WANDB_API_KEY\n</code></pre> <p>We do not want to store the <code>WANDB_API_KEY</code> in the config file, rather we would like to store it in the Secret Manager and inject it right before the job starts. Let's figure out how to do that.</p> <ol> <li> <p>Start by enabling the secrets manager API by running the following command:</p> <pre><code>gcloud services enable secretmanager.googleapis.com\n</code></pre> </li> <li> <p>Next, go to the secrets manager in the cloud console and create a new secret. You just need to give it a name, a     value and leave the rest as default. Add one or more secrets like in the image below.</p> <p> </p> </li> <li> <p>We are going to inject the secrets into our training job by using cloudbuild. Create a new cloudbuild file called     <code>vertex_ai_train.yaml</code> and add the following content:</p> vertex_ai_train.yaml<pre><code>steps:\n- name: \"alpine\"\n  id: \"Replace values in the training config\"\n  entrypoint: \"sh\"\n  args:\n    - '-c'\n    - |\n      apk add --no-cache gettext\n      envsubst &lt; config.yaml &gt; config.yaml.tmp\n      mv config.yaml.tmp config.yaml\n  secretEnv: ['WANDB_API_KEY']\n\n- name: 'alpine'\n  id: \"Show config\"\n  waitFor: ['Replace values in the training config']\n  entrypoint: \"sh\"\n  args:\n    - '-c'\n    - |\n      cat config.yaml\n\n- name: 'gcr.io/cloud-builders/gcloud'\n  id: 'Train on vertex AI'\n  waitFor: ['Replace values in the training config']\n  args: [\n    'ai',\n    'custom-jobs',\n    'create',\n    '--region',\n    'europe-west1',\n    '--display-name',\n    'example-mlops-job',\n    '--config',\n    '${_VERTEX_TRAIN_CONFIG}',\n  ]\nsubstitutions:\n  _VERTEX_TRAIN_CONFIG: 'config.yaml'\navailableSecrets:\n  secretManager:\n  - versionName: projects/$PROJECT_ID/secrets/WANDB_API_KEY/versions/latest\n    env: 'WANDB_API_KEY'\n</code></pre> <p>Slowly go through the file and try to understand what each step does.</p> Solution <p>There are two parts to using secrets in cloud build. First, there is the <code>availableSecrets</code> field that specifies what secrets from the Secret Manager should be injected into the build. In this case, we are injecting the <code>WANDB_API_KEY</code> and setting it as an environment variable. The second part is the <code>secretEnv</code> field in the first step. This field specifies which secrets should be available in the first step. The steps are then doing:</p> <ol> <li> <p>The first step calls the envsubst command which is a     general Linux command that replaces environment variables in a file. In this case, it replaces the     <code>$WANDB_API_KEY</code> with the actual value of the secret. We then save the file as <code>config.yaml.tmp</code> and     rename it back to <code>config.yaml</code>.</p> </li> <li> <p>The second step is just to show that the replacement was successful. This is mostly for debugging     purposes and can be removed.</p> </li> <li> <p>The third step is the actual training job. It waits for the first step to finish before running.</p> </li> </ol> </li> <li> <p>Finally, try to trigger the build</p> <pre><code>gcloud builds submit . --config=vertex_ai_train.yaml\n</code></pre> <p>and check that the <code>WANDB_API_KEY</code> is correctly injected into the <code>config.yaml</code> file.</p> </li> </ol> </li> </ol>"},{"location":"s6_the_cloud/using_the_cloud/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>In the Compute Engine, we have the option to either stop or suspend the VMs, can you describe what the difference is?</p> Solution <p>Suspended instances preserve the guest OS memory, device state, and application state. You will not be charged for a suspended VM but will be charged for the storage of the aforementioned states. Stopped instances do not preserve any of the states and you will be charged for the storage of the disk. However, in both cases if the VM instances have resources attached to them, such as static IPs and persistent disks, they are charged until they are deleted.</p> </li> <li> <p>As seen in the exercises, a <code>cloudbuild.yaml</code> file often contains multiple steps. How would you make steps dependent     on each other e.g. one step can only run if another step has finished? And how would you make steps execute     concurrently?</p> Solution <p>In both cases, the solution is the <code>waitFor</code> field. If you want a step to wait for another step to finish you you need to give the first step an <code>id</code> and then specify that <code>id</code> in the <code>waitFor</code> field of the second step.</p> <pre><code>steps:\n- name: 'alpine'\n  id: 'step1'\n  entrypoint: 'sh'\n  args: ['-c', 'echo \"Hello World\"']\n- name: 'alpine'\n  id: 'step2'\n  entrypoint: 'sh'\n  args: ['-c', 'echo \"Hello World 2\"']\n  waitFor: ['step1']\n</code></pre> <p>If you want steps to run concurrently you can set the <code>waitFor</code> field to <code>['-']</code>:</p> <pre><code>steps:\n- name: 'alpine'\n  id: 'step1'\n  entrypoint: 'sh'\n  args: ['-c', 'echo \"Hello World\"']\n- name: 'alpine'\n  id: 'step2'\n  entrypoint: 'sh'\n  args: ['-c', 'echo \"Hello World 2\"']\n  waitFor: ['-']\n</code></pre> </li> </ol> <p>This ends the session on how to use Google Cloud services for now. In a future session, we are going to investigate some more of the services offered in GCP, in particular for deploying the models that we have just trained.</p>"},{"location":"s7_deployment/","title":"Model deployment","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn how to use requests and how to create custom APIs</p> <p> M22: Requests and APIs</p> </li> <li> <p></p> <p>Learn how to deploy custom APIs using serverless functions and serverless containers in the cloud</p> <p> M23: Cloud Deployment</p> </li> <li> <p></p> <p>Learn how to test APIs for functionality and load</p> <p> M24: API testing</p> </li> <li> <p></p> <p>Learn about different ways to improve the deployment of machine learning models</p> <p> M25: ML Deployment</p> </li> <li> <p></p> <p>Learn how to create a frontend for your application using Streamlit</p> <p> M26: Frontend</p> </li> </ul> <p>Let's say that you have spent 1000 GPU hours and trained the most awesome model that you want to share with the world. One way to do this is, of course, to just place all your code in a GitHub repository, upload a file with the trained model weights to your favorite online storage (assuming it is too big for GitHub to handle) and ask people to just download your code and the weights to run the code by themselves. This is a fine approach in a small research setting, but in production, you need to be able to deploy the model to an environment that is fully contained such that people can just execute without looking (too hard) at the code.</p> <p> </p>  Image credit  <p>In this session we try to look at methods specialized towards deployment of models on your local machine and also how to deploy services in the cloud.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the basics of requests and APIs</li> <li>Be able to create custom APIs using the framework <code>fastapi</code> and run it locally</li> <li>Knowledge about serverless deployments and how to deploy custom APIs using both serverless functions and     serverless containers</li> <li>Can create basic continuouss deployment pipelines for your models</li> <li>Understand the basics of frontend development and how to create a frontend for your application using Streamlit</li> <li>Know how to use more advanced frameworks like onnx and bentoml to deploy your machine learning models</li> </ul>"},{"location":"s7_deployment/apis/","title":"M22 - Requests and APIs","text":""},{"location":"s7_deployment/apis/#requests-and-apis","title":"Requests and APIs","text":"<p>Core Module</p> <p>Before we can deploy our models we need to understand concepts such as APIs and requests. The core reason for this is that we need a new abstraction layer on top of our applications that are not Python-specific. While Python is the defacto language for machine learning, we cannot expect everybody else to use it and in particular, we cannot expect network protocols (both local and external) to be able to communicate with our Python programs out of the box. For this reason, we need to understand requests, in particular HTTP requests and how to create APIs that can interact with those requests.</p>"},{"location":"s7_deployment/apis/#requests","title":"Requests","text":"<p>When we talk about requests, we are essentially talking about the communication method used in client-server types of architectures. As shown in the image below, in this architecture, the client (user) is going to send requests to a server (our machine learning application) and the server will give a response. For example, the user may send a request to get the class of a specific image, which our application will do and then send back the response in terms of a label.</p> <p></p>  Image credit  <p>The common way of sending requests is called HTTP (Hypertext Transfer Protocol). It is essentially a specification of the intermediary transportation method between the client and server. An HTTP request essentially consists of two parts:</p> <ul> <li>A request URL: the location of the server we want to send our request to</li> <li>A request method: describing what action we want to perform on the server</li> </ul> <p>The common request methods are (case sensitive):</p> <ul> <li>GET: get data from the server</li> <li>POST/PUT: send data to the server</li> <li>DELETE: delete data on the server</li> </ul> <p>You can read more about the different methods here. For most machine learning applications, GET and POST are the core methods to remember. Additionally, if you want to read more about HTTP in general we highly recommend that you go over this comic strip protocol, but the TLDR is that it provides privacy, integrity and identification over the web.</p>"},{"location":"s7_deployment/apis/#exercises","title":"\u2754 Exercises","text":"<p>We are going to do a couple of exercises on sending requests using the requests package to get familiar with the syntax.</p> <ol> <li> <p>Start by installing the `requests`` package.</p> <pre><code>pip install requests\n</code></pre> </li> <li> <p>Afterwards, create a small script and try to execute the code:</p> <pre><code>import requests\nresponse = requests.get('https://api.github.com/this-api-should-not-exist')\nprint(response.status_code)\n</code></pre> <p>As you can see from the syntax, we are sending a request using the GET method. This code should return status code 404 (1). Take a look at this page that contains a list of status codes. Next, let's call a page that exists.</p> <ol> <li> If you do not get a status code of 404, it can either be due to some firewall settings or the     fact that many other students are doing the same exercise at the same time and we are essentially     DDOS'ing GitHub. It does not matter if you get a     different status code.</li> </ol> <pre><code>import requests\nresponse = requests.get('https://api.github.com')\nprint(response.status_code)\n</code></pre> <p>What is the status code now and what does it mean? Status codes are important when you have an application that is interacting with a server and want to make sure that it does not fail, which can be done with simple <code>if</code> statements on the status codes.</p> <pre><code>if response.status_code == 200:\n    print('Success!')\nelif response.status_code == 404:\n    print('Not Found.')\n</code></pre> </li> <li> <p>Next, try to call the following</p> <pre><code>response=requests.get(\"https://api.github.com/repos/SkafteNicki/dtu_mlops\")\n</code></pre> <p>which gives back a payload. Essentially, payload refers to any additional data that is sent from the client to the server or vice-versa. Try looking at the <code>response.content</code> attribute. What is the type of this attribute?</p> </li> <li> <p>You should hopefully observe that the <code>.content</code> attribute is of type <code>bytes</code>. It is important to note that     the standard way of sending payloads to encode them into <code>byte</code> objects. To get a more human-readable version of     the response, we can convert it to JSON format.</p> <pre><code>response.json()\n</code></pre> <p>It is important to remember that a JSON object in Python is just a nested dictionary if you ever want to iterate over the object in some way.</p> </li> <li> <p>When we use the GET method we can additionally provide a <code>params</code> argument that specifies what we want the server     to send back for a specific request URL:</p> <pre><code>response = requests.get(\n    'https://api.github.com/search/repositories',\n    params={'q': 'requests+language:python'},\n)\n</code></pre> <p>Before looking at <code>response.json()</code> can you explain what the code does? You can try looking at this page for help.</p> </li> <li> <p>Sometimes the content of a page cannot be converted into JSON, because as already stated data is sent as bytes.     Say that we want to download an image, which we can do in the following way:</p> <pre><code>import requests\nresponse = requests.get('https://imgs.xkcd.com/comics/making_progress.png')\n</code></pre> <p>Try calling <code>response.json()</code>, what happens? Next, try calling <code>response.content</code>. To get the result in this case we would need to convert from bytes to an image:</p> <pre><code>with open(r'img.png','wb') as f:\n    f.write(response.content)\n</code></pre> </li> <li> <p>The <code>get</code> method is the most useful method because it allows us to get data from the server. However, as stated in     the beginning, multiple request methods exist, for example, the POST method for sending data to the server. Try     executing:</p> <pre><code>pload = {'username':'Olivia','password':'123'}\nresponse = requests.post('https://httpbin.org/post', data = pload)\n</code></pre> <p>Investigate the response (this is an artificial example because we do not control the server).</p> </li> <li> <p>Finally, we should also know that requests can be sent directly from the command line using the <code>curl</code> command.     Sometimes it is easier to send a request directly from the terminal and sometimes it is easier to do it from a     script.</p> <ol> <li> <p>Make sure you have <code>curl</code> installed, or else find instructions for installing it. To check, call <code>curl --help</code> with     the documentation on curl.</p> </li> <li> <p>To execute <code>requests.get('https://api.github.com')</code> using curl we would simply do</p> <pre><code>curl -X GET \"https://api.github.com\"\ncurl -X GET -I \"https://api.github.com\" # if you want the status code\n</code></pre> <p>Try it yourself.</p> </li> <li> <p>Try to redo some of the exercises yourself using <code>curl</code>.</p> </li> </ol> </li> </ol> <p>That ends the intro session on <code>requests</code>. Do not worry if you are still not completely comfortable with sending requests, we are going to return to how we do it in practice when we have created our API. If you want to learn more about the <code>requests</code> package you can check out this tutorial and if you want to see more examples of how to use <code>curl</code> you can check out this page.</p>"},{"location":"s7_deployment/apis/#creating-apis","title":"Creating APIs","text":"<p>Requests are all about being on the client side of our client-server architecture. We are now going to move on to the server side where we will be learning about writing the APIs that requests can interact with. An application programming interface (API) is essentially the way for the developer (you) to tell a user how to use the application that you have created. The API is an abstraction layer that allows the user to interact with our application in the way we want them to interact with it, without the user ever having to look at the code.</p> <p>We can take the API from GitHub as an example https://api.github.com. This API allows any user to retrieve, integrate and send data to GitHub without ever having to visit their webpage. The API exposes multiple endpoints that have various functions:</p> <ul> <li>https://api.github.com/repos/OWNER/REPO/branches: check out the branches on a given repository</li> <li>https://api.github.com/search/code: search through GitHub for repositories</li> <li>https://api.github.com/repos/OWNER/REPO/actions/workflows: check the status of workflows for a given repository</li> </ul> <p>and we could go on. However, there may be functionality that GitHub is not interested in users having access to and they may therefore choose not to have endpoints for specific features (1).</p> <ol> <li> Many companies provide public APIs to interact with their services/data. For a general list of     public APIs you can check out this page. For the Danes out there, you     can check out this list of public and private APIs from Danish companies and     organizations.</li> </ol> <p>The particular kind of API we are going to work with is called REST API (or RESTful API). The REST API specifies specific constraints that a particular API needs to fulfill to be considered RESTful. You can read more about the six guiding principles behind REST API on this page, but one of the most important to have in mind is that the client-server architecture needs to be stateless. This means that whenever a request is sent to the server it needs to be self-contained (all information included) and the server cannot rely on any previously stored information from previous requests.</p> <p>To implement APIs in practice we are going to use FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. FastAPI is only one of many frameworks for defining APIs. However, compared to other frameworks such as Flask and django it offers a sweet spot of being flexible enough to do what you want without having many additional (unnecessary) features.</p>"},{"location":"s7_deployment/apis/#exercises_1","title":"\u2754 Exercises","text":"<p>The exercises below are a condensed version of this and this tutorial. If you ever need context for the exercises, we recommend trying to go through these. Additionally, we also provide this solution file that you can look through for help.</p> <ol> <li> <p>Install FastAPI.</p> <pre><code>pip install fastapi\n</code></pre> <p>This contains the functions, modules, and variables we are going to need to define our interface.</p> </li> <li> <p>Additionally, also install <code>uvicorn</code> which is a package for defining low level server applications.</p> <pre><code>pip install uvicorn[standard]\n</code></pre> </li> <li> <p>Start by defining a small application like this in a file called <code>main.py</code>:</p> <pre><code>from fastapi import FastAPI\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    return {\"item_id\": item_id}\n</code></pre> <p>Important here is the use of the <code>@app.get</code> decorator. What could this decorator refer to? Explain what the two functions are probably doing.</p> </li> <li> <p>Next lets launch our app. Since we called our script <code>main.py</code> and we inside the script initialized our API with     <code>app = FastAPI</code>, our application that we want to deploy can be referenced by <code>main:app</code>:</p> <pre><code>uvicorn --reload --port 8000 main:app\n</code></pre> <p>This will launch a server on this page: <code>http://localhost:8000/</code>. As you will hopefully see, this page will return the content of the <code>root</code> function, like the image below. Remember to also check the output in your terminal as that will give info on when and how your application is being invoked.</p> <p> </p> <ol> <li> <p>What webpage should you open to get the server to return <code>1</code>?</p> </li> <li> <p>Also checkout the pages: <code>http://localhost:8000/docs</code> and <code>http://localhost:8000/redoc</code>. What do     these pages show?</p> </li> <li> <p>The power of the <code>docs</code> and <code>redoc</code> pages is that they allow you to easily test your application with their     simple UI. As shown in the image below, simply open the endpoint you want to test, click the <code>Try it out</code>     button, input any values and execute it. It will return both the corresponding <code>curl</code> command for invoking     your endpoint, the corresponding URL and response of your application. Try it out.</p> <p> </p> </li> <li> <p>You can also checkout <code>http://localhost:8000/openapi.json</code> to check out the schema that is generated     which is essentially a <code>json</code> file containing the overall specifications of your program.</p> </li> <li> <p>Try to access <code>http://localhost:8000/items/foo</code>, what happens in this case? When you specify types in your API,     FastAPI will automatically do type validation using pydantic, making sure users     can only access your API with the correct types. Therefore, remember to include types in your applications!</p> </li> </ol> </li> <li> <p>With the fundamentals in place let's configure it a bit more:</p> <ol> <li> <p>Lets start by changing the root function to include a bit more info. In particular we are also interested in     returning the status code so the end user can easily read that. Default status codes are included in the     http built-in Python package:</p> <pre><code>from http import HTTPStatus\n\n@app.get(\"/\")\ndef root():\n    \"\"\" Health check.\"\"\"\n    response = {\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n    }\n    return response\n</code></pre> <p>Try to reload the app and see what is returned now. You should not have to re-launch the app because we initialized the app with the <code>--reload</code> argument.</p> </li> <li> <p>When we decorate our functions with <code>@app.get(\"/items/{item_id}\")</code>, <code>item_id</code> is what we call a     path parameter because it is a parameter that is directly included in the path of our endpoint. We have     already seen how we can restrict a path to a single type, but what if we want to restrict it to specific values?     This is often the case if we are working with parameters of type <code>str</code>. In this case we would need to define an     <code>enum</code>:</p> <pre><code>from enum import Enum\nclass ItemEnum(Enum):\n    alexnet = \"alexnet\"\n    resnet = \"resnet\"\n    lenet = \"lenet\"\n\n@app.get(\"/restric_items/{item_id}\")\ndef read_item(item_id: ItemEnum):\n    return {\"item_id\": item_id}\n</code></pre> <p>Add this API, reload and execute both a valid parameter and an invalid parameter.</p> </li> <li> <p>In addition to path parameters we have query parameters. In the requests exercises we saw an example of this     where we were calling https://api.github.com/search/code with the query <code>'q': 'requests+language:python'</code>.     Any parameter in FastAPI that is not a path parameter will be considered a query parameter:</p> <pre><code>@app.get(\"/query_items\")\ndef read_item(item_id: int):\n    return {\"item_id\": item_id}\n</code></pre> <p>Add this API, reload and figure out how to pass in a query parameter.</p> </li> <li> <p>We have until now worked with the <code>.get</code> method, but let's also see an example of the <code>.post</code> method. As already     described the POST request method is used for uploading data to the server. Here is a simple app that saves the     username and password in a database (please never implement this in real life like this):</p> <pre><code>database = {'username': [ ], 'password': [ ]}\n\n@app.post(\"/login/\")\ndef login(username: str, password: str):\n    username_db = database['username']\n    password_db = database['password']\n    if username not in username_db and password not in password_db:\n        with open('database.csv', \"a\") as file:\n            file.write(f\"{username}, {password} \\n\")\n        username_db.append(username)\n        password_db.append(password)\n    return \"login saved\"\n</code></pre> <p>Make sure you understand what the function does and then try to execute it a couple of times to see your database updating. It is important to note that we sometimes in the following exercises use the <code>.get</code> method and sometimes the <code>.post</code> method. For our usage it does not really matter.</p> </li> </ol> </li> <li> <p>We are now moving on to figuring out how to provide different standard inputs like text, images, json to our APIs. It     is important that you try out each example yourself and in particular look at the <code>curl</code> commands that are     necessary to invoke each application.</p> <ol> <li> <p>Here is a small application that takes a single text input.</p> <pre><code>@app.get(\"/text_model/\")\ndef contains_email(data: str):\n    regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    response = {\n        \"input\": data,\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n        \"is_email\": re.fullmatch(regex, data) is not None\n    }\n    return response\n</code></pre> <p>What does the application do? Try it out yourself.</p> </li> <li> <p>Let's say we wanted to extend the application to check for a specific email domain, either <code>gmail</code> or <code>hotmail</code>.     Assume that we want to feed this into our application as a <code>json</code> object, e.g.</p> <pre><code>{\n    \"email\": \"mlops@gmail.com\",\n    \"domain_match\": \"gmail\"\n}\n</code></pre> <p>Figure out how to alter the <code>data</code> parameter such that it takes in the <code>json</code> object and make sure to extend the application to check if the email and domain also match. Hint: take a look at this page</p> </li> <li> <p>Let's move on to an application that requires a file input:</p> <pre><code>from fastapi import UploadFile, File\nfrom typing import Optional\n\n@app.post(\"/cv_model/\")\nasync def cv_model(data: UploadFile = File(...)):\n    with open('image.jpg', 'wb') as image:\n        content = await data.read()\n        image.write(content)\n        image.close()\n\n    response = {\n        \"input\": data,\n        \"message\": HTTPStatus.OK.phrase,\n        \"status-code\": HTTPStatus.OK,\n    }\n    return response\n</code></pre> <p>A couple of new things are going on here: we use the specialized <code>UploadFile</code> and <code>File</code> bodies in our input definition. Additionally, we added the <code>async</code>/<code>await</code> keywords. Figure out what everything does and try to run the application (you can use any image file you like).</p> </li> <li> <p>The above application actually does not do anything. Let's add opencv     as a package and resize the image. It can be done with the following three lines:</p> <pre><code>import cv2\nimg = cv2.imread(\"image.jpg\")\nres = cv2.resize(img, (h, w))\n</code></pre> <p>Figure out where to add them in the application and additionally add <code>h</code> and <code>w</code> as optional parameters, with a default value of 28. Try running the application where you specify everything and one more time where you leave out <code>h</code> and <code>w</code>.</p> </li> <li> <p>Finally, let's also figure out how to return a file from our application. You will need to add the following     lines:</p> <pre><code>from fastapi.responses import FileResponse\ncv2.imwrite('image_resize.jpg', res)\nFileResponse('image_resize.jpg')\n</code></pre> <p>Figure out where to add them to the code and try running the application one more time to see that you get a file back with the resized image.</p> </li> </ol> </li> <li> <p>A common pattern in most applications is that we want some code to run on startup and some code to run on shutdown.     FastAPI allows us to do this by controlling the lifespan of our application. This is done by implementing the     <code>lifespan</code> function. Look at the documentation for lifespan events     and implement a small application that prints <code>Hello</code> on startup and <code>Goodbye</code> on shutdown.</p> Solution <p>Here is a simple example that will print <code>Hello</code> on startup and <code>Goodbye</code> on shutdown.</p> <pre><code>from contextlib import asynccontextmanager\nfrom fastapi import FastAPI\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    print(\"Hello\")\n    yield\n    print(\"Goodbye\")\n\napp = FastAPI(lifespan=lifespan)\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n</code></pre> </li> <li> <p>Let's try to figure out how to use FastAPI in a machine learning context. Below is a script that downloads     a <code>VisionEncoderDecoder</code> from     huggingface     . The model can be used to create captions for a given image. Thus calling</p> <pre><code>predict_step(['s7_deployment/exercise_files/my_cat.jpg'])\n</code></pre> <p>returns a list of strings like <code>['a cat laying on a couch with a stuffed animal']</code> (try this yourself). Create a FastAPI application that can do inference using this model, i.e., it should take in an image, preferably some optional hyperparameters (like <code>max_length</code>) and should return a string (or list of strings) containing the generated caption.</p> <p>simple ML application</p> <pre><code>from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\nimport torch\nfrom PIL import Image\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ngen_kwargs = {\"max_length\": 16, \"num_beams\": 8, \"num_return_sequences\": 1}\ndef predict_step(image_paths):\n    images = []\n    for image_path in image_paths:\n        i_image = Image.open(image_path)\n        if i_image.mode != \"RGB\":\n            i_image = i_image.convert(mode=\"RGB\")\n\n        images.append(i_image)\n    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n    pixel_values = pixel_values.to(device)\n    output_ids = model.generate(pixel_values, **gen_kwargs)\n    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n    preds = [pred.strip() for pred in preds]\n    return preds\n\nif __name__ == \"__main__\":\n    print(predict_step(['s7_deployment/exercise_files/my_cat.jpg']))\n</code></pre> Solution ml_app.py<pre><code>from contextlib import asynccontextmanager\n\nimport torch\nfrom fastapi import FastAPI, File, UploadFile\nfrom PIL import Image\nfrom transformers import AutoTokenizer, VisionEncoderDecoderModel, ViTFeatureExtractor\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load and clean up model on startup and shutdown.\"\"\"\n    global model, feature_extractor, tokenizer, device, gen_kwargs\n    print(\"Loading model\")\n    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n    feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    gen_kwargs = {\"max_length\": 16, \"num_beams\": 8, \"num_return_sequences\": 1}\n\n    yield\n\n    print(\"Cleaning up\")\n    del model, feature_extractor, tokenizer, device, gen_kwargs\n\n\napp = FastAPI(lifespan=lifespan)\n\n\n@app.post(\"/caption/\")\nasync def caption(data: UploadFile = File(...)):\n    \"\"\"Generate a caption for an image.\"\"\"\n    i_image = Image.open(data.file)\n    if i_image.mode != \"RGB\":\n        i_image = i_image.convert(mode=\"RGB\")\n\n    pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values\n    pixel_values = pixel_values.to(device)\n    output_ids = model.generate(pixel_values, **gen_kwargs)\n    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n    return [pred.strip() for pred in preds]\n</code></pre> </li> <li> <p>As the final step, we want to figure out how to include our FastAPI application in a docker container, as that will help     us when we want to deploy in the cloud because docker as always can take care of the dependencies for our     application. For the following set of exercises you can use any of the previous FastAPI applications as the base     application for the container.</p> <ol> <li> <p>Start by creating a <code>requirement.txt</code> file for your application. You will at least need <code>fastapi</code> and <code>uvicorn</code>     in the file and we always recommend that you are specific about the version you want to use.</p> <pre><code># newest version of fastapi and uvicorn as of time of writing\nfastapi==0.115.6\nuvicorn==0.34.0\n# add anything else you application needs to be able to run\n</code></pre> </li> <li> <p>Next, create a <code>Dockerfile</code> with the following content:</p> <pre><code>FROM python:3.11-slim\nWORKDIR /code\nCOPY ./requirements.txt /code/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\nCOPY ./app /code/app\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n</code></pre> <p>The above assumes that your file structure looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 requirements.txt\n</code></pre> <p>Hopefully, all these steps should look familiar if you already went through module M9, except for maybe the last line. However, this is just the standard way that we have run our FastAPI applications in the last couple of exercises, this time with some extra arguments regarding the ports we allow.</p> </li> <li> <p>Next, build the corresponding docker image.</p> <pre><code>docker build -t my_fastapi_app .\n</code></pre> </li> <li> <p>Finally, run the image such that a container is spun up that runs our application. The important part here is     to remember to specify the <code>-p</code> argument (p for port) which should be the same number as the port we have     specified in the last line of our Dockerfile.</p> <pre><code>docker run --name mycontainer -p 80:80 myimage\n</code></pre> </li> <li> <p>Check that everything is working by going to the corresponding localhost page     http://localhost/items/5?q=somequery.</p> </li> </ol> </li> </ol> <p>This ends the module on APIs. If you want to go further in this direction we highly recommend that you check out bentoml which is an API standard that focuses solely on creating easy-to-understand APIs and services for ml-applications. Additionally, we also highly recommend checking out Postman which can help design, document and in particular test the API you are writing to make sure that it works as expected.</p>"},{"location":"s7_deployment/cloud_deployment/","title":"M23 - Cloud Deployment","text":""},{"location":"s7_deployment/cloud_deployment/#cloud-deployment","title":"Cloud deployment","text":"<p>Core Module</p> <p>We are now returning to using the cloud. In this module, you should have gone through the steps of having your code in your GitHub repository to automatically build into a docker container, store that, store data and pull it all together to make a training run. After the training is completed you should hopefully have a file stored in the cloud with your trained model weights.</p> <p>Today's exercises will be about serving those model weights to an end user. We focus on two different ways of deploying our model: Google cloud functions and Google cloud run. Both services are serverless, meaning that you do not have to manage the server that runs your code.</p> <p></p>  GCP in general has 5 core deployment options. We are going to focus on Cloud Functions and Cloud Run, which are two of the serverless options. In contrast to these two, you have the option to deploy to Kubernetes Engine and Compute Engine which are more traditional ways of deploying your code. Here you have to manage the underlying infrastructure."},{"location":"s7_deployment/cloud_deployment/#cloud-functions","title":"Cloud Functions","text":"<p>Google Cloud Functions is the most simple way that we can deploy our code to the cloud. As stated above, it is a serverless service, meaning that you do not have to worry about the underlying infrastructure. You just write your code and deploy it. The service is great for small applications that can be encapsulated in a single script.</p>"},{"location":"s7_deployment/cloud_deployment/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Go to the start page of <code>Cloud Functions</code>. It can be found in the sidebar on the homepage or you can just search for it.     Activate the service in the cloud console or use the following command:</p> <pre><code>gcloud services enable cloudfunctions.googleapis.com\n</code></pre> </li> <li> <p>Click the <code>Create Function</code> button which should take you to a screen like the image below. Make sure it is a 2nd     Gen function, give it a name, set the server region to somewhere close by and change the authentication policy to     <code>Allow unauthenticated invocations</code> so we can access it directly from a browser. Remember to note down the</p> <p> </p> </li> <li> <p>On the next page, for <code>Runtime</code> pick the <code>Python 3.11</code> option (or newer). This will make the inline editor show both     a <code>main.py</code> and <code>requirements.py</code> file. Look over them and try to understand what they do. Be sure to take a     look at the functions-framework which is a     needed requirement of any Cloud function.</p> <p> </p> <p>After you have looked over the files, click the <code>Deploy</code> button.</p> Solution <p>The <code>functions-framework</code> is a lightweight, open-source framework for turning Python functions into HTTP functions. Any function that you deploy to Cloud Functions must be wrapped in the <code>@functions_framework.http</code> decorator.</p> </li> <li> <p>Afterwards, the function should begin to deploy. When it is done, you should see \u2705. Now let's test it by going to     the <code>Testing</code> tab.</p> <p> </p> </li> <li> <p>If you know what the application does, it should come as no surprise that it can run without any input. We     therefore just send an empty request by clicking the <code>Test The Function</code> button. Does the function return     the output you expected? Wait for the logs to show up. What do they show?</p> <ol> <li> <p>What should the <code>Triggering event</code> look like in the testing prompt for the program to respond with</p> <pre><code>Hallo General Kenobi!\n</code></pre> <p>Try it out.</p> Solution <p>The default triggering event is a JSON object with a key <code>name</code> and a value. Therefore the triggering event should look like this:</p> <pre><code>{\n    \"name\": \"General Kenobi\"\n}\n</code></pre> </li> <li> <p>Go to the trigger tab and go to the URL for the application. Execute the API a couple of times. How can you     change the URL to make the application respond with the same output as above?</p> Solution <p>You can change the URL to include a query parameter <code>name</code> with the value <code>General Kenobi</code>. For example</p> <pre><code>https://us-central1-my-personal-mlops-project.cloudfunctions.net/function-3?name=General%20Kanobi\n</code></pre> <p>where you would need to replace everything before the <code>?</code> with your URL.</p> </li> <li> <p>Click on the metrics tab. You should hopefully see it being populated with a few data points. Identify what each     panel is showing.</p> Solution <ul> <li>Invocations/Second: The number of times the function is invoked per second</li> <li>Execution time (ms): The time it takes for the function to execute in milliseconds</li> <li>Memory usage (MB): The memory usage of the function in MB</li> <li>Instance count (instances): The number of instances that are running the function</li> </ul> </li> <li> <p>Check out the logs tab. You should see that your application has already been invoked multiple times. Also, try     to execute this command in a terminal:</p> <pre><code>gcloud functions logs read\n</code></pre> </li> </ol> </li> <li> <p>Next, we are going to create our own application that takes some input so we can try to send it requests. We provide     a very simple script to get started.</p> <p>Simple script</p> sklearn_cloud_functions.py<pre><code># Load data\nimport pickle\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\n\niris_x, iris_y = datasets.load_iris(return_X_y=True)\n\n# Split iris data in train and test data\n# A random permutation, to split the data randomly\nnp.random.seed(0)\nindices = np.random.permutation(len(iris_x))\niris_x_train = iris_x[indices[:-10]]\niris_y_train = iris_y[indices[:-10]]\niris_x_test = iris_x[indices[-10:]]\niris_y_test = iris_y[indices[-10:]]\n\n# Create and fit a nearest-neighbor classifier\n\nknn = KNeighborsClassifier()\nknn.fit(iris_x_train, iris_y_train)\nknn.predict(iris_x_test)\n\n# save model\n\nwith open(\"model.pkl\", \"wb\") as file:\n    pickle.dump(knn, file)\n</code></pre> <ol> <li> <p>Figure out what the script does and run the script. This should create a file with a trained model.</p> Solution <p>The file trains a simple KNN model on the iris dataset and saves it to a file called <code>model.pkl</code>.</p> </li> <li> <p>Next, create a storage bucket and upload the model file to the bucket. Try to do this using the <code>gsutil</code> command     and check afterward that the file is in the bucket.</p> Solution <pre><code>gsutil mb gs://&lt;bucket-name&gt;  # mb stands for make bucket\ngsutil cp model.pkl gs://&lt;bucket-name&gt;  # cp stands for copy\n</code></pre> <p>where you replace <code>&lt;bucket-name&gt;</code> with the name of your bucket.</p> </li> <li> <p>Create a new cloud function with the same initial settings as the first one, e.g. <code>Python 3.11</code> and <code>HTTP</code>. Then     implement in the <code>main.py</code> file code that:</p> <ul> <li>Loads the model from the bucket</li> <li>Takes a request with a list of integers as input</li> <li>Returns the prediction of the model</li> </ul> <p>In addition to writing the <code>main.py</code> file, you also need to fill out the <code>requirements.txt</code> file. You need at least three packages to run the application. Remember to also change the <code>Entry point</code> to the name of your function. If your deployment fails, try to go to the <code>Logs Explorer</code> page in <code>gcp</code> which can help you identify why.</p> Solution <p>The main script should look something like this:</p> main.py<pre><code>import pickle\n\nimport functions_framework\nfrom google.cloud import storage\n\nBUCKET_NAME = \"my_sklearn_model_bucket\"\nMODEL_FILE = \"model.pkl\"\n\nclient = storage.Client()\nbucket = client.get_bucket(BUCKET_NAME)\nblob = bucket.get_blob(MODEL_FILE)\nmy_model = pickle.loads(blob.download_as_string())\n\n\n@functions_framework.http\ndef knn_classifier(request):\n    \"\"\"Simple knn classifier function for iris prediction.\"\"\"\n    request_json = request.get_json()\n    if request_json and \"input_data\" in request_json:\n        input_data = request_json[\"input_data\"]\n        input_data = [float(in_data) for in_data in input_data]\n        input_data = [input_data]\n        prediction = my_model.predict(input_data)\n        return {\"prediction\": prediction.tolist()}\n    return {\"error\": \"No input data provided.\"}\n</code></pre> <p>And the requirement file should look like this:</p> <pre><code>functions-framework&gt;=3.7.0\ngoogle-cloud-storage&gt;=2.14.0\nscikit-learn&gt;=1.4.0\n</code></pre> <p>Make sure that you are using the same version of <code>scikit-learn</code> as when you trained the model. Otherwise when trying to load the model you will most likely get an error.</p> </li> <li> <p>When you have successfully deployed the model, try to make predictions with it. What should the request     look like?</p> Solution <p>It depends on how exactly you have chosen to implement <code>main.py</code>. But for the provided solution, the payload should look like this:</p> <pre><code>{\n    \"data\": [1, 2, 3, 4]\n}\n</code></pre> <p>with the corresponding <code>curl</code> command:</p> <pre><code>curl -X POST \\\n    https://your-cloud-function-url/knn_classifier \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"input_data\": [5.1, 3.5, 1.4, 0.2]}'\n</code></pre> </li> </ol> </li> <li> <p>Let's try to figure out how to do the above deployment using <code>gcloud</code> instead of the console UI. The relevant command     is gcloud functions deploy. For this function     to work you will need to put the <code>main.py</code> and <code>requirements.txt</code> in a separate folder. Try to execute the command     to successfully deploy the function.</p> Solution <pre><code>gcloud functions deploy &lt;func-name&gt; \\\n    --gen2 --runtime python311 --trigger-http --source &lt;folder&gt; --entry-point knn_classifier\n</code></pre> <p>where you need to replace <code>&lt;func-name&gt;</code> with the name of your function and <code>&lt;folder&gt;</code> with the path to the folder containing the <code>main.py</code> and <code>requirements.txt</code> files.</p> </li> <li> <p>(Optional) You can finally try to redo the exercises by deploying a PyTorch application. You will essentially     need to go through the same steps as the sklearn example, including uploading a trained model to storage and     writing a cloud function that loads it and returns some output. You are free to choose whatever PyTorch model you     want.</p> </li> </ol>"},{"location":"s7_deployment/cloud_deployment/#cloud-run","title":"Cloud Run","text":"<p>Cloud functions are great for simple deployments that can be encapsulated in a single script with only simple requirements. However, they do not scale with more advanced applications that may depend on multiple programming languages. We are already familiar with how we can deal with this through containers, and Cloud Run is the corresponding service in GCP for deploying containers.</p>"},{"location":"s7_deployment/cloud_deployment/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>We are going to start locally by developing a small app that we can deploy. We provide two small examples to choose     from: first is a small FastAPI app consisting of a single Python script and a docker file. The second is a small     Streamlit app (which you can learn more about in this module) consisting of a single docker file.     You can choose which one you want to work with.</p> Simple FastAPI app simple_fastapi_app.py<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    \"\"\"Root endpoint.\"\"\"\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    \"\"\"Get an item by id.\"\"\"\n    return {\"item_id\": item_id}\n</code></pre> simple_fastapi_app.dockerfile<pre><code>FROM python:3.11-slim\n\nEXPOSE $PORT\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    software-properties-common \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN pip install fastapi\nRUN pip install pydantic\nRUN pip install uvicorn\n\nCOPY simple_fastapi_app.py simple_fastapi_app.py\n\nCMD exec uvicorn simple_fastapi_app:app --port $PORT --host 0.0.0.0 --workers 1\n</code></pre> Simple Streamlit app streamlit_app.dockerfile<pre><code>FROM python:3.9-slim\n\nEXPOSE $PORT\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    software-properties-common \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN git clone https://github.com/streamlit/streamlit-example.git .\n\nRUN pip3 install -r requirements.txt\n\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=$PORT\", \"--server.address=0.0.0.0\"]\n</code></pre> <ol> <li> <p>Start by going over the files belonging to your chosen app and understand what they do.</p> </li> <li> <p>Next, build the docker image belonging to the app.</p> <pre><code>docker build -f &lt;dockerfile&gt; . -t gcp_test_app:latest\n</code></pre> </li> <li> <p>Next tag and push the image to your artifact registry.</p> <pre><code>docker tag gcp_test_app &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/gcp_test_app:latest\ndocker push &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;registry-name&gt;/gcp_test_app:latest\n</code></pre> <p>Afterward, check that your artifact registry contains the pushed image.</p> </li> </ol> </li> <li> <p>Next, go to <code>Cloud Run</code> in the cloud console and enable the service or use the following command:</p> <pre><code>gcloud services enable run.googleapis.com\n</code></pre> </li> <li> <p>Click the <code>Create Service</code> button which should bring you to a page similar to the one below.</p> <p> </p> <p>Do the following:</p> <ul> <li> <p>Click the select button, which will bring up all built containers, and pick the one you want to deploy. In the     future, you probably want to choose the Continuously deploy new revisions from a source repository such that     a new version is always deployed when a new container is built.</p> </li> <li> <p>Hereafter, give the service a name and select the region. We recommend choosing a region close to you.</p> </li> <li> <p>Set the authentication method to Allow unauthenticated invocations such that we can call it without     providing credentials. In the future, you may only set that authenticated invocations are allowed.</p> </li> <li> <p>Expand the Container, Connections, Security tab and edit the port such that it matches the port exposed in your     chosen application. If your docker file exposes the env variable <code>$PORT</code> you can set the port to anything.</p> </li> </ul> <p>Finally, click the create button and wait for the service to be deployed (may take some time).</p> <p>Common problems</p> <p>If you get an error saying The user-provided container failed to start and listen on the port defined by the PORT environment variable. there are two common reasons for this:</p> <ol> <li> <p>You need to add an <code>EXPOSE</code> statement in your docker container:</p> <pre><code>EXPOSE 8080\nCMD exec uvicorn my_application:app --port 8080 --workers 1 main:app\n</code></pre> <p>and make sure that your application is also listening on that port. If you hard code the port in your application (as in the above code) it is best to set it 8080 which is the default port for cloud run. Alternatively, a better approach is to set it to the <code>$PORT</code> environment variable which is set by cloud run and can be accessed in your application:</p> <pre><code>EXPOSE $PORT\nCMD exec uvicorn my_application:app --port $PORT --workers 1 main:app\n</code></pre> <p>If you do this and then want to run locally you can run it as:</p> <pre><code>docker run -p 8080:8080 -e PORT=8080 &lt;image-name&gt;:&lt;image-tag&gt;\n</code></pre> </li> <li> <p>If you are serving a large machine-learning model, it may also be that your deployed container is running     out of memory. You can try to increase the memory of the container by going to the Edit container and     the Resources tab and increasing the memory.</p> </li> </ol> </li> <li> <p>If you manage to deploy the service you should see an image like this:</p> <p> </p> <p>You can now access your application by clicking the URL. This will access the root of your application, so you may need to add <code>/</code> or <code>/&lt;path&gt;</code> to the URL depending on how the app works.</p> </li> <li> <p>Everything we just did in the console UI we can also do with     gcloud run deploy. How would you do that?</p> Solution <p>The command should look something like this</p> <pre><code>gcloud run deploy &lt;service-name&gt; \\\n    --image &lt;image-name&gt;:&lt;image-tag&gt; --platform managed --region &lt;region&gt; --allow-unauthenticated\n</code></pre> <p>where you need to replace <code>&lt;service-name&gt;</code> with the name of your service, <code>&lt;image-name&gt;</code> with the name of your image and <code>&lt;region&gt;</code> with the region you want to deploy to. The <code>--allow-unauthenticated</code> flag is optional but is needed if you want to access the service without providing credentials.</p> </li> <li> <p>After deploying using the command line, make sure that the service is up and running by using these two commands:</p> <pre><code>gcloud run services list\ngcloud run services describe &lt;service-name&gt; --region &lt;region&gt;\n</code></pre> </li> <li> <p>Instead of deploying our docker container using the UI or command line, which is a manual operation, we can do it     continuously by using the <code>cloudbuild.yaml</code> file we learned about in the previous section. This is called     continuous deployment, and it is a way to     automate the deployment process.</p> <p>  Image credit  </p> <p>Let's revise the <code>cloudbuild.yaml</code> file from the artifact registry exercises in this module which will build and push a specified docker image.</p> <p>cloudbuild.yaml</p> cloudbuild.yaml<pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Build container image'\n  args: [\n    'build',\n    '.',\n    '-t',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/&lt;image-name&gt;',\n    '-f',\n    '&lt;path-to-dockerfile&gt;'\n  ]\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Push container image'\n  args: [\n    'push',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/&lt;image-name&gt;'\n  ]\noptions:\n  logging: CLOUD_LOGGING_ONLY\n</code></pre> <p>Add a third step to the <code>cloudbuild.yaml</code> file that deploys the container image to Cloud Run. The relevant service you need to use is called <code>'gcr.io/cloud-builders/gcloud'</code> and the command is <code>'gcloud run deploy'</code>. Afterwards, reuse the trigger you created in the previous module or create a new one to build and deploy the container image continuously. Confirm that this works by making a change to your application and pushing it to GitHub and see if the application is updated continuously.</p> Solution <p>The full <code>cloudbuild.yaml</code> file should look like this:</p> <pre><code>steps:\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Build container image'\n  args: [\n    'build',\n    '.',\n    '-t',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/&lt;image-name&gt;',\n    '-f',\n    '&lt;path-to-dockerfile&gt;'\n  ]\n- name: 'gcr.io/cloud-builders/docker'\n  id: 'Push container image'\n  args: [\n    'push',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/&lt;image-name&gt;'\n  ]\n- name: 'gcr.io/cloud-builders/gcloud'\n  id: 'Deploy to Cloud Run'\n  args: [\n    'run',\n    'deploy',\n    '&lt;service-name&gt;',\n    '--image',\n    'europe-west1-docker.pkg.dev/$PROJECT_ID/&lt;registry-name&gt;/&lt;image-name&gt;',\n    '--region',\n    'europe-west1',\n    '--platform',\n    'managed',\n    # optional argument if you want to inject secrets as environment variables\n    '--update-secrets=&lt;env-name-in-container&gt;=&lt;secret-name-in-secrets-manager&gt;:latest'\n  ]\noptions:\n  logging: CLOUD_LOGGING_ONLY\n</code></pre> </li> <li> <p>(Optional) A common pattern when using cloud run is that your application need access to some storage during     operations, either for reading or writing a file. This could be reading in a model checkpoint on startup and then     writing some statistics during runtime. The easiest way to do this is to mount a storage bucket to your container.</p> <ol> <li> <p>Consider the following application that reads and writes for a folder <code>FOLDER = \"/gcs/&lt;bucket-name&gt;/\"</code>.</p> Simple FastAPI app with storage simple_fastapi_app_volume.py<pre><code>import os\n\nfrom fastapi import FastAPI, File, UploadFile\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    \"\"\"Root endpoint.\"\"\"\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    \"\"\"Get an item by id.\"\"\"\n    return {\"item_id\": item_id}\n\n\nFOLDER = \"/gcs/&lt;bucket-name&gt;/\"\n\n\n@app.post(\"/upload/\")\ndef upload_file(file: UploadFile = File(...)):\n    \"\"\"Upload a file.\"\"\"\n    file_location = os.path.join(FOLDER, file.filename)\n    with open(file_location, \"wb\") as f:\n        f.write(file.read())\n    return {\"info\": f\"file '{file.filename}' saved at '{file_location}'\"}\n\n\n@app.get(\"/files/\")\ndef list_files():\n    \"\"\"List files in the upload folder.\"\"\"\n    files = os.listdir(FOLDER)\n    return {\"files\": files}\n</code></pre> <p>Replace the <code>&lt;bucket-name&gt;</code> in the folder path with a bucket you have created. Then write a small dockerfile and deploy the application to cloud run.</p> Solution <p>The dockerfile should look like this:</p> simple_fastapi_app_volume.dockerfile<pre><code>FROM python:3.11-slim\n\nEXPOSE $PORT\n\nWORKDIR /app\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    software-properties-common \\\n    git \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN pip install fastapi\nRUN pip install pydantic\nRUN pip install uvicorn\n\nCOPY simple_fastapi_app.py simple_fastapi_app.py\n\nCMD exec uvicorn simple_fastapi_app:app --port $PORT --host 0.0.0.0 --workers 1\n</code></pre> <p>and the deployment command should look like this:</p> <pre><code>gcloud run deploy simple-fastapi-with-mounted-volume \\\n    --image &lt;image-name&gt;:&lt;image-tag&gt; --platform managed --region &lt;region&gt; --allow-unauthenticated \\\n</code></pre> </li> <li> <p>If you then try to access either the <code>/upload/</code> or <code>/files/</code> endpoints you should see that the application at     this point is unable to read or write to the folder location, because it is not mounted to the container yet.     Mounting a volume can either be done though the command line or through the UI. You can read how to do     here. Add a     volume to your cloud run service and try to access the endpoints again making sure you can read and write.</p> Solution <p>If you are using the UI, follow the instructions in the link above. If you are using the command line, the command should look like this:</p> <pre><code>gcloud run services update SERVICE \\\n    --add-volume name=&lt;volume-name&gt;,type=cloud-storage,bucket=&lt;bucket-name&gt; \\\n    --add-volume-mount volume=&lt;volume-name&gt;,mount-path=\"/gcs/&lt;bucket-name&gt;\"\n</code></pre> <p>The <code>&lt;volume-name&gt;</code> you can choose arbitrarily, but the <code>&lt;bucket-name&gt;</code> should be the name of the bucket you want to mount. And the <code>&lt;mount-path&gt;</code> should be the path you are reading from/writing to inside your application.</p> </li> </ol> </li> </ol>"},{"location":"s7_deployment/cloud_deployment/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>In the previous module on using the cloud you learned about the Secrets     Manager in GCP. How can you use this service in combination with Cloud Run?</p> Solution <p>In the cloud console, secrets can be set in the Container(s), Volumes, Networking, Security tab under the Variables &amp; Secrets section, see image below.</p> <p> </p> <p>In the <code>gcloud</code> command, you can set the secret by using the <code>--update-secrets</code> flag.</p> <pre><code>gcloud run deploy &lt;service-name&gt; \\\n    --image &lt;image-name&gt;:&lt;image-tag&gt; --platform managed \\\n    --region &lt;region&gt; --allow-unauthenticated \\\n    --update-secrets &lt;secret-name&gt;=&lt;secret-version&gt;\n</code></pre> </li> </ol> <p>That ends the exercises on deployment. The exercises above are just a small taste of what deployment has to offer. In both sections, we have explicitly chosen to work with serverless deployments. But what if you wanted to do the opposite, i.e., being the one in charge of the management of the cluster that handles the deployed services? If you are interested in taking deployment to the next level, you should get started on Kubernetes which is the de-facto open-source container orchestration platform that is used in production environments. If you want to deep dive we recommend starting here, which describes how to make pipelines that are a necessary component before you start to create your own Kubernetes cluster.</p>"},{"location":"s7_deployment/frontend/","title":"M26 - Frontend","text":""},{"location":"s7_deployment/frontend/#frontend","title":"Frontend","text":"<p>If you have gone over the deployment module you should be at the point where you have a machine learning model running in the cloud. The model can be interacted with by sending HTTP requests to the API endpoint. In general we refer to this as the backend of the application. It is the part of our application that is behind-the-scenes that the user does not see and is not really that user-friendly. Instead we want to create a frontend that the user can interact with in a more user-friendly way. This is what we will be doing in this module.</p> <p>Another point of splitting our application into a frontend and a backend has to do with scalability. If we have a lot of users interacting with our application, we might want to scale only the backend and not the frontend, because that is the part that will be running our heavy machine learning model. In general dividing an application into smaller pieces is the pattern that is used in microservice architectures.</p> <p></p>  In monollithic applications everything the user may be requesting of our application is handled by a single process/ container. In microservice architectures the application is split into smaller pieces that can be scaled independently. This also leads to easier maintainability and faster development.  <p>Frontends have for the longest time been created using HTML, CSS and JavaScript. This is still the case, but there are now a lot of frameworks that can help us create a frontend in Python:</p> <ul> <li>Django</li> <li>Reflex</li> <li>Streamlit</li> <li>Bokeh</li> <li>Gradio</li> </ul> <p>In this module we will be looking at <code>streamlit</code>. <code>streamlit</code> is an easy-to-use framework that allows us to create interactive web applications in Python. It is not at all as powerful as a framework like <code>Django</code>, but it is very easy to get started with and it is very easy to integrate with our machine learning models.</p>"},{"location":"s7_deployment/frontend/#exercises","title":"\u2754 Exercises","text":"<p>In these exercises we go through the process of setting up a backend using <code>fastapi</code> and a frontend using <code>streamlit</code>, containerizing both applications and then deploying them to the cloud. We have already created an example of this which can be found in the <code>samples/frontend_backend</code> folder.</p> <ol> <li> <p>Let's start by creating the backend application in a <code>backend.py</code> file. You can use essentially any backend you want,     but we will be using a simple imagenet classifier that we have created in the <code>samples/frontend_backend/backend</code>     folder.</p> <ol> <li> <p>Create a new file called <code>backend.py</code> and implement a FastAPI interface with a single <code>/predict</code> endpoint that     takes an image as input and returns the predicted class (and probabilities) of the image.</p> Solution backend.py<pre><code>import json\nfrom contextlib import asynccontextmanager\n\nimport anyio\nimport torch\nfrom fastapi import FastAPI, File, HTTPException, UploadFile\nfrom PIL import Image\nfrom torchvision import models, transforms\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Context manager to start and stop the lifespan events of the FastAPI application.\"\"\"\n    global model, transform, imagenet_classes\n    # Load model\n    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n    model.eval()\n\n    transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ],\n    )\n\n    async with await anyio.open_file(\"imagenet-simple-labels.json\") as f:\n        imagenet_classes = json.load(f)\n\n    yield\n\n    # Clean up\n    del model\n    del transform\n    del imagenet_classes\n\n\napp = FastAPI(lifespan=lifespan)\n\n\ndef predict_image(image_path: str) -&gt; str:\n    \"\"\"Predict image class (or classes) given image path and return the result.\"\"\"\n    img = Image.open(image_path).convert(\"RGB\")\n    img = transform(img).unsqueeze(0)\n    with torch.no_grad():\n        output = model(img)\n    _, predicted_idx = torch.max(output, 1)\n    return output.softmax(dim=-1), imagenet_classes[predicted_idx.item()]\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Root endpoint.\"\"\"\n    return {\"message\": \"Hello from the backend!\"}\n\n\n# FastAPI endpoint for image classification\n@app.post(\"/classify/\")\nasync def classify_image(file: UploadFile = File(...)):\n    \"\"\"Classify image endpoint.\"\"\"\n    try:\n        contents = await file.read()\n        async with await anyio.open_file(file.filename, \"wb\") as f:\n            f.write(contents)\n        probabilities, prediction = predict_image(file.filename)\n        return {\"filename\": file.filename, \"prediction\": prediction, \"probabilities\": probabilities.tolist()}\n    except Exception as e:\n        raise HTTPException(status_code=500) from e\n</code></pre> </li> <li> <p>Run the backend using <code>uvicorn</code>.</p> <pre><code>uvicorn backend:app --reload\n</code></pre> </li> <li> <p>Test the backend by sending a request to the <code>/predict</code> endpoint, preferably using the <code>curl</code> command.</p> Solution <p>In this example we are sending a request to the <code>/predict</code> endpoint with a file called <code>my_cat.jpg</code>. The response should be \"tabby cat\" for the solution we have provided.</p> <pre><code>curl -X 'POST' \\\n    'http://127.0.0.1:8000/classify/' \\\n    -H 'accept: application/json' \\\n    -H 'Content-Type: multipart/form-data' \\\n    -F 'file=@my_cat.jpg;type=image/jpeg'\n</code></pre> </li> <li> <p>Create a <code>requirements_backend.txt</code> file with the dependencies needed for the backend.</p> Solution requirements_backend.txt<pre><code>fastapi&gt;=0.108.0\nuvicorn&gt;=0.25.0\ntorch&gt;=2.1.2\ntorchvision&gt;=0.16.2\n</code></pre> </li> <li> <p>Containerize the backend into a file called <code>backend.dockerfile</code>.</p> Solution backend.dockerfile<pre><code>FROM python:3.11-slim\n\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc git &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN mkdir /app\n\nWORKDIR /app\n\nCOPY requirements_backend.txt /app/requirements_backend.txt\nCOPY backend.py /app/backend.py\n\nRUN --mount=type=cache,target=/root/.cache/pip pip install -r requirements_backend.txt\n\nEXPOSE $PORT\nCMD exec unicorn --port $PORT --host 0.0.0.0 backend:app\n</code></pre> </li> <li> <p>Build the backend image</p> <pre><code>docker build -t backend:latest -f backend.dockerfile .\n</code></pre> </li> <li> <p>Recheck that the backend works by running the image in a container</p> <pre><code>docker run --rm -p 8000:8000 -e \"PORT=8000\" backend\n</code></pre> <p>and test that it works by sending a request to the <code>/predict</code> endpoint.</p> </li> <li> <p>Deploy the backend to Cloud run using the <code>gcloud</code> command.</p> Solution <p>Assuming that we have created an artifact registry called <code>frontend_backend</code> we can deploy the backend to Cloud Run using the following commands:</p> <pre><code>docker tag \\\n    backend:latest \\\n    &lt;region&gt;-docker.pkg.dev/&lt;project&gt;/frontend-backend/backend:latest\ndocker push \\\n    &lt;region&gt;.pkg.dev/&lt;project&gt;/frontend-backend/backend:latest\ngcloud run deploy backend \\\n    --image=europe-west1-docker.pkg.dev/&lt;project&gt;/frontend-backend/backend:latest \\\n    --region=europe-west1 \\\n    --platform=managed \\\n</code></pre> <p>where <code>&lt;region&gt;</code> and <code>&lt;project&gt;</code> should be replaced with the appropriate values.</p> </li> <li> <p>Finally, test that the deployed backend works as expected by sending a request to the <code>/predict</code> endpoint.</p> Solution <p>In this solution we are first extracting the url of the deployed backend and then sending a request to the <code>/predict</code> endpoint.</p> <pre><code>export MYENDPOINT=$(gcloud run services describe backend --region=&lt;region&gt; --format=\"value(status.url)\")\ncurl -X 'POST' \\\n    $MYENDPOINT/predict \\\n    -H 'accept: application/json' \\\n    -H 'Content-Type: multipart/form-data' \\\n    -F 'file=@my_cat.jpg;type=image/jpeg'\n</code></pre> </li> </ol> </li> <li> <p>With the backend taken care of let's now write our frontend. Our frontend just needs to be a \"nice\" interface to our     backend. Its main functionality will be to send a request to the backend and display the result     (streamlit documentation).</p> <ol> <li> <p>Start by installing <code>streamlit</code>.</p> <pre><code>pip install streamlit\n</code></pre> </li> <li> <p>Now create a file called <code>frontend.py</code> and implement a streamlit application. You can design it however you want,     but we recommend that the following can be done in the frontend:</p> <ol> <li> <p>Have a file uploader that allows the user to upload an image</p> </li> <li> <p>Display the image that the user uploaded</p> </li> <li> <p>Have a button that sends the image to the backend and displays the result</p> </li> </ol> <p>For now just assume that an environment variable called <code>BACKEND</code> is available that contains the URL of the backend. We will in the next step show how to get this URL automatically.</p> Solution frontend.py<pre><code>import os\n\nimport pandas as pd\nimport requests\nimport streamlit as st\nfrom google.cloud import run_v2\n\n\ndef get_backend_url():\n    \"\"\"Get the URL of the backend service.\"\"\"\n    parent = \"projects/my-personal-mlops-project/locations/europe-west1\"\n    client = run_v2.ServicesClient()\n    services = client.list_services(parent=parent)\n    for service in services:\n        if service.name.split(\"/\")[-1] == \"production-model\":\n            return service.uri\n    return os.environ.get(\"BACKEND\", None)\n\n\ndef classify_image(image, backend):\n    \"\"\"Send the image to the backend for classification.\"\"\"\n    predict_url = f\"{backend}/predict\"\n    response = requests.post(predict_url, files={\"image\": image}, timeout=10)\n    if response.status_code == 200:\n        return response.json()\n    return None\n\n\ndef main() -&gt; None:\n    \"\"\"Main function of the Streamlit frontend.\"\"\"\n    backend = get_backend_url()\n    if backend is None:\n        msg = \"Backend service not found\"\n        raise ValueError(msg)\n\n    st.title(\"Image Classification\")\n\n    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n\n    if uploaded_file is not None:\n        image = uploaded_file.read()\n        result = classify_image(image, backend=backend)\n\n        if result is not None:\n            prediction = result[\"prediction\"]\n            probabilities = result[\"probabilities\"]\n\n            # show the image and prediction\n            st.image(image, caption=\"Uploaded Image\")\n            st.write(\"Prediction:\", prediction)\n\n            # make a nice bar chart\n            data = {\"Class\": [f\"Class {i}\" for i in range(10)], \"Probability\": probabilities}\n            df = pd.DataFrame(data)\n            df.set_index(\"Class\", inplace=True)\n            st.bar_chart(df, y=\"Probability\")\n        else:\n            st.write(\"Failed to get prediction\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> </li> <li> <p>We need to make sure that the frontend knows where the backend is located, and we want that to happen     automatically so we do not have to hardcode the URL into our frontend. We can do this by using the     Python SDK for Google Cloud Run. The following code snippet shows how to get the URL of the backend service     or fall back on an environment variable if the service is not found.</p> <pre><code>from google.cloud import run_v2\nimport streamlit as st\n\n@st.cache_resource  # (1)!\ndef get_backend_url():\n    \"\"\"Get the URL of the backend service.\"\"\"\n    parent = \"projects/&lt;project&gt;/locations/&lt;region&gt;\"\n    client = run_v2.ServicesClient()\n    services = client.list_services(parent=parent)\n    for service in services:\n        if service.name.split(\"/\")[-1] == \"production-model\":\n            return service.uri\n    name = os.environ.get(\"BACKEND\", None)\n    return name\n</code></pre> <ol> <li> The <code>st.cache_resource</code> is a decorator that tells <code>streamlit</code> to cache the result of the     function. This is useful if the function is expensive to run and we want to avoid running it multiple times.</li> </ol> <p>Add the above code snippet to the top of your <code>frontend.py</code> file and replace <code>&lt;project&gt;</code> and <code>&lt;region&gt;</code> with the appropriate values. You will need to install <code>pip install google-cloud-run</code> to be able to use the code snippet.</p> </li> <li> <p>Run the frontend using <code>streamlit</code>.</p> <pre><code>streamlit run frontend.py\n</code></pre> </li> <li> <p>Create a <code>requirements_frontend.txt</code> file with the dependencies needed for the frontend.</p> Solution requirements_frontend.txt<pre><code>streamlit&gt;=1.28.2\npandas&gt;=2.1.3\ngoogle-cloud-run&gt;=0.10.5\n</code></pre> </li> <li> <p>Containerize the frontend into a file called <code>frontend.dockerfile</code>.</p> Solution frontend.dockerfile<pre><code>FROM python:3.11-slim\n\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc git &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN mkdir /app\n\nWORKDIR /app\n\nCOPY requirements_frontend.txt /app/requirements_frontend.txt\nCOPY frontend.py /app/frontend.py\n\nRUN --mount=type=cache,target=/root/.cache/pip pip install -r requirements_frontend.txt\n\nEXPOSE $PORT\n\nENTRYPOINT [\"streamlit\", \"run\", \"frontend.py\", \"--server.port\", \"$PORT\", \"--server.address=0.0.0.0\"]\n</code></pre> </li> <li> <p>Build the frontend image.</p> <pre><code>docker build -t frontend:latest -f frontend.dockerfile .\n</code></pre> </li> <li> <p>Run the frontend image</p> <pre><code>docker run --rm -p 8001:8001 -e \"PORT=8001\" frontend\n</code></pre> <p>and check in your web browser that the frontend works as expected.</p> </li> <li> <p>Deploy the frontend to Cloud run using the <code>gcloud</code> command.</p> Solution <p>Assuming that we have created an artifact registry called <code>frontend_backend</code> we can deploy the frontend to Cloud Run using the following commands:</p> <pre><code>docker tag frontend:latest \\\n    &lt;region&gt;-docker.pkg.dev/&lt;project&gt;/frontend-backend/frontend:latest\ndocker push &lt;region&gt;.pkg.dev/&lt;project&gt;/frontend-backend/frontend:latest\ngcloud run deploy frontend \\\n    --image=europe-west1-docker.pkg.dev/&lt;project&gt;/frontend-backend/frontend:latest \\\n    --region=europe-west1 \\\n    --platform=managed \\\n</code></pre> </li> <li> <p>Test that the frontend works as expected by opening the URL of the deployed frontend in your web browser.</p> </li> </ol> </li> <li> <p>(Optional) If you have gotten this far you have successfully created a frontend and a backend and deployed them to     the cloud. Finally, it may be worth load testing your application to see how it performs under load. Write a     locust file which is covered in this module and run it on your frontend.     Make sure that it can handle the load you expect it to handle.</p> </li> <li> <p>(Optional) Feel free to experiment further with streamlit and see what you can create. For example, you can try to     create an option for the user to upload a video and then display the video with the predicted class overlaid on     top of the video.</p> </li> </ol>"},{"location":"s7_deployment/frontend/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>We have created separate requirement files for the frontend and the backend. Why is this a good idea?</p> Solution <p>This is a good idea because the frontend and the backend may have different dependencies. By having separate requirements files we can make sure that we only install the dependencies that are needed for the specific application. This also has the positive side effect that we can keep the docker images smaller. For example, the frontend does not need the <code>torch</code> library which is huge and only needed for the backend.</p> </li> </ol> <p>This ends the exercises for this module.</p>"},{"location":"s7_deployment/ml_deployment/","title":"M25 - ML deployment","text":""},{"location":"s7_deployment/ml_deployment/#deployment-of-machine-learning-models","title":"Deployment of Machine Learning Models","text":"<p>In one of the previous modules you learned about how to use FastAPI to create an API to interact with your machine learning models. FastAPI is a great framework, but it is a general framework meaning that it was not developed with machine learning applications in mind. This means that there are features which you may consider to be missing when considering running large scale machine learning models:</p> <ul> <li> <p>Dynamic-batching: if you have a large number of requests coming in, you may want to process them in batches to     reduce the overhead of loading the model and running the inference. This is especially true if you are running your     model on a GPU, where the overhead of loading the model is significant.</p> </li> <li> <p>Async inference: FastAPI does support async requests but not a way to call the model asynchronously. This means that     if you have a large number of requests coming in, you will have to wait for the model to finish processing (because     the model is not async) before you can start processing the next request.</p> </li> <li> <p>Native GPU support: you can definitely run part of your application in FastAPI if you want to. But again it was not     built with machine learning in mind, so you will have to do some extra work to get it to work.</p> </li> </ul> <p>It should come as no surprise that multiple frameworks have therefore sprung up that better support deployment of machine learning algorithms (just listing a few here):</p> \ud83c\udf1f Framework \ud83e\udde9 Backend Agnostic \ud83e\udde0 Model Agnostic \ud83d\udcc2 Repository \u2b50 GitHub Stars Cortex \u2705 \u2705 \ud83d\udd17 Link 8.0k BentoML \u2705 \u2705 \ud83d\udd17 Link 7.8k Ray Serve \u2705 \u2705 \ud83d\udd17 Link 37.8k Triton Inference Server \u2705 \u2705 \ud83d\udd17 Link 9.4k OpenVINO \u2705 \u2705 \ud83d\udd17 Link 8.5k Seldon-core \u2705 \u2705 \ud83d\udd17 Link 4.6k Litserve \u2705 \u2705 \ud83d\udd17 Link 3.3k Torchserve \u274c \u2705 \ud83d\udd17 Link 4.3k TensorFlow serve \u274c \u2705 \ud83d\udd17 Link 6.3k vLLM \u274c \u274c \ud83d\udd17 Link 51.1k <p>The first 7 frameworks are backend agnostic, meaning that they are intended to work with whatever computational backend your model is implemented in (TensorFlow, PyTorch, Jax, Sklearn, etc.), whereas the last 3 are backend specific (PyTorch, TensorFlow and a custom framework). The first 9 frameworks are model agnostic, meaning that they are intended to work with whatever model you have implemented, whereas the last one is model specific in this case to LLM's. When choosing a framework to deploy your model, you should consider the following:</p> <ul> <li> <p>Ease of use. Some frameworks are easier to use and get started with than others, but may have fewer features. As     an example from the list above, <code>Litserve</code> is very easy to get started with but is a relatively new framework and     may not have all the features you need.</p> </li> <li> <p>Performance. Some frameworks are optimized for performance, but may be harder to use. As an example from the list     above, <code>vLLM</code> is a very high performance framework for serving large language models but it cannot be used for other     types of models.</p> </li> <li> <p>Community. Some frameworks have a large community, which can be helpful if you run into problems. As an example     from the list above, <code>Triton Inference Server</code> was developed by Nvidia and has a large community of users. As a good     rule of thumb, the more stars a repository has on GitHub, the larger the community.</p> </li> </ul> <p>In this module we are going to be looking at the <code>BentoML</code> framework because it strikes a good balance between ease of use and having a lot of features that can improve the performance of serving your models. However, before we dive into this serving framework, we are going to look at a general way to package our machine learning models that should work with most of the above frameworks.</p>"},{"location":"s7_deployment/ml_deployment/#model-packaging","title":"Model Packaging","text":"<p>Whenever we want to serve a machine learning model, we in general need three things:</p> <ul> <li>The computational graph of the model, e.g. how to pass data through the model to get a prediction</li> <li>The weights of the model, e.g. the parameters that the model has learned during training</li> <li>A computational backend that can run the model</li> </ul> <p>In the past module on Docker we learned how to package all of these things into a container. This is a great way to package a model, but it is not the only way. The core assumption we have currently made is that the computational backend is the same as the one we trained the model on. However, this does not need to be the case. As long as we can export our model and weights to a common format, we can run the model on any backend that supports this format.</p> <p>This is exactly what the Open Neural Network Exchange (ONNX) is designed to do. ONNX is a standardized format for creating and sharing machine learning models. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types. The idea behind ONNX is that a model trained with a specific framework on a specific device, let's say PyTorch on your local computer, can be exported and run with an entirely different framework and hardware easily. Learning how to export your models to ONNX is therefore a great way to increase the longevity of your models and not be locked into a specific framework for serving your models.</p> <p></p>  The ONNX format is designed to bridge the gap between development and deployment of machine learning models by making it easy to export models between different frameworks and hardware. For example, PyTorch is in general considered to be a developer-friendly framework, though it has historically been slow to run inference with.  Image credit"},{"location":"s7_deployment/ml_deployment/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by installing ONNX, ONNX runtime and ONNX script. This can be done by running the following command:</p> <pre><code>pip install onnx onnxruntime onnxscript\n</code></pre> <p>The first package contains the core ONNX framework, the second package contains the runtime for running ONNX models and the third package contains a new experimental package that is designed to make it easier to export models to ONNX.</p> </li> <li> <p>Let's start out by converting a model to ONNX. The following code snippets show how to export a PyTorch model to     ONNX.</p> PyTorch =&gt; 2.0PyTorch &lt; 2.0 or WindowsPyTorch-lightning <pre><code>import torch\nimport torchvision\n\nmodel = torchvision.models.resnet18(weights=None)\nmodel.eval()\n\ndummy_input = torch.randn(1, 3, 224, 224)\nonnx_model = torch.onnx.dynamo_export(\n    model=model,\n    model_args=(dummy_input,),\n    export_options=torch.onnx.ExportOptions(dynamic_shapes=True),\n)\nonnx_model.save(\"resnet18.onnx\")\n</code></pre> <pre><code>import torch\nimport torchvision\n\nmodel = torchvision.models.resnet18(weights=None)\nmodel.eval()\n\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(\n    model=model,\n    args=(dummy_input,),\n    f=\"resnet18.onnx\",\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n)\n</code></pre> <pre><code>import torch\nimport torchvision\nimport pytorch_lightning as pl\nimport onnx\nimport onnxruntime\n\nclass LitModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = torchvision.models.resnet18(pretrained=True)\n        self.model.eval()\n\n    def forward(self, x):\n        return self.model(x)\n\nmodel = LitModel()\nmodel.eval()\n\ndummy_input = torch.randn(1, 3, 224, 224)\nmodel.to_onnx(\n    file_path=\"resnet18.onnx\",\n    input_sample=dummy_input,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n)\n</code></pre> <p>Export a model of your own choice to ONNX or just try to export the <code>resnet18</code> model as shown in the examples above, and confirm that the model was exported by checking that the file exists. Can you figure out what is meant by <code>dynamic_axes</code>?</p> Solution <p>The <code>dynamic_axes</code> argument is used to specify which axes of the input tensor that should be considered dynamic. This is useful when the model can accept inputs of different sizes, e.g. when the model is used in a dynamic batching scenario. In the example above we have specified that the first axis of the input tensor should be considered dynamic, meaning that the model can accept inputs of different batch sizes. While it may be tempting to specify all axes as dynamic, this can lead to slower inference times because the ONNX runtime will not be able to optimize the computational graph as well.</p> </li> <li> <p>Check that the model was correctly exported by loading it using the <code>onnx</code> package and afterwards check the graph     of the model using the following code:</p> <pre><code>import onnx\nmodel = onnx.load(\"resnet18.onnx\")\nonnx.checker.check_model(model)\nprint(onnx.helper.printable_graph(model.graph))\n</code></pre> </li> <li> <p>To get a better understanding of what is actually exported, let's try to visualize the computational graph of the     model. This can be done using the open-source tool netron. You can either     try it out directly in webbrowser or you can install it locally using <code>pip install netron</code>     and then run it using <code>netron resnet18.onnx</code>. Can you figure out what method of the model is exported to ONNX?</p> Solution <p>When a PyTorch model is exported to ONNX, it is only the <code>forward</code> method of the model that is exported. This means that that is the only method we have access to when we load the model later. Therefore, make sure that the <code>forward</code> method of your model is implemented in a way that it can be used for inference.</p> </li> <li> <p>After converting a model to ONNX format we can use ONNX Runtime to run it.     The benefit of this is that ONNX Runtime is able to optimize the computational graph of the model, which can lead     to faster inference times. Let's try to look into that.</p> <ol> <li> <p>Figure out how to run a model using the ONNX Runtime. Relevant     documentation.</p> Solution <p>To use the ONNX runtime to run a model, we first need to start an inference session, then extract the input and output names of our model and finally run the model. The following code snippet shows how to do this.</p> <pre><code>import onnxruntime as rt\nort_session = rt.InferenceSession(\"&lt;path-to-model&gt;\")\ninput_names = [i.name for i in ort_session.get_inputs()]\noutput_names = [i.name for i in ort_session.get_outputs()]\nbatch = {input_names[0]: np.random.randn(1, 3, 224, 224).astype(np.float32)}\nout = ort_session.run(output_names, batch)\n</code></pre> </li> <li> <p>Let's experiment with the performance of ONNX vs. PyTorch. Implement a benchmark that measures the time it takes to     run a model using PyTorch and ONNX. Bonus points if you test for multiple input sizes. To get you started we     have implemented a timing decorator that you can use to measure the time it takes to run a function.</p> <pre><code>from statistics import mean, stdev\nimport time\ndef timing_decorator(func, function_repeat: int = 10, timing_repeat: int = 5):\n    \"\"\" Decorator that times the execution of a function. \"\"\"\n    def wrapper(*args, **kwargs):\n        timing_results = []\n        for _ in range(timing_repeat):\n            start_time = time.time()\n            for _ in range(function_repeat):\n                result = func(*args, **kwargs)\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            timing_results.append(elapsed_time)\n        print(f\"Avg +- Stddev: {mean(timing_results):0.3f} +- {stdev(timing_results):0.3f} seconds\")\n        return result\n    return wrapper\n</code></pre> Solution onnx_benchmark.py<pre><code>import sys\nimport time\nfrom statistics import mean, stdev\n\nimport onnxruntime as ort\nimport torch\nimport torchvision\n\n\ndef timing_decorator(func, function_repeat: int = 10, timing_repeat: int = 5):\n    \"\"\"Decorator that times the execution of a function.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        timing_results = []\n        for _ in range(timing_repeat):\n            start_time = time.time()\n            for _ in range(function_repeat):\n                result = func(*args, **kwargs)\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            timing_results.append(elapsed_time)\n        print(f\"Avg +- Stddev: {mean(timing_results):0.3f} +- {stdev(timing_results):0.3f} seconds\")\n        return result\n\n    return wrapper\n\n\nmodel = torchvision.models.resnet18()\nmodel.eval()\n\ndummy_input = torch.randn(1, 3, 224, 224)\nif sys.platform == \"win32\":\n    # Windows doesn't support the new TorchDynamo-based ONNX Exporter\n    torch.onnx.export(\n        model,\n        dummy_input,\n        \"resnet18.onnx\",\n        input_names=[\"input.1\"],\n        dynamic_axes={\"input.1\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}},\n    )\nelse:\n    torch.onnx.dynamo_export(model, dummy_input).save(\"resnet18.onnx\")\n\nort_session = ort.InferenceSession(\"resnet18.onnx\")\n\n\n@timing_decorator\ndef torch_predict(image) -&gt; None:\n    \"\"\"Predict using PyTorch model.\"\"\"\n    model(image)\n\n\n@timing_decorator\ndef onnx_predict(image) -&gt; None:\n    \"\"\"Predict using ONNX model.\"\"\"\n    ort_session.run(None, {\"input.1\": image.numpy()})\n\n\nif __name__ == \"__main__\":\n    for size in [224, 448, 896]:\n        dummy_input = torch.randn(1, 3, size, size)\n        print(f\"Image size: {size}\")\n        torch_predict(dummy_input)\n        onnx_predict(dummy_input)\n</code></pre> </li> <li> <p>To get a better understanding of why running the model using the ONNX runtime is usually faster let's try to see     what happens to the computational graph. By default the ONNX Runtime will apply this optimization in online     mode, meaning that the optimizations are applied when the model is loaded. However, it is also possible to apply     the optimizations in offline mode, such that the optimized model is saved to disk. Below is an example of how     to do this.</p> <pre><code>import onnxruntime as rt\nsess_options = rt.SessionOptions()\n\n# Set graph optimization level\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n\n# To enable model serialization after graph optimization set this\nsess_options.optimized_model_filepath = \"optimized_model.onnx&gt;\"\n\nsession = rt.InferenceSession(\"&lt;model_path&gt;\", sess_options)\n</code></pre> <p>Try to apply the optimizations in offline mode and use <code>netron</code> to visualize both the original and optimized models side by side. Can you see any differences?</p> Solution <p>You should hopefully see that the optimized model consists of fewer nodes and edges than the original model. These nodes are often called fused nodes, because they are the result of multiple nodes being fused together. In the image below we have visualized the first part of the computational graph of a resnet18 model, before and after optimization.</p> <p> </p> </li> <li> <p>Exporting a model to ONNX is not always perfect out of the box. To the conversion of your PyTorch models there     need to be a one-to-one correspondence between PyTorch and ONNX operators. Especially, the     opset number is important to set correctly     in ONNX to get the correct operators. If this is not the case, the exported model can lead to a difference in     results. To check the model, it is therefore also a good idea to check if the difference between the PyTorch     and ONNX model is within a certain threshold. Implement a simple function that loads the model using PyTorch     and ONNX and checks if the difference between the two models is within a certain threshold.</p> Solution <p>The function below should work for a neural network which takes in a single input tensor and returns a single output tensor. If this is not the case, you will need to modify the function to fit your model.</p> onnx_check.py<pre><code>import torch\n\ndef check_onnx_model(\n    onnx_model_file: str,\n    pytorch_model: torch.nn.Module,\n    random_input: torch.Tensor,\n    rtol: float = 1e-03,\n    atol: float = 1e-05,\n) -&gt; None:\n    import onnxruntime as rt\n    import numpy as np\n\n    ort_session = rt.InferenceSession(onnx_model_file)\n    ort_inputs = {ort_session.get_inputs()[0].name: random_input.numpy()}\n    ort_outs = ort_session.run(None, ort_inputs)\n    pytorch_outs = pytorch_model(random_input).detach().numpy()\n\n    assert np.allclose(ort_outs[0], pytorch_outs, rtol=rtol, atol=atol)\n</code></pre> </li> </ol> </li> <li> <p>As mentioned in the introduction, ONNX is able to run on many different types of hardware and execution engines.     You can check all the providers and all the available providers by running the following code:</p> <pre><code>import onnxruntime\nprint(onnxruntime.get_all_providers())\nprint(onnxruntime.get_available_providers())\n</code></pre> <p>Can you figure out how to set which provider the ONNX runtime should use?</p> Solution <p>The provider that the ONNX runtime should use can be set by passing the <code>providers</code> argument to the <code>InferenceSession</code> class. A list should be provided, which prioritizes the providers in the order they are listed.</p> <pre><code>import onnxruntime as rt\nprovider_list = ['CUDAExecutionProvider', 'CPUExecutionProvider']\nort_session = rt.InferenceSession(\"&lt;path-to-model&gt;\", providers=provider_list)\n</code></pre> <p>In this case we will prefer CUDA Execution Provider over CPU Execution Provider if both are available.</p> </li> <li> <p>As you have probably realized in the exercises on docker, it can take a long time     to build the kind of containers we are working with and they can be quite large. There is a reason for this and that     is that PyTorch is a very large framework with a lot of dependencies. ONNX on the other hand is a much smaller     framework. This kind of makes sense, because PyTorch is a framework that primarily was designed for developing, e.g.     training models, while ONNX is a framework that is designed for serving models. Let's try to quantify this.</p> <ol> <li> <p>Construct a dockerfile that builds a docker image with PyTorch as a dependency. The dockerfile does not actually     need to run anything. Repeat the same process for the ONNX runtime. Bonus point for developing a docker     image that takes a build arg at build time that specifies     if the image should be built with CUDA support or not.</p> Solution <p>The dockerfile for the PyTorch image could look something like this</p> inference_pytorch.dockerfile<pre><code>FROM python:3.11-slim\n\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nARG CUDA\nENV CUDA=${CUDA}\nRUN echo \"CUDA is set to: ${CUDA}\"\n\nRUN echo \"CUDA is set to: ${CUDA}\" &amp;&amp; \\\n    if [ -n \"$CUDA\" ]; then \\\n        pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cu121; \\\n    else \\\n        pip install --no-cache-dir torch --index-url https://download.pytorch.org/whl/cpu; \\\n    fi\n</code></pre> <p>and the dockerfile for the ONNX image could look something like this</p> inference_onnx.dockerfile<pre><code>FROM python:3.11-slim\n\nRUN apt update &amp;&amp; \\\n    apt install --no-install-recommends -y build-essential gcc &amp;&amp; \\\n    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN echo \"CUDA is set to: ${CUDA}\" &amp;&amp; \\\n    if [ -n \"$CUDA\" ]; then \\\n        pip install onnxruntime-gpu; \\\n    else \\\n        pip install onnxruntime; \\\n    fi\n</code></pre> </li> <li> <p>Build both containers and measure the time it takes to build them. How much faster is it to build the ONNX     container compared to the PyTorch container?</p> Solution <p>On unix/linux you can use the time command to measure the time it takes to build the containers. Building both images, with and without CUDA support, can be done with the following commands:</p> <pre><code>time docker build . -t pytorch_inference_cuda:latest -f inference_pytorch.dockerfile \\\n    --no-cache --build-arg CUDA=true\ntime docker build . -t pytorch_inference:latest -f inference_pytorch.dockerfile \\\n    --no-cache --build-arg CUDA=\ntime docker build . -t onnx_inference_cuda:latest -f inference_onnx.dockerfile \\\n    --no-cache --build-arg CUDA=true\ntime docker build . -t onnx_inference:latest -f inference_onnx.dockerfile \\\n    --no-cache --build-arg CUDA=\n</code></pre> <p>The <code>--no-cache</code> flag is used to ensure that the build process is not cached and ensures a fair comparison. On my laptop this respectively took <code>5m1s</code>, <code>1m4s</code>, <code>0m4s</code>, <code>0m50s</code> meaning that the ONNX container was respectively 7x (with CUDA) and 1.28x (no CUDA) faster to build than the PyTorch container.</p> </li> <li> <p>Figure out the sizes of the two docker images. This can be done in the terminal by running the <code>docker images</code>     command. How much smaller is the ONNX model compared to the PyTorch model?</p> Solution <p>As of writing, the docker image containing the PyTorch framework was 5.54GB (with CUDA) and 1.25GB (no CUDA). In comparison the ONNX image was 647MB (with CUDA) and 647MB (no CUDA). This means that the ONNX image is respectively 8.5x (with CUDA) and 1.94x (no CUDA) smaller than the PyTorch image.</p> </li> </ol> </li> <li> <p>(Optional) Assuming you have completed the module on FastAPI, try creating a small     FastAPI application that serves a model using the ONNX runtime.</p> Solution <p>Here is a simple example of how to create a FastAPI application that serves a model using the ONNX runtime.</p> onnx_fastapi.py<pre><code>import numpy as np\nimport onnxruntime\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/predict\")\ndef predict():\n    \"\"\"Predict using ONNX model.\"\"\"\n    # Load the ONNX model\n    model = onnxruntime.InferenceSession(\"model.onnx\")\n\n    # Prepare the input data\n    input_data = {\"input\": np.random.rand(1, 3).astype(np.float32)}\n\n    # Run the model\n    output = model.run(None, input_data)\n\n    return {\"output\": output[0].tolist()}\n</code></pre> </li> </ol> <p>This completes the exercises on the ONNX format. Do note that one limitation of the ONNX format is that it is based on ProtoBuf, which is a binary format. A protobuf file can have a maximum size of 2GB, which means that the <code>.onnx</code> format is not enough for very large models. However, through the use of external data it is possible to circumvent this limitation.</p>"},{"location":"s7_deployment/ml_deployment/#bentoml","title":"BentoML","text":"<p>BentoML cloud vs BentoML OSS</p> <p>We are only going to be looking at the open-source version of BentoML in this module. However, BentoML also has a cloud version that makes it very easy to deploy models that are coded in BentoML to the cloud. If you are interested in this, you can check out the BentoML cloud documentation. This business strategy of having an open-source product and a cloud product is very common in the machine learning space (HuggingFace, LightningAI, Weights and Biases, etc.), because it allows companies to make money from the cloud product while still providing a free product to the community.</p> <p>BentoML is a framework that is designed to make it easy to serve machine learning models. It is designed to be backend agnostic, meaning that it can be used with any computational backend. It is also model agnostic, meaning that it can be used with any machine learning model.</p> <p>Let's consider a simple example of how to serve a model using BentoML. The following code snippet shows how to serve a model that uses the <code>transformers</code> library to summarize text.</p> <pre><code>import bentoml\nfrom transformers import pipeline\n\nEXAMPLE_INPUT = (\n    \"Breaking News: In an astonishing turn of events, the small town of Willow Creek has been taken by storm as \"\n    \"local resident Jerry Thompson's cat, Whiskers, performed what witnesses are calling a 'miraculous and gravity-\"\n    \"defying leap.' Eyewitnesses report that Whiskers, an otherwise unremarkable tabby cat, jumped a record-breaking \"\n    \"20 feet into the air to catch a fly. The event, which took place in Thompson's backyard, is now being investigated \"\n    \"by scientists for potential breaches in the laws of physics. Local authorities are considering a town festival to \"\n    \"celebrate what is being hailed as 'The Leap of the Century.'\"\n)\n\n@bentoml.service(resources={\"cpu\": \"2\"}, traffic={\"timeout\": 10})\nclass Summarization:\n    def __init__(self) -&gt; None:\n        self.pipeline = pipeline('summarization')\n\n    @bentoml.api\n    def summarize(self, text: str = EXAMPLE_INPUT) -&gt; str:\n        result = self.pipeline(text)\n        return result[0]['summary_text']\n</code></pre> <p>In <code>BentoML</code> we organize our services in classes, where each class is a service that we want to serve. The two important parts of the code snippet are the <code>@bentoml.service</code> and <code>@bentoml.api</code> decorators.</p> <ul> <li> <p>The <code>@bentoml.service</code> decorator is used to specify the resources that the service should use and in general how the     service should be run. In this case we are specifying that the service should use 2 CPU cores and that the timeout     for the service should be 10 seconds.</p> </li> <li> <p>The <code>@bentoml.api</code> decorator is used to specify the API that the service should expose. In this case we are specifying     that the service should have an API called <code>summarize</code> that takes a string as input and returns a string as output.</p> </li> </ul> <p>To serve the model using <code>BentoML</code> we can execute the following command, which is very similar to the command we used to serve the model using FastAPI.</p> <pre><code>bentoml serve service:Summarization\n</code></pre>"},{"location":"s7_deployment/ml_deployment/#exercises_1","title":"\u2754 Exercises","text":"<p>In general, we recommend looking through the docs for Bento ML if you need help with any of the exercises. We are going to assume that you have done the exercises on ONNX and we are therefore going to be using <code>BentoML</code> to serve ONNX models. If you have not done that part, you can still follow along but you will need to use a PyTorch model instead of an ONNX model.</p> <ol> <li> <p>Install BentoML.</p> <pre><code>pip install bentoml\n</code></pre> <p>Remember to add the dependency to your <code>requirements.txt</code> file.</p> </li> <li> <p>You are in principal free to serve any model you like, but we recommend just using a     torchvision model as in the ONNX exercises. Write your first service     in <code>BentoML</code> that serves a model of your choice. We recommend experimenting with providing     input/output as tensors because bentoml supports this     natively. Secondly, write a client that can send a request to the service and print the result. Here we recommend     using the built-in bentoml.SyncHTTPClient.</p> Solution <p>The following implements a simple BentoML service that serves an ONNX resnet18 model. The service expects both the input and output to be numpy arrays.</p> bentoml_service.py<pre><code>from __future__ import annotations\n\nimport bentoml\nimport numpy as np\nfrom onnxruntime import InferenceSession\n\n\n@bentoml.service\nclass ImageClassifierService:\n    \"\"\"Image classifier service using ONNX model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.model = InferenceSession(\"model.onnx\")\n\n    @bentoml.api\n    def predict(self, image: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict the class of the input image.\"\"\"\n        output = self.model.run(None, {\"input\": image.astype(np.float32)})\n        return output[0]\n</code></pre> <p>The service can be served using the following command:</p> <pre><code>bentoml serve bentoml_service:ImageClassifierService\n</code></pre> <p>To test that the service works the following client can be used:</p> bentoml_client.py<pre><code>import bentoml\nimport numpy as np\nfrom PIL import Image\n\nif __name__ == \"__main__\":\n    image = Image.open(\"my_cat.jpg\")\n    image = image.resize((224, 224))  # Resize to match the minimum input size of the model\n    image = np.array(image)\n    image = np.transpose(image, (2, 0, 1))  # Change to CHW format\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n\n    with bentoml.SyncHTTPClient(\"http://localhost:4040\") as client:\n        resp = client.predict(image=image)\n        print(resp)\n</code></pre> </li> <li> <p>We are now going to look at features where <code>BentoML</code> really sets itself apart from <code>FastAPI</code>. The first is     adaptive batching. As you are hopefully aware, modern machine learning models can process multiple samples at the     same time and in doing so increase the throughput of the model. When we train a model we often set a fixed     batch size, however we cannot do that when serving the model because that would mean that we would have to wait for     the batch to be full before we can process it. Adaptive batching simply refers to the process where we specify a     maximum batch size and also a timeout. When either the batch is full or the timeout is reached, however many     samples we have collected are sent to the model for processing. This can be a very powerful feature because it     allows us to process samples as soon as they arrive, while still taking advantage of the increased throughput of     batching.</p> <p>  The overall architecture of the adaptive batching feature in BentoML. The feature is implemented on the server side and mainly consists of a dispatcher that is in charge of collecting requests and sending them to the model server when either the batch is full or a timeout is reached.  Image credit  </p> <ol> <li> <p>Look through the     documentation on adaptive batching and     add adaptive batching to your service from the previous exercise. Make sure your service works as expected by     testing it with the client from the previous exercise.</p> Solution bentoml_service_adaptive_batching.py<pre><code>from __future__ import annotations\n\nimport bentoml\nimport numpy as np\nfrom onnxruntime import InferenceSession\n\n\n@bentoml.service\nclass ImageClassifierService:\n    \"\"\"Image classifier service using ONNX model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.model = InferenceSession(\"model.onnx\")\n\n    @bentoml.api(\n        batchable=True,\n        batch_dim=(0, 0),\n        max_batch_size=128,\n        max_latency_ms=1000,\n    )\n    def predict(self, image: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict the class of the input image.\"\"\"\n        output = self.model.run(None, {\"input\": image.astype(np.float32)})\n        return output[0]\n</code></pre> </li> <li> <p>Try to measure the throughput of your model with and without adaptive batching. Assuming that you have completed     the module on testing APIs and therefore are familiar with the <code>locust</code> framework, we     recommend that you write a simple locustfile and use the <code>locust</code> command to measure the throughput of your     model.</p> Solution <p>The following locust file can be used to measure the throughput of the model with and without adaptive batching</p> locustfile.py<pre><code>import numpy as np\nfrom locust import HttpUser, between, task\nfrom PIL import Image\n\n\ndef prepare_image():\n    \"\"\"Load and preprocess the image as required.\"\"\"\n    image = Image.open(\"my_cat.jpg\")\n    image = image.resize((224, 224))\n    image = np.array(image)\n    image = np.transpose(image, (2, 0, 1))  # Convert to CHW format\n    image = np.expand_dims(image, axis=0)  # Add batch dimension\n    # Convert to list format for JSON serialization\n    return image.tolist()\n\n\nimage = prepare_image()\n\n\nclass BentoMLUser(HttpUser):\n    \"\"\"Locust user class for sending prediction requests to the server.\"\"\"\n\n    wait_time = between(1, 2)\n\n    @task\n    def send_prediction_request(self):\n        \"\"\"Send a prediction request to the server.\"\"\"\n        payload = {\"image\": image}  # Package the image as JSON\n        self.client.post(\"/predict\", json=payload, headers={\"Content-Type\": \"application/json\"})\n</code></pre> <p>and then the following command can be used to measure the throughput of the model</p> <pre><code>locust -f locustfile_bentoml.py --host http://localhost:4040 --headless -u 50 -t 60s\n</code></pre> <p>You should hopefully see that the throughput of the model is higher when adaptive batching is enabled, but the speedup is largely dependent on the model you are running, the configuration of the adaptive batching and the hardware you are running on.</p> <p>On my laptop I saw about a 1.5 - 2x speedup when adaptive batching was enabled.</p> </li> </ol> </li> <li> <p>(Optional, requires GPU) Look through the     documentation for inference on GPU and add this to     your service. Check that your service works as expected by testing it with the client from the previous exercise and     make sure you are seeing a speedup when running on the GPU.</p> Solution <p>A simple change to the <code>bento.service</code> decorator is all that is needed to run the model on the GPU.</p> <p>```python @bentoml.service(resources={\"gpu\": 1}) class MyService:     def init(self):         self.model = torch.load('model.pth').to('cuda:0')</p> </li> <li> <p>Another way to speed up the inference is to just use multiple workers. This duplicates the server over multiple     processes taking advantage of modern multi-core CPUs. This is similar to running the <code>uvicorn</code> command with the     <code>--workers</code> flag for FastAPI applications. Implement multiple workers in your service and test that it works as     expected by testing it with the client from the previous exercise. Also test that you are seeing a speedup when     running with multiple workers.</p> Solution <p>Multiple workers can be added to the <code>bento.service</code> decorator as shown below.</p> <pre><code>@bentoml.service(workers=4)\nclass MyService:\n    # Service implementation\n</code></pre> <p>Alternatively, you can set <code>workers=\"cpu_count\"</code> to use all available CPU cores. The speedup depends on the model you are serving, the hardware you are running on and the number of workers you are using, but it should be higher than using a single worker.</p> </li> <li> <p>In addition to increasing the throughput of your deployments <code>BentoML</code> can also help with ML applications that     require some kind of composition of multiple models. It is very normal in production setups to have multiple models     that either</p> <ul> <li>Run in a sequence, e.g., the output of one model is the input of another model. You may have a preprocessing     service that preprocesses the data before it is sent to a model that makes a prediction.</li> <li>Run concurrently, e.g., you have multiple models that are run at the same time and the outputs of all the models     are combined to make a prediction. Ensemble models are a good example of this.</li> </ul> <p><code>BentoML</code> makes it easy to compose multiple models together.</p> <ol> <li> <p>Implement two services that run in a sequence, e.g., the output of one service is used as the input to another     service. As an example you can implement either some pre- or post-processing service that is used in conjunction     with the model you have implemented in the previous exercises.</p> Solution <p>The following code snippet shows how to implement two services that run in sequence.</p> bentoml_service_composition.py<pre><code>from __future__ import annotations\n\nfrom pathlib import Path\n\nimport bentoml\nimport numpy as np\nfrom onnxruntime import InferenceSession\nfrom PIL import Image\n\n\n@bentoml.service\nclass ImagePreprocessorService:\n    \"\"\"Image preprocessor service.\"\"\"\n\n    @bentoml.api\n    def preprocess(self, image_file: Path) -&gt; np.ndarray:\n        \"\"\"Preprocess the input image.\"\"\"\n        image = Image.open(image_file)\n        image = image.resize((224, 224))\n        image = np.array(image)\n        image = np.transpose(image, (2, 0, 1))\n        return np.expand_dims(image, axis=0)\n\n\n@bentoml.service\nclass ImageClassifierService:\n    \"\"\"Image classifier service using ONNX model.\"\"\"\n\n    preprocessing_service = bentoml.depends(ImagePreprocessorService)\n\n    def __init__(self) -&gt; None:\n        self.model = InferenceSession(\"model.onnx\")\n\n    @bentoml.api\n    async def predict(self, image_file: Path) -&gt; np.ndarray:\n        \"\"\"Predict the class of the input image.\"\"\"\n        image = await self.preprocessing_service.to_async.preprocess(image_file)\n        output = self.model.run(None, {\"input\": image.astype(np.float32)})\n        return output[0]\n</code></pre> </li> <li> <p>Implement three services, where two of them run concurrently and the outputs of both services are combined in the     third service to make a prediction. As an example you can expand your previous service to serve two different     models and then implement a third service that combines the outputs of both models to make a prediction.</p> Solution <p>The following code snippet shows how to implement a service that consists of two concurrent services. The example assumes that two models called <code>model_a.onnx</code> and <code>model_b.onnx</code> are available.</p> bentoml_service_composition.py<pre><code>from __future__ import annotations\n\nimport asyncio\n\nimport bentoml\nimport numpy as np\nfrom onnxruntime import InferenceSession\n\n\n@bentoml.service\nclass ImageClassifierServiceModelA:\n    \"\"\"Image classifier service using ONNX model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.model = InferenceSession(\"model_a.onnx\")\n\n    @bentoml.api\n    def predict(self, image: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict the class of the input image.\"\"\"\n        output = self.model.run(None, {\"input\": image.astype(np.float32)})\n        return output[0]\n\n\n@bentoml.service\nclass ImageClassifierServiceModelB:\n    \"\"\"Image classifier service using ONNX model.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.model = InferenceSession(\"model_b.onnx\")\n\n    @bentoml.api\n    def predict(self, image: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict the class of the input image.\"\"\"\n        output = self.model.run(None, {\"input\": image.astype(np.float32)})\n        return output[0]\n\n\n@bentoml.service\nclass ImageClassifierService:\n    \"\"\"Image classifier service using ONNX model.\"\"\"\n\n    model_a = bentoml.depends(ImageClassifierServiceModelA)\n    model_b = bentoml.depends(ImageClassifierServiceModelB)\n\n    @bentoml.api\n    async def predict(self, image: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict the class of the input image.\"\"\"\n        result_a, result_b = await asyncio.gather(\n            self.model_a.to_async.predict(image), self.model_b.to_async.predict(image)\n        )\n        return (result_a + result_b) / 2\n</code></pre> </li> <li> <p>(Optional) Implement a server that consists of both sequential and concurrent services.</p> </li> </ol> </li> <li> <p>Similar to deploying a FastAPI application to the cloud, deploying a <code>BentoML</code> framework to the cloud     often requires you to first containerize the application. Because <code>BentoML</code> is designed to be easy to use for even     users not that familiar with Docker, it introduces the concept of a <code>bentofile</code>. A <code>bentofile</code> is a file that     specifies how the container should be built. Below is an example of how a <code>bentofile</code> could look.</p> <pre><code>service: 'service:Summarization'\nlabels:\n  owner: bentoml-team\n  project: gallery\ninclude:\n  - '*.py'\npython:\n  packages:\n    - torch\n    - transformers\n</code></pre> <p>This can then be used to build a <code>bento</code> using the following command:</p> <pre><code>bentoml build\n</code></pre> <p>A <code>bento</code> is not a docker image, but it can be used to build a docker image with the following command:</p> <pre><code>bentoml containerize summarization:latest\n</code></pre> <ol> <li> <p>Can you figure out how the different parts of the <code>bentofile</code> are used to build the docker image? Additionally,     can you figure out from the source repository how the <code>bentofile</code> is     used to build the docker image?</p> Solution <p>The <code>service</code> part specifies both what the container should be called and also what service it should serve, e.g., the last statement in the corresponding dockerfile is <code>CMD [\"bentoml\", \"serve\", \"service:Summarization\"]</code>. The <code>labels</code> part is used to specify labels about the container, see this link for more info. The <code>include</code> part corresponds to <code>COPY</code> statements in the dockerfile and finally the <code>python</code> part is used to specify what python packages should be installed in the container which corresponds to <code>RUN pip install ...</code> in the dockerfile.</p> <p>Regarding how the <code>bentofile</code> is used to build the docker image, the <code>bentoml</code> package contains a number of templates (written using the jinja2 templating language) that are used to generate the dockerfiles. The templates can be found here.</p> </li> <li> <p>Take any service from the previous exercises and try to containerize it. You are free to either write a     <code>bentofile</code> or a <code>dockerfile</code> to do this.</p> Solution <p>The following <code>bentofile</code> can be used to containerize the very first service we implemented in this set of exercises.</p> <pre><code>service: 'bentoml_service:ImageClassifierService'\nlabels:\n  owner: bentoml-team\n  project: gallery\ninclude:\n- 'bentoml_service.py'\n- 'model.onnx'\npython:\n  packages:\n    - onnxruntime\n    - numpy\n</code></pre> <p>The corresponding dockerfile would look something like this:</p> <pre><code>FROM python:3.11-slim\nWORKDIR /bento\nCOPY bentoml_service.py .\nCOPY model.onnx .\nRUN pip install onnxruntime numpy bentoml\nCMD [\"bentoml\", \"serve\", \"bentoml_service:ImageClassifierService\"]\n</code></pre> </li> <li> <p>Deploy the container to GCP Run and test that it works.</p> Solution <p>The following command can be used to deploy the container to GCP Run. We assume that you have already built the container and called it <code>bentoml_service:latest</code></p> <pre><code>docker tag bentoml_service:latest \\\n    &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository-name&gt;/bentoml_service:latest\ndocker push &lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository-name&gt;/bentoml_service:latest\ngcloud run deploy bentoml-service \\\n    --image=&lt;region&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repository-name&gt;/bentoml_service:latest \\\n    --platform managed \\\n    --port 3000  # default used by BentoML\n</code></pre> <p>where <code>&lt;project-id&gt;</code> should be replaced with the id of the project you are deploying to. The service should now be available at the URL that is printed in the terminal.</p> </li> </ol> </li> </ol> <p>This completes the exercises on the <code>BentoML</code> framework. If you want to deep dive more into this we recommend looking into their tasks feature for use cases that have a very long running time and built-in model management feature to unify the way models are loaded, managed and served.</p>"},{"location":"s7_deployment/ml_deployment/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>How would you export a <code>scikit-learn</code> model to ONNX? What method is exported when you export a <code>scikit-learn</code> model to     ONNX?</p> Solution <p>It is possible to export a <code>scikit-learn</code> model to ONNX using the <code>sklearn-onnx</code> package. The following code snippet shows how to export a <code>scikit-learn</code> model to ONNX.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom skl2onnx import to_onnx\nmodel = RandomForestClassifier(n_estimators=2)\ndummy_input = np.random.randn(1, 4)\nonx = to_onnx(model, dummy_input)\nwith open(\"model.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n</code></pre> <p>The method that is exported when you export a <code>scikit-learn</code> model to ONNX is the <code>predict</code> method.</p> </li> <li> <p>In your own words, describe what the concept of computational graph means.</p> Solution <p>A computational graph is a way to represent the mathematical operations that are performed in a model. It is essentially a graph where the nodes are the operations and the edges are the data that is passed between them. The computational graph normally represents the forward pass of the model and is the reason that we can easily backpropagate through the model to train it, because the graph contains all the necessary information to calculate the gradients of the model.</p> </li> <li> <p>In your own words, explain why fusing operations together in the computational graph often leads to better     performance.</p> Solution <p>Each time we want to do a computation, the data needs to be loaded from memory into the CPU/GPU. This is a slow process and the more operations we have, the more times we need to load the data. By fusing operations together, we can reduce the number of times we need to load the data, because we can do multiple operations on the same data before we need to load new data.</p> </li> </ol> <p>This ends the module on tools specifically designed for serving machine learning models. As stated in the beginning of the module, there are a lot of different tools that can be used to serve machine learning models and the choice of tool often depends on the specific use case. In general, we recommend that whenever you want to serve a machine learning model, you try out a few different frameworks and see which one fits your use case best.</p>"},{"location":"s7_deployment/testing_apis/","title":"M24 - API Testing","text":""},{"location":"s7_deployment/testing_apis/#api-testing","title":"API testing","text":"<p>Core Module</p> <p>API testing, similar to unit testing, is a type of software testing that involves testing the application programming interface (API) directly to ensure it meets requirements for functionality, reliability, performance, and security. The core difference from the unit testing we have been implementing until now is that instead of testing the individual functions, we are testing the entire API as a whole. API testing is therefore a form of integration testing. Additionally, another difference is that we need to simulate API calls that should be as similar as possible to the ones that will be made by the users of the API.</p> <p>There are in general two things that we want to test when working with APIs:</p> <ul> <li>Does the API work as intended? E.g. for a given input, does it return the expected output?</li> <li>Can the API handle the expected load? E.g. if we send 1,000 requests per second, does it crash?</li> </ul> <p>In this module, we go over how to do each of them.</p>"},{"location":"s7_deployment/testing_apis/#testing-for-functionality","title":"Testing for functionality","text":"<p>Similar to when we wrote unit tests for our code back in this module, we can also write tests for our API that check that our code does what it is supposed to do, e.g., by using <code>assert</code> statements. As always we recommend implementing the tests in a separate folder called <code>tests</code>, but we recommend that you add further subfolders to separate the different types of tests. For example, for the type of machine learning projects and APIs we have been working with in this course:</p> <pre><code>my_project\n|-- src/\n|   | &lt;project-name&gt;/\n|   |   |-- train.py\n|   |   |-- data.py\n|   |   |-- api.py\n|-- tests/\n|   |-- unittests/\n|   |   |-- test_train.py\n|   |   |-- test_data.py\n|   |-- integrationtests/\n|   |   |-- test_apis.py\n</code></pre>"},{"location":"s7_deployment/testing_apis/#exercises","title":"\u2754 Exercises","text":"<p>In these exercises, we are going to assume that we want to test an API written in FastAPI (see this module). If the API is written in a different framework then how to write the tests may have to change.</p> <ol> <li> <p>Start by installing httpx which is the client we are going to use during testing:</p> <pre><code>pip install httpx\n</code></pre> <p>Remember to add it to your <code>requirements.txt</code> file.</p> </li> <li> <p>If you have already done the module on unittesting then you should     already have a <code>tests/</code> folder. If not then create one. Inside the <code>tests/</code> folder create a new folder called     <code>integrationtests/</code>. Inside the <code>integrationtests/</code> folder create a file called <code>test_apis.py</code> and write the     following code:</p> <pre><code>from fastapi.testclient import TestClient\nfrom &lt;project-name&gt;.api import app\nclient = TestClient(app)\n</code></pre> <p>This code will create a client that can be used to send requests to the API. The <code>app</code> variable is the FastAPI application that we want to test.</p> </li> <li> <p>Now, you can write tests that check that the API works as intended, much like you would write unit tests. For     example, if you have a root endpoint that just returns a simple welcome message you could write a test like this:</p> <pre><code>def test_read_root(model):\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert response.json() == {\"message\": \"Welcome to the MNIST model inference API!\"}\n</code></pre> <p>Make sure to always <code>assert</code> that the status code is what you expect and that the response is what you expect. Add such tests for all the endpoints in your API.</p> Application with lifespans <p>If you have an application with lifespan events, i.e. you have implemented the <code>lifespan</code> function in your FastAPI application, you need to instead use the <code>TestClient</code> in a <code>with</code> statement. This is because the <code>TestClient</code> will close the connection to the application after the test is done. Here is an example:</p> <pre><code>def test_read_root(model):\n    with TestClient(app) as client:\n        response = client.get(\"/\")\n        assert response.status_code == 200\n        assert response.json() == {\"message\": \"Welcome to the MNIST model inference API!\"}\n</code></pre> </li> <li> <p>To run the tests, you can use the following command:</p> <pre><code>pytest tests/integrationtests/test_apis.py\n</code></pre> <p>Make sure that all your tests pass.</p> </li> </ol>"},{"location":"s7_deployment/testing_apis/#load-testing","title":"Load testing","text":"<p>The next type of testing we are going to implement for our application is load testing, which is a kind of performance testing. The goal of load testing is to determine how an application behaves under both normal and peak conditions. The purpose is to identify the maximum operating capacity of an application as well as any bottlenecks and to determine which element is causing degradation.</p> <p>Before we get started on the exercises we recommend that you start by defining an environment variable that contains the endpoint of your API, i.e., we need the API running to be able to test it. To begin with, you can just run the API locally, thus in a terminal window run the following command:</p> <pre><code>uvicorn app.main:app --reload\n</code></pre> <p>By default the API will run on <code>http://localhost:8000</code> which we can then define as an environment variable:</p> WindowsMac/Linux <pre><code>set MYENDPOINT=http://localhost:8000\n</code></pre> <pre><code>export MYENDPOINT=http://localhost:8000\n</code></pre> <p>However, the end goal is to test an API you have deployed in the cloud. If you have used Google Cloud Run to deploy your API then you can get the endpoint by going to the UI and looking at the service details:</p> <p></p>  The endpoint can be seen in the top center. It always starts with `https://` followed by a random string and then `.a.run.app`  <p>However, we can also use the <code>gcloud</code> command to get the endpoint:</p> WindowsMac/Linux <pre><code>for /f \"delims=\" %i in ^\n('gcloud run services describe &lt;name&gt; --region=&lt;region&gt; --format=\"value(status.url)\"') do set MYENDPOINT=%i\n</code></pre> <pre><code>export MYENDPOINT=$(gcloud run services describe &lt;name&gt; --region=&lt;region&gt; --format=\"value(status.url)\")\n</code></pre> <p>where you need to define <code>&lt;name&gt;</code> and <code>&lt;region&gt;</code> with the name of your service and the region it is deployed in.</p>"},{"location":"s7_deployment/testing_apis/#exercises_1","title":"\u2754 Exercises","text":"<p>For the exercises, we are going to use the locust framework for load testing (the name is a reference to a locust being a swarm of bugs invading your application). It is a Python framework that allows you to write tests that simulate many users interacting with your application. It is very easy to get started with and it is very easy to integrate into your CI/CD pipeline.</p> <ol> <li> <p>Install <code>locust</code></p> <pre><code>pip install locust\n</code></pre> <p>Remember to add it to your <code>requirements.txt</code> file.</p> </li> <li> <p>Make sure you have written an API that you can test. Otherwise you can for simplicity just use this simple example:</p> <p>Simple hallo world Fastapi example</p> model.py<pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    \"\"\"Root endpoint.\"\"\"\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int):\n    \"\"\"Get an item by id.\"\"\"\n    return {\"item_id\": item_id}\n</code></pre> </li> <li> <p>Add a new folder to your <code>tests/</code> folder called <code>performancetests</code> and inside it create a file called     <code>locustfile.py</code>. To that file, you need to add the appropriate code to simulate the users that you want to test.     You can read more about how to write a <code>locustfile.py</code> here.</p> Solution <p>Here we provide a solution to the above simple example:</p> locustfile.py<pre><code>import random\n\nfrom locust import HttpUser, between, task\n\n\nclass MyUser(HttpUser):\n    \"\"\"A simple Locust user class that defines the tasks to be performed by the users.\"\"\"\n\n    wait_time = between(1, 2)\n\n    @task\n    def get_root(self) -&gt; None:\n        \"\"\"A task that simulates a user visiting the root URL of the FastAPI app.\"\"\"\n        self.client.get(\"/\")\n\n    @task(3)\n    def get_item(self) -&gt; None:\n        \"\"\"A task that simulates a user visiting a random item URL of the FastAPI app.\"\"\"\n        item_id = random.randint(1, 10)\n        self.client.get(f\"/items/{item_id}\")\n</code></pre> </li> <li> <p>Then try to run the <code>locust</code> command:</p> <pre><code>locust -f tests/performancetests/locustfile.py\n</code></pre> <p>and then navigate to http://localhost:8089 in your web browser. You should see a page that looks similar to the top of this figure.</p> <p> </p> <p>You can here define the number of users you want to simulate and how many users you want to spawn per second. Finally, you can define which endpoint you want to test. When you are ready you can press <code>Start</code>.</p> <p>Afterward, you should see the results of the test in the web browser. Answer the following questions:</p> <ul> <li>What is the average response time of your API?</li> <li>What is the 99th percentile response time of your API?</li> <li>How many requests per second can your API handle?</li> </ul> </li> <li> <p>Maybe of more use to us is running locust in the terminal. To do this you can run the following command:</p> WindowsMac/Linux <pre><code>locust -f tests/performancetests/locustfile.py \\\n    --headless --users 10 --spawn-rate 1 --run-time 1m --host %MYENDPOINT%\n</code></pre> <pre><code>locust -f tests/performancetests/locustfile.py \\\n    --headless --users 10 --spawn-rate 1 --run-time 1m --host $MYENDPOINT\n</code></pre> <p>This will run the test with 10 users that are spawned at a rate of 1 per second for 1 minute.</p> </li> <li> <p>(Optional) A good use case for load testing in our case is to test that our API can handle a load right after it     has been deployed. To do this we need to add appropriate steps to our CI/CD pipeline. Try adding locust to an     existing or new workflow file in your <code>.github/workflows/</code> folder, such that it runs after the deployment step.</p> Solution <p>The solution here expects that a service called <code>production-model</code> has been deployed to Google Cloud Run. Then the following steps can be added to a workflow file, to first authenticate with Google Cloud, extract the relevant URL, and then run the load test:</p> <pre><code>- name: Auth with GCP\n  uses: google-github-actions/auth@v2\n  with:\n    credentials_json: ${{ secrets.GCP_SA_KEY }}\n\n- name: Set up Cloud SDK\n  uses: google-github-actions/setup-gcloud@v2\n\n- name: Extract deployed model URL\n  run: |\n    DEPLOYED_MODEL_URL=$(gcloud run services describe production-model \\\n      --region=europe-west1 \\\n      --format='value(status.url)')\n    echo \"DEPLOYED_MODEL_URL=$DEPLOYED_MODEL_URL\" &gt;&gt; $GITHUB_ENV\n\n- name: Run load test on deployed model\n  env:\n    DEPLOYED_MODEL_URL: ${{ env.DEPLOYED_MODEL_URL }}\n  run: |\n    locust -f tests/performancetests/locustfile.py \\\n      --headless -u 100 -r 10 --run-time 10m --host=$DEPLOYED_MODEL_URL --csv=/locust/results\n\n- name: Upload locust results\n  uses: actions/upload-artifact@v4\n  with:\n    name: locust-results\n    path: /locust\n</code></pre> <p>The results can then be downloaded from the artifacts tab in the GitHub UI.</p> </li> </ol>"},{"location":"s7_deployment/testing_apis/#knowledge-check","title":"\ud83e\udde0 Knowledge check","text":"<ol> <li> <p>In the <code>locust</code> framework, what does the <code>@task</code> decorator do and what does <code>@task(3)</code> mean?</p> Solution <p>The <code>@task</code> decorator is used to define a task that a user can perform. The <code>@task(3)</code> decorator is used to define a task that a user can perform that is three times more likely to be performed than the other tasks.</p> </li> <li> <p>In the <code>locust</code> framework, what does the <code>wait_time</code> attribute do?</p> Solution <p>The <code>wait_time</code> attribute is used to define how long a user should wait between tasks. It can be either be a fixed number or a random number between two values.</p> <pre><code>from locust import HttpUser, task, between, constant\n\nclass MyUser(HttpUser):\n    wait_time = between(5, 9)\n    # or\n    wait_time = constant(5)\n</code></pre> </li> <li> <p>Load testing can give numbers on average response time, 99th percentile response time, and requests per second. What     do these numbers tell us about the user experience of the API?</p> Solution <p>The average response time and 99th percentile response time are both measures of how \"snappy\" the API feels to the user. While the average response time is normally considered the most important, the 99th percentile response time is also important as it tells us if there is a small number of users that are experiencing a very slow response time. The requests per second tells us how many users the API can handle at the same time. If this number is too low it can lead to users experiencing slow response times and may indicate that something is wrong with the API.</p> </li> </ol>"},{"location":"s8_monitoring/","title":"Monitoring","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn how to detect data drifting using the <code>evidently</code> framework</p> <p> M27: Data Drifting</p> </li> <li> <p></p> <p>Learn how to setup a prometheus monitoring system for your application</p> <p> M28: System Monitoring</p> </li> </ul> <p>We have now reached the end of our machine-learning pipeline. We have successfully developed, trained and deployed a machine learning model. However, the question then becomes if you can trust that your newly deployed model still works as expected after 1 day without you intervening? What about 1 month? What about 1 year?</p> <p>There may be corner cases where an ML model is working as expected, but the vast majority of ML models will perform worse over time because they do not generalize well enough. For example, assume you have just deployed an application that classifies images from phones when suddenly a new phone comes out with a new kind of sensor that takes images that either have a very weird aspect ratio or something else your model is not robust towards. There is nothing wrong with this; you can essentially just retrain your model on new data that accounts for this corner case. However, you need a mechanism that informs you.</p> <p>This is where monitoring comes into play. Monitoring practices are in charge of collecting any information about your application in some format that can then be analyzed and reacted to. Monitoring is essential to ensuring the longevity of your applications.</p> <p>As with many other sub-fields within MLOps, we can divide monitoring into classic monitoring and ML-specific monitoring. Classic monitoring (known from classic DevOps) is often about</p> <ul> <li>Errors: Is my application working without problems?</li> <li>Logs: What is going on?</li> <li>Performance: How fast is my application?</li> </ul> <p>All this is basic information you are interested in regardless of what application type you are trying to deploy. However, then there is machine learning-related monitoring that especially relates to data. Take the example above, with the new phone, which we would in general consider to be a data drifting problem, i.e., the data you are trying to do inference on has drifted away from the distribution of the data your model was trained on. Such monitoring problems are unique to machine learning applications and need to be handled separately.</p> <p>In this session we are going to see examples of both kinds of monitoring.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand the concept of data drifting in machine learning applications</li> <li>Be able to detect data drifting using the <code>evidently</code> framework</li> <li>Understand the importance of system-level monitoring and be able to conceptually implement it</li> </ul>"},{"location":"s8_monitoring/data_drifting/","title":"M27 - Data Drifting","text":""},{"location":"s8_monitoring/data_drifting/#data-drifting","title":"Data drifting","text":"<p>Core Module</p> <p>Data drifting is one of the core reasons why model accuracy degrades over time in production. For machine learning models, data drift is the change in model input data that leads to model performance degradation. In practical terms, this means that the model is receiving input that is outside the scope that it was trained on, as seen in the figure below. This shows that the underlying distribution of a particular feature has slowly been increasing in value over two years.</p> <p></p>  Image credit  <p>In some cases, it may be that if you normalize some feature in a better way that you are able to generalize your model better, but this is not always the case. The reason for such a drift is commonly some external factor that you essentially have no control over. That really only leaves you with one option: retrain your model on the newly received input features and deploy that model to production. This process is probably going to repeat over the lifetime of your application if you want to keep it up-to-date with the real world.</p> <p></p>  Image credit  <p>We have now come up with a solution to the data drift problem, but there is one important detail that we have not taken care of: When we should actually trigger the retraining? We do not want to wait around for our model performance to degrade, thus we need tools that can detect when we are seeing a drift in our data.</p>"},{"location":"s8_monitoring/data_drifting/#exercises","title":"\u2754 Exercises","text":"<p>For these exercises we are going to use the framework Evidently ,developed by EvidentlyAI. Evidently currently supports detection for both regression and classification models. The exercises are in large part taken from here and in general we recommend if you are in doubt about an exercise to look at the docs for the API and examples (their documentation can be a bit lacking sometimes, so you may also have to dive into the source code).</p> <p>Additionally, we want to stress that data drift detection, concept drift detection, etc. is still an active field of research and so there exist multiple frameworks for doing this kind of detection. In addition to Evidently, we can also mention NannyML, WhyLogs and deepcheck.</p> <ol> <li> <p>Start by installing Evidently.</p> <pre><code>pip install evidently\n</code></pre> <p>You will also need <code>scikit-learn</code> and <code>pandas</code> installed if you do not already have them.</p> </li> <li> <p>Hopefully you have already gone through session S7 on deployment. As part of the     deployment exercises about GCP functions you should have developed an application that can classify the     iris dataset. Your solution should look something like the script     below:</p> Example GCP function for iris classification sklearn_train_function.py<pre><code>import pickle\n\nimport functions_framework\nfrom google.cloud import storage\n\nBUCKET_NAME = \"my_sklearn_model_bucket\"\nMODEL_FILE = \"model.pkl\"\n\nclient = storage.Client()\nbucket = client.get_bucket(BUCKET_NAME)\nblob = bucket.get_blob(MODEL_FILE)\nmy_model = pickle.loads(blob.download_as_string())\n\n\n@functions_framework.http\ndef knn_classifier(request):\n    \"\"\"Simple knn classifier function for iris prediction.\"\"\"\n    request_json = request.get_json()\n    if request_json and \"input_data\" in request_json:\n        input_data = request_json[\"input_data\"]\n        input_data = [float(in_data) for in_data in input_data]\n        input_data = [input_data]\n        prediction = my_model.predict(input_data)\n        return {\"prediction\": prediction.tolist()}\n    return {\"error\": \"No input data provided.\"}\n</code></pre> <p>Start by converting your GCP function into a FastAPI application. The appropriate <code>curl</code> command should look something like this:</p> <pre><code>curl -X 'POST' \\\n    'http://127.0.0.1:8000/iris_v1/?sepal_length=1.0&amp;sepal_width=1.0&amp;petal_length=1.0&amp;petal_width=1.0' \\\n    -H 'accept: application/json' \\\n    -d ''\n</code></pre> <p>and the response body should look like this:</p> <pre><code>{ \"prediction\": \"Iris-Setosa\", \"prediction_int\": 0 }\n</code></pre> Solution iris_fastapi_solution.py<pre><code>import pickle\nfrom collections.abc import Generator\n\nfrom fastapi import FastAPI\n\n\ndef lifespan(app: FastAPI) -&gt; Generator[None]:\n    \"\"\"Load model and classes.\"\"\"\n    global model, classes\n    classes = [\"Iris-Setosa\", \"Iris-Versicolour\", \"Iris-Virginica\"]\n    with open(\"model.pkl\", \"rb\") as file:\n        model = pickle.load(file)\n\n    yield\n\n    del model, classes\n\n\napp = FastAPI(lifespan=lifespan)\n\n\n@app.post(\"/predict\")\ndef iris_inference(sepal_length: float, sepal_width: float, petal_length: float, petal_width: float):\n    \"\"\"Version 1 of the iris inference endpoint.\"\"\"\n    prediction = model.predict([[sepal_length, sepal_width, petal_length, petal_width]])\n    prediction = prediction.item()\n    return {\"prediction\": classes[prediction], \"prediction_int\": prediction}\n</code></pre> <ol> <li> <p>Next we are going to add some functionality to our application. We need to add that the input for the user is     saved to a database whenever our application is called. However, to not slow down the response to our user we     want to implement this as a background task. A background task is a function that should be executed after     the user has gotten their response. Implement a background task that saves the user input to a database implemented     as a simple <code>.csv</code> file. You can read more about background tasks     here. The header of the database should look     something like this:</p> <pre><code>time, sepal_length, sepal_width, petal_length, petal_width, prediction\n2022-12-28 17:24:34.045649, 1.0, 1.0, 1.0, 1.0, 1\n2022-12-28 17:24:44.026432, 2.0, 2.0, 2.0, 2.0, 1\n...\n</code></pre> <p>Thus both input, timestamp and predicted value should be saved.</p> Solution iris_fastapi_solution.py<pre><code>import pickle\nfrom collections.abc import Generator\nfrom datetime import datetime\n\nfrom fastapi import BackgroundTasks, FastAPI\n\n\ndef lifespan(app: FastAPI) -&gt; Generator[None]:\n    \"\"\"Load model and classes, and create database file.\"\"\"\n    global model, classes\n    classes = [\"Iris-Setosa\", \"Iris-Versicolour\", \"Iris-Virginica\"]\n    with open(\"model.pkl\", \"rb\") as file:\n        model = pickle.load(file)\n\n    with open(\"prediction_database.csv\", \"w\") as file:\n        file.write(\"time, sepal_length, sepal_width, petal_length, petal_width, prediction\\n\")\n\n    yield\n\n    del model\n\n\napp = FastAPI(lifespan=lifespan)\n\n\ndef add_to_database(\n    now: str,\n    sepal_length: float,\n    sepal_width: float,\n    petal_length: float,\n    petal_width: float,\n    prediction: int,\n) -&gt; None:\n    \"\"\"Simple function to add prediction to database.\"\"\"\n    with open(\"prediction_database.csv\", \"a\") as file:\n        file.write(f\"{now}, {sepal_length}, {sepal_width}, {petal_length}, {petal_width}, {prediction}\\n\")\n\n\n@app.post(\"/predict\")\nasync def iris_inference(\n    sepal_length: float,\n    sepal_width: float,\n    petal_length: float,\n    petal_width: float,\n    background_tasks: BackgroundTasks,\n):\n    \"\"\"Version 2 of the iris inference endpoint.\"\"\"\n    prediction = model.predict([[sepal_length, sepal_width, petal_length, petal_width]])\n    prediction = prediction.item()\n\n    now = str(datetime.now(tz=datetime.UTC))\n    background_tasks.add_task(add_to_database, now, sepal_length, sepal_width, petal_length, petal_width, prediction)\n    return {\"prediction\": classes[prediction], \"prediction_int\": prediction}\n</code></pre> </li> <li> <p>Call your API a number of times to generate some dummy data in the database.</p> </li> </ol> </li> <li> <p>Create a new <code>data_drift.py</code> file where we are going to implement the data drifting detection and reporting. Start     by adding both the real iris data and your generated dummy data as pandas dataframes.</p> <pre><code>import pandas as pd\nfrom sklearn import datasets\nreference_data = datasets.load_iris(as_frame=True).frame\ncurrent_data = pd.read_csv('prediction_database.csv')\n</code></pre> <p>If done correctly you will most likely end up with two dataframes that look like</p> <pre><code># reference_data\nsepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n0                  5.1               3.5                1.4               0.2       0\n1                  4.9               3.0                1.4               0.2       0\n...\n148                6.2               3.4                5.4               2.3       2\n149                5.9               3.0                5.1               1.8       2\n[150 rows x 5 columns]\n\n# current_data\ntime                         sepal_length   sepal_width   petal_length   petal_width   prediction\n2022-12-28 17:24:34.045649   1.0            1.0            1.0           1.0           1\n...\n2022-12-28 17:24:34.045649   1.0            1.0            1.0           1.0           1\n[10 rows x 5 columns]\n</code></pre> <p>Standardize the dataframes such that they have the same column names and drop the time column from the <code>current_data</code> dataframe.</p> Solution <pre><code>import pandas as pd\nfrom sklearn import datasets\nreference_data = datasets.load_iris(as_frame=True).frame\nreference_data = reference_data.rename(\n    columns={\n        'sepal length (cm)': 'sepal_length',\n        'sepal width (cm)': 'sepal_width',\n        'petal length (cm)': 'petal_length',\n        'petal width (cm)': 'petal_width',\n        'target': 'target'\n    }\n)\n\ncurrent_data = pd.read_csv('prediction_database.csv')\ncurrent_data = current_data.drop(columns=['time'])\n</code></pre> <ol> <li> <p>Add the following code to the <code>data_drift.py</code> file to create a report on the data drift:</p> <pre><code>from evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset\nreport = Report(metrics=[DataDriftPreset()])\nreport.run(reference_data=reference_data, current_data=current_data)\nreport.save_html('report.html')\n</code></pre> <p>Open the generated <code>.html</code> page. What does it say about your data? Has it drifted? Make sure to poke around to understand what the different plots are actually showing.</p> </li> <li> <p>Data drifting is not the only kind of reporting evidently can make. We can also get reports on data quality.     Look through evidently's documentation and add the preset that has to do with data quality to the report.     Try adding a few <code>Nan</code> values to your <code>current_data</code> and re-run the report. Checkout the report and go over the     generated plots and make sure that it picked up on the missing values you just added.</p> Solution <p>The <code>DataQualityPreset</code> checks the quality of the data:</p> <pre><code>from evidently.metric_preset import DataDriftPreset, DataQualityPreset\nreport = Report(metrics=[DataDriftPreset(), DataQualityPreset()])\n</code></pre> </li> <li> <p>Another important kind of drift is called target drift, where the distribution of the target values has     changed. If your training data was balanced, and you are now seeing a lot of one class being predicted this may     indicate that your model is not performing as expected or that external factors have changed, which means that     you should retrain your model. Find the preset that checks for target drift, add it to the report and re-run     the analysis.</p> Solution <p>The <code>TargetDriftPreset</code> checks the distribution of the target values:</p> <pre><code>from evidently.metric_preset import DataDriftPreset, DataQualityPreset, TargetDriftPreset\nreport = Report(metrics=[DataDriftPreset(), DataQualityPreset(), TargetDriftPreset()])\n</code></pre> </li> </ol> </li> <li> <p>Evidently reports are meant for debugging, exploration and reporting of results. However, if we want to integrate     evidently functionality into our already developed pipelines, either as a simple script, as part of a GitHub action     workflow or something else, we need to be able to extract the results in a more programmatic way. This can be done     using their <code>Test</code> and <code>TestSuites</code> classes. Implement a simple test that checks if there are any missing values in     our dataset and print the results to the console.</p> Solution <p>Using the <code>.as_dict()</code> method on a <code>TestSuite</code> we can programmatically extract the results of the test. In particular the returned dictionary contains a key <code>summary</code> that contains a key <code>all_passed</code> that is <code>True</code> if all tests passed and <code>False</code> otherwise.</p> <pre><code>from evidently.test_suite import TestSuite\nfrom evidently.tests import TestNumberOfMissingValues\ndata_test = TestSuite(tests=[TestNumberOfMissingValues()])\ndata_test.run(reference_data=reference_data, current_data=current_data)\nresult = data_test.as_dict()\nprint(result)\nprint(\"All tests passed: \", result['summary']['all_passed'])\n</code></pre> <ol> <li>Take a look at this colab notebook     that contains all the tests implemented in Evidently. Pick 5 tests of your choice, where at least 1 fails by default     and implement them as a <code>TestSuite</code>. Then try changing the arguments of the test so they better fit your     use case and get them all passing.</li> </ol> </li> <li> <p>(Optional) When doing monitoring in practice, we are not always interested in running on all the data collected from our     API, maybe only the last <code>N</code> entries or maybe just from the last hour of observations. Since we are already logging     the timestamps of when our API is called we can use that for filtering. Implement a simple filter that</p> <ul> <li>Takes an integer <code>n</code> and returns the last <code>n</code> entries in our database</li> <li>Takes an integer <code>t</code> that filters out observations older than <code>t</code> hours</li> </ul> Solution <pre><code>import pandas as pd\ndef filter_data(data: pd.Dataframe, n: int | None = None, t: int | None = None) -&gt; pd.Dataframe:\n    if n is not None:\n        return data.tail(n)\n    if t is not None:\n        df['time'] = pd.to_datetime(df['time'])  # Ensure the 'time' column is a datetime object\n        one_hour_ago = datetime.now() - timedelta(hours=t)\n        return df[df['time'] &gt; one_hour_ago]\n    return data\n</code></pre> </li> <li> <p>Evidently by default only supports structured data, e.g., tabular data (so does nearly every other framework). Thus,     the question then becomes how we can extend unstructured data such as images or text? The solution is to extract     structured features from the data which we can then run the analysis on.</p> <ol> <li> <p>For images the simple solution would be to flatten the images and consider each pixel a feature,     however this does not work in practice because changes in the individual pixels do not really tell us anything     about the image. Instead, we should derive some features such as:</p> <ul> <li>Average brightness</li> <li>Contrast of an image</li> <li>Image sharpness</li> <li>...</li> </ul> <p>These are all numbers that can make up a feature vector for a given image. Try doing this yourself, for example by extracting such features from the MNIST and FashionMNIST datasets, and check if you can detect a drift between the two sets.</p> Solution image_drift.py<pre><code>import numpy as np\nimport pandas as pd\nfrom evidently.metrics import DataDriftTable\nfrom evidently.report import Report\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([transforms.ToTensor()])\n\nmnist = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\nfashion_mnist = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n\nmnist_images = mnist.data.numpy()\nfashion_images = fashion_mnist.data.numpy()\n\n\ndef extract_features(images):\n    \"\"\"Extract basic image features from a set of images.\"\"\"\n    features = []\n    for img in images:\n        avg_brightness = np.mean(img)\n        contrast = np.std(img)\n        sharpness = np.mean(np.abs(np.gradient(img)))\n        features.append([avg_brightness, contrast, sharpness])\n    return np.array(features)\n\n\nmnist_features = extract_features(mnist_images)\nfashion_features = extract_features(fashion_images)\n\nfeature_columns = [\"Average Brightness\", \"Contrast\", \"Sharpness\"]\n\nmnist_df = np.column_stack((mnist_features, [\"MNIST\"] * mnist_features.shape[0]))\nfashion_df = np.column_stack((fashion_features, [\"FashionMNIST\"] * fashion_features.shape[0]))\n\ncombined_features = np.vstack((mnist_df, fashion_df))\n\nfeature_df = pd.DataFrame(combined_features, columns=feature_columns + [\"Dataset\"])\nfeature_df[feature_columns] = feature_df[feature_columns].astype(float)\n\nreference_data = feature_df[feature_df[\"Dataset\"] == \"MNIST\"].drop(columns=[\"Dataset\"])\ncurrent_data = feature_df[feature_df[\"Dataset\"] == \"FashionMNIST\"].drop(columns=[\"Dataset\"])\n\nreport = Report(metrics=[DataDriftTable()])\nreport.run(reference_data=reference_data, current_data=current_data)\nreport.save_html(\"data_drift.html\")\n</code></pre> </li> <li> <p>(Optional) For text a common approach is to extract some higher-level embedding such as the very classical     GLOVE embedding. Try following     this tutorial     to understand how drift detection is done on text.</p> </li> <li> <p>Instead of manually specifying the features, let's take a deep learning-based approach to getting features from     unstructured data. To do this let's consider the CLIP model, which is a     state-of-the-art model for connecting text to images, i.e., image captioning. For our purpose this is perfect     because we can use the model to get abstract feature embeddings for both images and text. Implement a simple     script that extracts features from an image and a text using CLIP. We recommend using the     Huggingface implementation for doing this. What is     the size of the feature vector?</p> Solution <p>Both <code>img_features</code> and <code>text_features</code> for the standard CLIP model are a <code>(512,)</code> abstract feature embedding. We cannot interpret these features directly, but they should be able to tell us something about our data distribution.</p> clip_features.py<pre><code>import requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True, timeout=30).raw)\n\n# set either text=None or images=None when only the other is needed\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\nimg_features = model.get_image_features(inputs[\"pixel_values\"])\ntext_features = model.get_text_features(inputs[\"input_ids\"], inputs[\"attention_mask\"])\nprint(img_features.shape, text_features.shape)\n</code></pre> </li> <li> <p>Run your CLIP script on two different datasets, for example     CIFAR10 and     SVHN     for images or     IMDB movie review and     Amazon review for text.     Then run the data drift detection on the extracted features. What do you see? Does the data drift?</p> </li> </ol> </li> <li> <p>(Optional) If we have multiple applications and want to run monitoring for each application we often also want the     monitoring to be a deployed application (that only we can access). Implement a <code>/monitoring</code> endpoint that does     all the reporting we just went through such that you have two endpoints:</p> <pre><code>http://127.0.0.1:8000/predict/?sepal_length=1.0&amp;sepal_width=1.0&amp;petal_length=1.0&amp;petal_width=1.0  # user endpoint\nhttp://127.0.0.1:8000/monitoring/  # monitoring endpoint\n</code></pre> <p>Our monitoring endpoint should return an HTML page either showing an Evidently report or test suite. Try implementing this endpoint.</p> Solution iris_fastapi_solution.py<pre><code>import pickle\nfrom collections.abc import Generator\nfrom datetime import datetime\n\nimport anyio\nimport pandas as pd\nfrom evidently.metric_preset import (\n    DataDriftPreset,\n    DataQualityPreset,\n    TargetDriftPreset,\n)\nfrom evidently.report import Report\nfrom fastapi import BackgroundTasks, FastAPI\nfrom fastapi.responses import HTMLResponse\nfrom sklearn import datasets\n\n\ndef lifespan(app: FastAPI) -&gt; Generator[None]:\n    \"\"\"Load model and classes, and create database file.\"\"\"\n    global model, classes\n    classes = [\"Iris-Setosa\", \"Iris-Versicolour\", \"Iris-Virginica\"]\n    with open(\"model.pkl\", \"rb\") as file:\n        model = pickle.load(file)\n\n    with open(\"prediction_database.csv\", \"w\") as file:\n        file.write(\"time, sepal_length, sepal_width, petal_length, petal_width, prediction\\n\")\n\n    yield\n\n    del model\n\n\napp = FastAPI(lifespan=lifespan)\n\n\ndef add_to_database(\n    now: str,\n    sepal_length: float,\n    sepal_width: float,\n    petal_length: float,\n    petal_width: float,\n    prediction: int,\n) -&gt; None:\n    \"\"\"Simple function to add prediction to database.\"\"\"\n    with open(\"prediction_database.csv\", \"a\") as file:\n        file.write(f\"{now}, {sepal_length}, {sepal_width}, {petal_length}, {petal_width}, {prediction}\\n\")\n\n\n@app.post(\"/predict\")\nasync def iris_inference(\n    sepal_length: float,\n    sepal_width: float,\n    petal_length: float,\n    petal_width: float,\n    background_tasks: BackgroundTasks,\n):\n    \"\"\"Version 2 of the iris inference endpoint.\"\"\"\n    prediction = model.predict([[sepal_length, sepal_width, petal_length, petal_width]])\n    prediction = prediction.item()\n\n    now = str(datetime.now(tz=datetime.UTC))\n    background_tasks.add_task(add_to_database, now, sepal_length, sepal_width, petal_length, petal_width, prediction)\n    return {\"prediction\": classes[prediction], \"prediction_int\": prediction}\n\n\n@app.get(\"/monitoring\", response_class=HTMLResponse)\nasync def iris_monitoring():\n    \"\"\"Simple get request method that returns a monitoring report.\"\"\"\n    reference_data: pd.DataFrame = datasets.load_iris(as_frame=True).frame\n    reference_data = reference_data.rename(\n        columns={\n            \"sepal length (cm)\": \"sepal_length\",\n            \"sepal width (cm)\": \"sepal_width\",\n            \"petal length (cm)\": \"petal_length\",\n            \"petal width (cm)\": \"petal_width\",\n            \"target\": \"target\",\n        }\n    )\n    current_data = pd.read_csv(\"prediction_database.csv\")\n    current_data = current_data.drop(columns=[\"time\"])\n\n    data_drift_report = Report(metrics=[DataDriftPreset(), DataQualityPreset(), TargetDriftPreset()])\n    data_drift_report.run(current_data=current_data, reference_data=reference_data)\n    data_drift_report.save_html(\"monitoring.html\")\n\n    async with await anyio.open_file(\"monitoring.html\", encoding=\"utf-8\") as f:\n        html_content = f.read()\n\n    return HTMLResponse(content=html_content, status_code=200)\n</code></pre> </li> </ol>"},{"location":"s8_monitoring/data_drifting/#data-drift-in-the-cloud","title":"Data drift in the Cloud","text":"<p>In the next section we are going to look at how we can incorporate the data drifting in our cloud environment. In particular, we are going to be looking at how we can deploy a monitoring application that will run on a schedule and then report those statistics directly back into GCP for us to study.</p>"},{"location":"s8_monitoring/data_drifting/#exercises_1","title":"\u2754 Exercises","text":"<p>In this set of exercises we are going to deploy a machine learning model for sentiment analysis trained on Google Play Store Reviews. The model's task is to predict if a user's review is positive, neutral or negative in sentiment. We are then going to deploy a monitoring service that will check if the distribution of the reviews has drifted over time. This may be useful if we are seeing a decrease in the number of positive reviews over time, which may indicate that our application is not performing as expected.</p> <p>We have already created downloaded the training data, created a training script and trained a model for you. The training data and the trained model are available to download from the following Google Drive folder, which can be quickly downloaded by running the following commands (which uses the gdown Python package):</p> <pre><code>pip install gdown\ngdown --folder https://drive.google.com/drive/folders/19rZSGk4A4O7kDqPQiomgV0TiZkRpZ1Rs?usp=sharing\n</code></pre> <p>And the training script can be seen below. You are free to retrain the model yourself, but it takes about 30 mins to train using a GPU. Overall the model achieves around 74% accuracy on a held-out test set. We recommend that you scroll through the files to get an understanding of what is going on.</p> Training script for sentiment analysis model sentiment_classifier.py<pre><code># All credits to\n# https://www.kaggle.com/code/prakharrathi25/sentiment-analysis-using-bert/notebook\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup\n\nMODEL_NAME = \"bert-base-cased\"\nBATCH_SIZE = 16\nMAX_LEN = 160\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndf = pd.read_csv(\"reviews.csv\")\n\n\ndef to_sentiment(rating):\n    \"\"\"Convert rating to sentiment class.\"\"\"\n    rating = int(rating)\n    if rating &lt;= 2:\n        return 0  # Negative\n    if rating == 3:\n        return 1  # Neutral\n    return 2  # Positive\n\n\n# Apply to the dataset\ndf[\"sentiment\"] = df.score.apply(to_sentiment)\nclass_names = [\"negative\", \"neutral\", \"positive\"]\n\n# Build a BERT based tokenizer\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n\n\nclass GPReviewDataset(Dataset):\n    \"\"\"Google Play Review Dataset class.\"\"\"\n\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        \"\"\"Return the length of the dataset.\"\"\"\n        return len(self.reviews)\n\n    def __getitem__(self, item):\n        \"\"\"Get a single review from the dataset and tokenize it.\"\"\"\n        review = str(self.reviews[item])\n        target = self.targets[item]\n\n        # Encoded format to be returned\n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n            truncation=True,\n        )\n        return {\n            \"review_text\": review,\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"targets\": torch.tensor(target, dtype=torch.long),\n        }\n\n\ndf_train, df_test = train_test_split(df, test_size=0.2)\ndf_val, df_test = train_test_split(df_test, test_size=0.5)\n\n\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    \"\"\"Create a data loader for the dataset.\"\"\"\n    ds = GPReviewDataset(\n        reviews=df.content.to_numpy(), targets=df.sentiment.to_numpy(), tokenizer=tokenizer, max_len=max_len\n    )\n    return DataLoader(ds, batch_size=batch_size)\n\n\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n\n\n# Build the Sentiment Classifier class\nclass SentimentClassifier(nn.Module):\n    \"\"\"Sentiment Classifier class. Combines BERT model with a dropout and linear layer.\"\"\"\n\n    def __init__(self, n_classes, model_name=MODEL_NAME):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"Forward pass of the model.\"\"\"\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.drop(output[1])\n        return self.out(output)\n\n\nmodel = SentimentClassifier(len(class_names))\nmodel = model.to(device)\n\nEPOCHS = 10\n\n# Optimizer Adam\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n# Set the loss function\nloss_fn = nn.CrossEntropyLoss().to(device)\n\n\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    \"\"\"Train the model for one epoch e.g. one pass through the dataset.\"\"\"\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n\n    for d in tqdm(data_loader):\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n\n        # Backward prop\n        loss.backward()\n\n        # Gradient Descent\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    \"\"\"Evaluate the model.\"\"\"\n    model = model.eval()\n\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            # Get model outputs\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    # Show details\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(\"-\" * 10)\n\n    train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n\n    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n\n    # Get model performance (accuracy and loss)\n    val_acc, val_loss = eval_model(model, val_data_loader, loss_fn, device, len(df_val))\n\n    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n    history[\"train_acc\"].append(train_acc)\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_acc\"].append(val_acc)\n    history[\"val_loss\"].append(val_loss)\n\n    # If we beat prev performance\n    if val_acc &gt; best_accuracy:\n        torch.save(model.state_dict(), \"bert_sentiment_model.pt\")\n        best_accuracy = val_acc\n\ntest_acc, _ = eval_model(model, test_data_loader, loss_fn, device, len(df_test))\nprint(f\"Test Accuracy {test_acc.item()}\")\n</code></pre> <ol> <li> <p>To begin let's start by uploading the training data and model to a GCP bucket. Upload to a new GCP bucket     called <code>gcp_monitoring_exercise</code> (or something similar). Upload the training data and the trained model to the     bucket.</p> Solution <p>This can be done by running the following commands or by manually uploading the files to the bucket using the GCP console.</p> <pre><code>gsutil mb gs://gcp_monitoring_exercise\ngsutil cp reviews.csv gs://gcp_monitoring_exercise/reviews.csv\ngsutil cp bert_sentiment_model.pt gs://gcp_monitoring_exercise/bert_sentiment_model.pt\n</code></pre> </li> <li> <p>Next we need to create a FastAPI application that takes a review as input and returns the predicted sentiment of     the review. We provide a starting point for the application in the file below that should be able to run as is.</p> Starting point for sentiment analysis API sentiment_api_starter.py<pre><code>from contextlib import asynccontextmanager\n\nimport torch\nimport torch.nn as nn\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom transformers import BertModel, BertTokenizer\n\n# Define model and device configuration\nMODEL_NAME = \"bert-base-cased\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ReviewInput(BaseModel):\n    \"\"\"Define input data structure for the endpoint.\"\"\"\n\n    review: str\n\n\nclass PredictionOutput(BaseModel):\n    \"\"\"Define output data structure for the endpoint.\"\"\"\n\n    sentiment: str\n\n\nclass SentimentClassifier(nn.Module):\n    \"\"\"Sentiment Classifier class. Combines BERT model with a dropout and linear layer.\"\"\"\n\n    def __init__(self, n_classes, model_name=MODEL_NAME):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"Forward pass of the model.\"\"\"\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.drop(output[1])\n        return self.out(output)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load the model and tokenizer when the app starts and clean up when the app stops.\"\"\"\n    global model, tokenizer, class_names\n    model = SentimentClassifier(n_classes=3)\n    model.load_state_dict(torch.load(\"bert_sentiment_model.pt\", map_location=device))\n    model = model.to(device)\n    model.eval()\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n    print(\"Model and tokenizer loaded successfully\")\n\n    yield\n\n    del model, tokenizer\n\n\n# Initialize FastAPI app\napp = FastAPI(lifespan=lifespan)\n\n\n# Prediction endpoint\n@app.post(\"/predict\", response_model=PredictionOutput)\nasync def predict_sentiment(review_input: ReviewInput):\n    \"\"\"Predict sentiment of the input text.\"\"\"\n    try:\n        # Encode input text\n        encoding = tokenizer.encode_plus(\n            review_input.review,\n            add_special_tokens=True,\n            max_length=160,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        input_ids = encoding[\"input_ids\"].to(device)\n        attention_mask = encoding[\"attention_mask\"].to(device)\n\n        # Model prediction\n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask)\n            _, prediction = torch.max(outputs, dim=1)\n            sentiment = class_names[prediction]\n\n        return PredictionOutput(sentiment=sentiment)\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e)) from e\n</code></pre> <ol> <li> <p>Confirm that you can run the application by running the following command in the terminal</p> <pre><code>uvicorn sentiment_api_starter:app --reload\n</code></pre> <p>You need the model file saved in the same directory as the application to run it. Write a small <code>client.py</code> script that calls the application with a review and prints the predicted sentiment. Try executing the script in another terminal (while the <code>uvicorn</code> server is running) to confirm that the application is working.</p> Solution <pre><code>import requests\n\nurl = \"http://localhost:8000/predict\"\nreview = \"This is a great app, I love it!\"\nresponse = requests.post(url, json={\"review\": review})\nprint(response.json())\n</code></pre> </li> <li> <p>Next, we need to extend the application in two ways. First, instead of loading the model from our local computer,     it should load from the bucket we just uploaded the model to. Second, we need to save the request data and the     predicted label to the cloud. Normally this would best be suited in a database, but we are going to just save     to the same bucket as the model. We just need to make sure each request is saved under a unique name (e.g., the     time and date of the request). Implement both of these functionalities in the application. To interact with     GCP buckets in Python you should install the <code>google-cloud-storage</code> package if you have not already done so.</p> <pre><code>pip install google-cloud-storage\n</code></pre> Solution sentiment_api.py<pre><code>import datetime\nimport json\nimport os\nfrom contextlib import asynccontextmanager\n\nimport torch\nimport torch.nn as nn\nfrom fastapi import BackgroundTasks, FastAPI, HTTPException\nfrom google.cloud import storage\nfrom pydantic import BaseModel\nfrom transformers import BertModel, BertTokenizer\n\n# Define model and device configuration\nBUCKET_NAME = \"gcp_monitoring_exercise\"\nMODEL_NAME = \"bert-base-cased\"\nMODEL_FILE_NAME = \"bert_sentiment_model.pt\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ReviewInput(BaseModel):\n    \"\"\"Define input data structure for the endpoint.\"\"\"\n\n    review: str\n\n\nclass PredictionOutput(BaseModel):\n    \"\"\"Define output data structure for the endpoint.\"\"\"\n\n    sentiment: str\n\n\nclass SentimentClassifier(nn.Module):\n    \"\"\"Sentiment Classifier class. Combines BERT model with a dropout and linear layer.\"\"\"\n\n    def __init__(self, n_classes, model_name=MODEL_NAME):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"Forward pass of the model.\"\"\"\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.drop(output[1])\n        return self.out(output)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load the model and tokenizer when the app starts and clean up when the app stops.\"\"\"\n    global model, tokenizer, class_names\n    if \"bert_sentiment_model.pt\" not in os.listdir():\n        download_model_from_gcp()  # Download the model from GCP\n    model = SentimentClassifier(n_classes=3)\n    model.load_state_dict(torch.load(\"bert_sentiment_model.pt\", map_location=device))\n    model = model.to(device)\n    model.eval()\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n    print(\"Model and tokenizer loaded successfully\")\n\n    yield\n\n    del model, tokenizer\n\n\n# Initialize FastAPI app\napp = FastAPI(lifespan=lifespan)\n\n\ndef download_model_from_gcp():\n    \"\"\"Download the model from GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    blob = bucket.blob(MODEL_FILE_NAME)\n    blob.download_to_filename(MODEL_FILE_NAME)\n    print(f\"Model {MODEL_FILE_NAME} downloaded from GCP bucket {BUCKET_NAME}.\")\n\n\n# Save prediction results to GCP\ndef save_prediction_to_gcp(review: str, outputs: list[float], sentiment: str):\n    \"\"\"Save the prediction results to GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    time = datetime.datetime.now(tz=datetime.UTC)\n    # Prepare prediction data\n    data = {\n        \"review\": review,\n        \"sentiment\": sentiment,\n        \"probability\": outputs,\n        \"timestamp\": datetime.datetime.now(tz=datetime.UTC).isoformat(),\n    }\n    blob = bucket.blob(f\"prediction_{time}.json\")\n    blob.upload_from_string(json.dumps(data))\n    print(\"Prediction saved to GCP bucket.\")\n\n\n# Prediction endpoint\n@app.post(\"/predict\", response_model=PredictionOutput)\nasync def predict_sentiment(review_input: ReviewInput, background_tasks: BackgroundTasks):\n    \"\"\"Predict sentiment of the input text.\"\"\"\n    try:\n        # Encode input text\n        encoding = tokenizer.encode_plus(\n            review_input.review,\n            add_special_tokens=True,\n            max_length=160,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        input_ids = encoding[\"input_ids\"].to(device)\n        attention_mask = encoding[\"attention_mask\"].to(device)\n\n        # Model prediction\n        with torch.no_grad():\n            outputs: torch.Tensor = model(input_ids, attention_mask)\n            _, prediction = torch.max(outputs, dim=1)\n            sentiment = class_names[prediction]\n\n        background_tasks.add_task(save_prediction_to_gcp, review_input.review, outputs.softmax(-1).tolist(), sentiment)\n\n        return PredictionOutput(sentiment=sentiment)\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e)) from e\n</code></pre> </li> <li> <p>You should confirm that the application is working locally before moving on. You can do this by running the     following command in the terminal</p> <pre><code>uvicorn sentiment_api:app --reload\n</code></pre> <p>And use the same <code>client.py</code> script as before to confirm that the application is working. You should also check that the data is saved to the bucket.</p> </li> <li> <p>Write a small Dockerfile that containerizes the application</p> Solution sentiment_api.dockerfilepy<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nRUN pip install fastapi torch transformers google-cloud-storage pydantic --no-cache-dir\n\nCOPY sentiment_api.py .\n\nEXPOSE $PORT\n\nCMD exec uvicorn sentiment_api:app --port $PORT --host 0.0.0.0 --workers 1\n</code></pre> <p>which can be built by running the following command</p> <pre><code>docker build -f sentiment_api.dockerfile -t sentiment_api:latest .\n</code></pre> </li> <li> <p>Deploy the container to cloud run and confirm that the application still runs as expected.</p> Solution <p>The following four commands should be able to deploy the application to GCP cloud run. Make sure to replace <code>&lt;location&gt;</code>, <code>&lt;project-id&gt;</code> and <code>&lt;repo-name&gt;</code> with the appropriate values.</p> <pre><code>gcloud artifacts repositories create &lt;repo-name&gt; --repository-format=docker --location=&lt;location&gt;\ndocker tag sentiment_api:latest &lt;location&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repo-name&gt;/sentiment_api:latest\ndocker push &lt;location&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repo-name&gt;/sentiment_api:latest\ngcloud run deploy sentiment-api \\\n    --image &lt;location&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repo-name&gt;/sentiment_api:latest \\\n    --region &lt;region&gt; --allow-unauthenticated\n</code></pre> </li> <li> <p>Make sure that the application still works by trying to send a couple of requests to the deployed application and     make sure that the request/response data is correctly saved to the bucket.</p> Solution <p>To get the URL of the deployed service you can run the following command</p> <pre><code>gcloud run services describe sentiment-api --format 'value(status.url)'\n</code></pre> <p>which can then be used in the <code>client.py</code> script to call the deployed service.</p> </li> </ol> </li> <li> <p>We now have a working application that we are ready to monitor for data drift in real time. We therefore now need to     write a FastAPI application that takes in the training data and the predicted data and runs evidently to check if the     data or the labels have drifted. Furthermore, we again provide a starting point for the application below.</p> sentiment_monitoring_starter.py<pre><code>from pathlib import Path\n\nimport anyio\nimport nltk\nimport pandas as pd\nfrom evidently.metric_preset import TargetDriftPreset, TextEvals\nfrom evidently.report import Report\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\n\nnltk.download(\"words\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\n\ndef run_analysis(reference_data: pd.DataFrame, current_data: pd.DataFrame):\n    \"\"\"Run the analysis and return the report.\"\"\"\n    text_overview_report = Report(metrics=[TextEvals(column_name=\"content\"), TargetDriftPreset(columns=[\"sentiment\"])])\n    text_overview_report.run(reference_data=reference_data, current_data=current_data)\n    text_overview_report.save(\"text_overview_report.html\")\n\n\ndef lifespan(app: FastAPI):\n    \"\"\"Load the data and class names before the application starts.\"\"\"\n    global training_data, class_names\n    training_data = pd.read_csv(\"reviews.csv\")\n\n    def to_sentiment(rating):\n        \"\"\"Convert rating to sentiment class.\"\"\"\n        rating = int(rating)\n        if rating &lt;= 2:\n            return 0  # Negative\n        if rating == 3:\n            return 1  # Neutral\n        return 2  # Positive\n\n    training_data[\"sentiment\"] = training_data.score.apply(to_sentiment)\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n\n    yield\n\n    del training_data, class_names\n\n\napp = FastAPI(lifespan=lifespan)\n\n\ndef download_files(n: int = 5) -&gt; None:\n    \"\"\"Download the N latest prediction files from the GCP bucket.\"\"\"\n\n\ndef load_latest_files(directory: Path, n: int) -&gt; pd.DataFrame:\n    \"\"\"Fetch latest data from the database.\"\"\"\n    download_files(n=n)\n\n\n@app.get(\"/report\")\nasync def get_report(n: int = 5):\n    \"\"\"Generate and return the report.\"\"\"\n    prediction_data = load_latest_files(Path(\".\"), n=n)\n    run_analysis(training_data, prediction_data)\n\n    async with await anyio.open_file(\"monitoring.html\", encoding=\"utf-8\") as f:\n        html_content = f.read()\n\n    return HTMLResponse(content=html_content, status_code=200)\n</code></pre> <p>Look over the script and make sure you know what kind of features we are going to monitor.</p> Solution <p>The provided starting script makes use of two presets from evidently: TextOverviewPreset and TargetDriftPreset. The first preset extracts descriptive text statistics (like number of words, average word length etc.) and runs data drift detection on these and the second preset runs target drift detection on the predicted labels.</p> <ol> <li> <p>The script is missing one key function to work: <code>fetch_latest_data(n: int)</code> which should fetch the     latest <code>n</code> predictions. Implement this function in the script.</p> Solution sentiment_monitoring.py<pre><code>import json\nimport os\nfrom pathlib import Path\n\nimport anyio\nimport nltk\nimport pandas as pd\nfrom evidently.metric_preset import TargetDriftPreset, TextEvals\nfrom evidently.report import Report\nfrom fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\nfrom google.cloud import storage\n\nnltk.download(\"words\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\nBUCKET_NAME = \"gcp_monitoring_exercise\"\n\n\ndef to_sentiment(rating):\n    \"\"\"Convert rating to sentiment class.\"\"\"\n    rating = int(rating)\n    if rating &lt;= 2:\n        return 0  # Negative\n    if rating == 3:\n        return 1  # Neutral\n    return 2  # Positive\n\n\ndef sentiment_to_numeric(sentiment: str) -&gt; int:\n    \"\"\"Convert sentiment class to numeric.\"\"\"\n    if sentiment == \"negative\":\n        return 0\n    if sentiment == \"neutral\":\n        return 1\n    return 2\n\n\ndef run_analysis(reference_data: pd.DataFrame, current_data: pd.DataFrame) -&gt; None:\n    \"\"\"Run the analysis and return the report.\"\"\"\n    text_overview_report = Report(metrics=[TextEvals(column_name=\"content\"), TargetDriftPreset(columns=[\"sentiment\"])])\n    text_overview_report.run(reference_data=reference_data, current_data=current_data)\n    text_overview_report.save(\"text_overview_report.html\")\n\n\ndef lifespan(app: FastAPI):\n    \"\"\"Load the data and class names before the application starts.\"\"\"\n    global training_data, class_names\n    training_data = pd.read_csv(\"reviews.csv\")\n    training_data[\"sentiment\"] = training_data.score.apply(to_sentiment)\n    training_data[\"target\"] = training_data[\"sentiment\"]  # evidently expects the target column to be named \"target\"\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n\n    yield\n\n    del training_data, class_names\n\n\napp = FastAPI(lifespan=lifespan)\n\n\ndef load_latest_files(directory: Path, n: int) -&gt; pd.DataFrame:\n    \"\"\"Load the N latest prediction files from the directory.\"\"\"\n    # Download the latest prediction files from the GCP bucket\n    download_files(n=n)\n\n    # Get all prediction files in the directory\n    files = directory.glob(\"prediction_*.json\")\n\n    # Sort files based on when they where created\n    files = sorted(files, key=os.path.getmtime)\n\n    # Get the N latest files\n    latest_files = files[-n:]\n\n    # Load or process the files as needed\n    reviews, sentiment = [], []\n    for file in latest_files:\n        with file.open() as f:\n            data = json.load(f)\n            reviews.append(data[\"review\"])\n            sentiment.append(sentiment_to_numeric(data[\"sentiment\"]))\n    dataframe = pd.DataFrame({\"content\": reviews, \"sentiment\": sentiment})\n    dataframe[\"target\"] = dataframe[\"sentiment\"]\n    return dataframe\n\n\ndef download_files(n: int = 5) -&gt; None:\n    \"\"\"Download the N latest prediction files from the GCP bucket.\"\"\"\n    bucket = storage.Client().bucket(BUCKET_NAME)\n    blobs = bucket.list_blobs(prefix=\"prediction_\")\n    blobs.sort(key=lambda x: x.updated, reverse=True)\n    latest_blobs = blobs[:n]\n\n    for blob in latest_blobs:\n        blob.download_to_filename(blob.name)\n\n\n@app.get(\"/report\", response_class=HTMLResponse)\nasync def get_report(n: int = 5):\n    \"\"\"Generate and return the report.\"\"\"\n    prediction_data = load_latest_files(Path(\".\"), n=n)\n    run_analysis(training_data, prediction_data)\n\n    async with await anyio.open_file(\"monitoring.html\", encoding=\"utf-8\") as f:\n        html_content = f.read()\n\n    return HTMLResponse(content=html_content, status_code=200)\n</code></pre> </li> <li> <p>Test out the script locally. This can be done by downloading a couple of the request/response data points from the     bucket and running the script on this data.</p> </li> <li> <p>Write a Dockerfile that containerizes the monitoring application.</p> Solution sentiment_monitoring.dockerfile<pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nRUN pip install fastapi nltk evidently google-cloud-storage --no-cache-dir\n\nCOPY sentiment_monitoring.py .\n\nEXPOSE $PORT\n\nCMD exec uvicorn sentiment_monitoring:app --port $PORT --host 0.0.0.0 --workers 1\n</code></pre> </li> <li> <p>Deploy the monitoring application to cloud run and confirm that the application returns a monitoring report when     asked for it.</p> Solution <pre><code>docker tag sentiment_monitoring:latest \\\n    &lt;location&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repo-name&gt;/sentiment_monitoring:latest\ndocker push &lt;location&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repo-name&gt;/sentiment_monitoring:latest\ngcloud run deploy sentiment-monitoring \\\n    --image &lt;location&gt;-docker.pkg.dev/&lt;project-id&gt;/&lt;repo-name&gt;/sentiment_monitoring:latest \\\n    --region &lt;region&gt; --allow-unauthenticated\n</code></pre> </li> </ol> </li> <li> <p>We are now finally ready to test our services. Since we need to observe some long-term behavior this part may take     some time to run depending on how exactly you have configured things. Below we have implemented a client script that     is meant to call our service.</p> <p>Client script for sentiment analysis model</p> sentiment_client.py<pre><code>import argparse\nimport random\nimport time\n\nimport requests\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--url\", type=str, default=\"http://localhost:8000/predict\")\n    parser.add_argument(\"--wait_time\", type=int, default=5)\n    parser.add_argument(\"--max_iterations\", type=int, default=1000)\n    args = parser.parse_args()\n\n    reviews = [\n        \"This app is fantastic! I use it every day.\",\n        \"I enjoy using this app, it's very helpful.\",\n        \"It's a decent app, nothing extraordinary but useful.\",\n        \"This app is okay, but it could be improved.\",\n        \"I'm not very impressed with this app.\",\n        \"This app is not meeting my expectations.\",\n    ]\n\n    negative_phrases = [\n        \"It\u2019s getting frustrating to use.\",\n        \"There are so many bugs now.\",\n        \"The app crashes often and I am really disappointed.\",\n        \"I think I'm going to stop using this app soon.\",\n        \"It has become completely unusable.\",\n    ]\n\n    count = 0\n    while count &lt; args.max_iterations:\n        review = random.choice(reviews)\n        negativity_probability = min(count / args.max_iterations, 1.0)\n\n        updated_review = review\n        for phrase in negative_phrases:\n            if random.random() &lt; negativity_probability:\n                updated_review += \" \" + phrase\n\n        response = requests.post(args.url, json={\"review\": updated_review}, timeout=10)\n        print(f\"Iteration {count}, Sent review: {updated_review}, Response: {response.json()}\")\n        time.sleep(args.wait_time)\n        count += 1\n</code></pre> <ol> <li> <p>What does the client script do?</p> Solution <p>The client script will iteratively call our deployed sentiment analysis service every <code>wait_time</code> seconds. In each iteration it:</p> <ul> <li>Randomly samples a review from a list of positive, neutral and negative reviews</li> <li>Randomly adds negative phrases to the review. Each review is added if a randomly uniform number is lower     than probability `negative_probability=min(count / args.max_iterations, 1.0), meaning that it becomes     more and more likely that the negative phrases are added as the number of iterations increases.</li> <li>Sends the review to the sentiment analysis service and saves the response to a file.</li> </ul> </li> <li> <p>Run the client script for 1,000 iterations. What happens to the distribution of the reviews over time? Does the     data drift?</p> </li> </ol> </li> </ol> <p>That ends the module on detection of data drifting, data quality, etc. We have a couple of final points to make before we end the module:</p> <ul> <li> <p>Monitoring of machine learning applications is an extremely hard discipline because it is not clear-cut when we should     actually respond to feature/targets beginning to drift and when it is probably fine letting the system run as is.     It comes down to the individual application what kinds of rules should be implemented.</p> </li> <li> <p>The cloud setup we have developed is very simple and not meant for production. In a real-world scenario we would not     have deployed our monitoring application on an endpoint that generates a report, but rather have it return the test     results in a JSON format that can be ingested into more complex monitoring systems where we can show how drift     scores evolve over time. You will learn more about this in the next module.</p> </li> <li> <p>The tools presented here are in no way comprehensive and are especially limited in one way: they only consider the     marginal data distribution. Every analysis that we've done has been on the distribution per feature (the     marginal distribution), however as the image below shows it is possible for data to have drifted to another     distribution with the marginal being approximately the same.</p> <p> </p> <p>There are methods such as Maximum Mean Discrepancy (MMD) tests that are able to do testing on multivariate distributions, which you are free to dive into. The general recommendation is to just always consider multiple features when making decisions. In this course we will just always recommend considering multiple features when making decisions regarding your deployed applications.</p> </li> </ul> <p>Finally, we want to stress that monitoring is a very active field of research and that there are many more tools and frameworks that can be used for monitoring.</p>"},{"location":"s8_monitoring/monitoring/","title":"M28 - System Monitoring","text":""},{"location":"s8_monitoring/monitoring/#monitoring","title":"Monitoring","text":"<p>In this module we are going to look into more classical monitoring of applications. The key concept we are often working with here is called telemetry. Telemetry in general refers to any automatic measurement and wireless transmission of data from our application. It could be numbers such as:</p> <ul> <li>The number of requests our application receives per minute/hour/day. This number is of interest because it is     directly proportional to the running cost of the application.</li> <li>The amount of time (on average) our application runs per request. The number is of interest because it most likely is     the core contributor to the latency that our users experience (which we want to be low).</li> <li>...</li> </ul> <p>In general there are three different kinds of telemetry we are interested in:</p> Name Description Example Purpose Metrics Metrics are quantitative measurements of the system. They are usually numbers that are aggregated over a period of time. E.g. the number of requests per minute. The number of requests per minute. Metrics are used to get an overview of the system. They are often used to create dashboards that can be used to get an overview of the system. Logs Logs are textual or structured records generated by applications. They provide a detailed account of events, errors, warnings, and informational messages that occur during the operation of the system. System logs, error logs Logs are essential for diagnosing issues, debugging, and auditing. They provide a detailed history of what happened in a system, making it easier to trace the root cause of problems and track the behavior of components over time. Traces Traces are detailed records of specific transactions or events as they move through a system. A trace typically includes information about the sequence of operations, timing, and dependencies between different components. Distributed tracing in microservices architecture Traces help in understanding the flow of a request or a transaction across different components. They are valuable for identifying bottlenecks, understanding latency, and troubleshooting issues related to the flow of data or control. <p>We are going to focus on metrics in this module, because that is the first kind of telemetry data that most people want to set up first.</p>"},{"location":"s8_monitoring/monitoring/#attaching-metrics-to-an-application","title":"Attaching metrics to an application","text":"<p>Let's start by looking at how we can attach metrics to an application and expose them afterwards. The standard framework for exposing metrics is called prometheus. Prometheus is a time series database that is designed to store metrics. It is also designed to be very easy to instrument applications with and it is designed to scale to large amounts of data. The way prometheus works is that it exposes a <code>/metrics</code> endpoint that can be queried to get the current state of the metrics. The metrics are exposed in a format called prometheus text format.</p>"},{"location":"s8_monitoring/monitoring/#exercises","title":"\u2754 Exercises","text":"<ol> <li> <p>Start by installing the Prometheus python client.</p> <pre><code>pip install prometheus-client\n</code></pre> <p>Remember to add the package to your <code>requirements.txt</code> file.</p> </li> <li> <p>Before getting started on the coding part, we first need to understand the different kinds of metrics that Prometheus     allows for us to specify. Look through the documentation and     describe the different types of metrics that Prometheus allows for.</p> Solution <p>The different types of metrics that prometheus allows for are:</p> <ul> <li><code>Counter</code>: A counter is a cumulative metric that represents a single monotonically increasing counter whose     value can only increase or be reset to zero on restart.</li> <li><code>Gauge</code>: A gauge is a metric that represents a single numerical value that can arbitrarily go up and down.     This is typically used for measured values like current memory usage, CPU usage, etc.</li> <li><code>Histogram</code>: A histogram samples observations (usually things like request durations or response sizes) and     counts them in configurable buckets. It also provides a sum of all observed values.</li> <li><code>Summary</code>: Similar to a histogram, a summary samples observations (usually things like request durations and     response sizes). While it also provides a total count of observations and a sum of all observed values, it     calculates configurable quantiles over a sliding time window.</li> </ul> </li> <li> <p>Assuming you have done the previous module on data drifting you should be familiar with the     following API that uses a Bert model to classify the sentiment of a given review. The purpose of this exercise is     to add some meaningful metrics to that API.</p> Sentiment API sentiment_api.py<pre><code>import datetime\nimport json\nimport os\nfrom contextlib import asynccontextmanager\n\nimport torch\nimport torch.nn as nn\nfrom fastapi import BackgroundTasks, FastAPI, HTTPException\nfrom google.cloud import storage\nfrom pydantic import BaseModel\nfrom transformers import BertModel, BertTokenizer\n\n# Define model and device configuration\nBUCKET_NAME = \"gcp_monitoring_exercise\"\nMODEL_NAME = \"bert-base-cased\"\nMODEL_FILE_NAME = \"bert_sentiment_model.pt\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ReviewInput(BaseModel):\n    \"\"\"Define input data structure for the endpoint.\"\"\"\n\n    review: str\n\n\nclass PredictionOutput(BaseModel):\n    \"\"\"Define output data structure for the endpoint.\"\"\"\n\n    sentiment: str\n\n\nclass SentimentClassifier(nn.Module):\n    \"\"\"Sentiment Classifier class. Combines BERT model with a dropout and linear layer.\"\"\"\n\n    def __init__(self, n_classes, model_name=MODEL_NAME):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"Forward pass of the model.\"\"\"\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.drop(output[1])\n        return self.out(output)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load the model and tokenizer when the app starts and clean up when the app stops.\"\"\"\n    global model, tokenizer, class_names\n    if \"bert_sentiment_model.pt\" not in os.listdir():\n        download_model_from_gcp()  # Download the model from GCP\n    model = SentimentClassifier(n_classes=3)\n    model.load_state_dict(torch.load(\"bert_sentiment_model.pt\", map_location=device))\n    model = model.to(device)\n    model.eval()\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n    print(\"Model and tokenizer loaded successfully\")\n\n    yield\n\n    del model, tokenizer\n\n\n# Initialize FastAPI app\napp = FastAPI(lifespan=lifespan)\n\n\ndef download_model_from_gcp():\n    \"\"\"Download the model from GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    blob = bucket.blob(MODEL_FILE_NAME)\n    blob.download_to_filename(MODEL_FILE_NAME)\n    print(f\"Model {MODEL_FILE_NAME} downloaded from GCP bucket {BUCKET_NAME}.\")\n\n\n# Save prediction results to GCP\ndef save_prediction_to_gcp(review: str, outputs: list[float], sentiment: str):\n    \"\"\"Save the prediction results to GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    time = datetime.datetime.now(tz=datetime.UTC)\n    # Prepare prediction data\n    data = {\n        \"review\": review,\n        \"sentiment\": sentiment,\n        \"probability\": outputs,\n        \"timestamp\": datetime.datetime.now(tz=datetime.UTC).isoformat(),\n    }\n    blob = bucket.blob(f\"prediction_{time}.json\")\n    blob.upload_from_string(json.dumps(data))\n    print(\"Prediction saved to GCP bucket.\")\n\n\n# Prediction endpoint\n@app.post(\"/predict\", response_model=PredictionOutput)\nasync def predict_sentiment(review_input: ReviewInput, background_tasks: BackgroundTasks):\n    \"\"\"Predict sentiment of the input text.\"\"\"\n    try:\n        # Encode input text\n        encoding = tokenizer.encode_plus(\n            review_input.review,\n            add_special_tokens=True,\n            max_length=160,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        input_ids = encoding[\"input_ids\"].to(device)\n        attention_mask = encoding[\"attention_mask\"].to(device)\n\n        # Model prediction\n        with torch.no_grad():\n            outputs: torch.Tensor = model(input_ids, attention_mask)\n            _, prediction = torch.max(outputs, dim=1)\n            sentiment = class_names[prediction]\n\n        background_tasks.add_task(save_prediction_to_gcp, review_input.review, outputs.softmax(-1).tolist(), sentiment)\n\n        return PredictionOutput(sentiment=sentiment)\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e)) from e\n</code></pre> <p>To begin with you need to do the following: Add a single metric of the type <code>Counter</code> that counts the number of errors the API has encountered. Then, secondly add an <code>app.mount</code> that exposes the metrics on the <code>/metrics</code> endpoint. When you have done this you should be able to see the metrics by running the application and going to the <code>/metrics</code> endpoint. You can look at the documentation for help on how to do this.</p> Solution <p>The important parts that implement the prometheus metrics are highlighted below:</p> <pre><code>import datetime\nimport json\nimport os\nfrom contextlib import asynccontextmanager\n\nimport torch\nimport torch.nn as nn\nfrom fastapi import BackgroundTasks, FastAPI, HTTPException\nfrom google.cloud import storage\nfrom prometheus_client import Counter, make_asgi_app\nfrom pydantic import BaseModel\nfrom transformers import BertModel, BertTokenizer\n\n# Define model and device configuration\nBUCKET_NAME = \"gcp_monitoring_exercise\"\nMODEL_NAME = \"bert-base-cased\"\nMODEL_FILE_NAME = \"bert_sentiment_model.pt\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ReviewInput(BaseModel):\n    \"\"\"Define input data structure for the endpoint.\"\"\"\n\n    review: str\n\n\nclass PredictionOutput(BaseModel):\n    \"\"\"Define output data structure for the endpoint.\"\"\"\n\n    sentiment: str\n\n\nclass SentimentClassifier(nn.Module):\n    \"\"\"Sentiment Classifier class. Combines BERT model with a dropout and linear layer.\"\"\"\n\n    def __init__(self, n_classes, model_name=MODEL_NAME):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"Forward pass of the model.\"\"\"\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.drop(output[1])\n        return self.out(output)\n\n\n# Define Prometheus metrics\nerror_counter = Counter(\"prediction_error\", \"Number of prediction errors\")\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load the model and tokenizer when the app starts and clean up when the app stops.\"\"\"\n    global model, tokenizer, class_names\n    if \"bert_sentiment_model.pt\" not in os.listdir():\n        download_model_from_gcp()  # Download the model from GCP\n    model = SentimentClassifier(n_classes=3)\n    model.load_state_dict(torch.load(\"bert_sentiment_model.pt\", map_location=device))\n    model = model.to(device)\n    model.eval()\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n    print(\"Model and tokenizer loaded successfully\")\n\n    yield\n\n    del model, tokenizer\n\n\n# Initialize FastAPI app\napp = FastAPI(lifespan=lifespan)\napp.mount(\"/metrics\", make_asgi_app())\n\n\ndef download_model_from_gcp():\n    \"\"\"Download the model from GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    blob = bucket.blob(MODEL_FILE_NAME)\n    blob.download_to_filename(MODEL_FILE_NAME)\n    print(f\"Model {MODEL_FILE_NAME} downloaded from GCP bucket {BUCKET_NAME}.\")\n\n\n# Save prediction results to GCP\ndef save_prediction_to_gcp(review: str, outputs: list[float], sentiment: str):\n    \"\"\"Save the prediction results to GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    time = datetime.datetime.now(tz=datetime.UTC)\n    # Prepare prediction data\n    data = {\n        \"review\": review,\n        \"sentiment\": sentiment,\n        \"probability\": outputs,\n        \"timestamp\": datetime.datetime.now(tz=datetime.UTC).isoformat(),\n    }\n    blob = bucket.blob(f\"prediction_{time}.json\")\n    blob.upload_from_string(json.dumps(data))\n    print(\"Prediction saved to GCP bucket.\")\n\n\n# Prediction endpoint\n@app.post(\"/predict\", response_model=PredictionOutput)\nasync def predict_sentiment(review_input: ReviewInput, background_tasks: BackgroundTasks):\n    \"\"\"Predict sentiment of the input text.\"\"\"\n    try:\n        # Encode input text\n        encoding = tokenizer.encode_plus(\n            review_input.review,\n            add_special_tokens=True,\n            max_length=160,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        input_ids = encoding[\"input_ids\"].to(device)\n        attention_mask = encoding[\"attention_mask\"].to(device)\n\n        # Model prediction\n        with torch.no_grad():\n            outputs: torch.Tensor = model(input_ids, attention_mask)\n            _, prediction = torch.max(outputs, dim=1)\n            sentiment = class_names[prediction]\n\n        background_tasks.add_task(save_prediction_to_gcp, review_input.review, outputs.softmax(-1).tolist(), sentiment)\n\n        return PredictionOutput(sentiment=sentiment)\n\n    except Exception as e:\n        error_counter.inc()\n        raise HTTPException(status_code=500, detail=str(e)) from e\n</code></pre> <ol> <li> <p>If you have done the previous exercise correctly you should be seeing something like the image below.</p> <p> </p> <p>Figure out why there are many more metrics than the single one you added and try to make the metric you defined increase in value by sending error requests to the API.</p> Solution <p>All metrics in prometheus belong to a <code>registry</code>. The registry is a collection of metrics. If you do not specify a registry when you create a metric like this</p> <pre><code>MY_REGISTRY = CollectorRegistry()\nmy_counter = Counter('my_counter', 'This is my counter', registry=MY_REGISTRY)\n</code></pre> <p>then the metric will be added to the default registry. The default registry already contains a lot of metrics that are added by the <code>prometheus-client</code> package. To only see the metrics that you have added you need to specify your own registry like above and only expose that registry on the <code>/metrics</code> endpoint.</p> <pre><code>app.mount(\"/metrics\", make_asgi_app(registry=MY_REGISTRY))\n</code></pre> <p>For the second part of the question you can increase the value of the counter by sending a request to the API that will raise an error. In this case the API will raise a 500 error if the review is too long. After this you should see the counter increase in value.</p> </li> <li> <p>Next, we ask you to add a few more metrics to the API. Specifically we ask you to add the following metrics:</p> <ul> <li>Add a <code>Counter</code> metric that counts the number of requests the API has received.</li> <li>Add a <code>Histogram</code> metric that measures the time it takes to classify a review.</li> <li>Add a <code>Summary</code> metric that measures the size of the reviews that are classified.</li> </ul> <p>Confirm that everything works by running the application, sending a couple of requests to the API and then checking the <code>/metrics</code> endpoint updates as expected.</p> Solution <p>The important parts that implement the prometheus metrics are highlighted below:</p> <pre><code>import datetime\nimport json\nimport os\nfrom contextlib import asynccontextmanager\n\nimport torch\nimport torch.nn as nn\nfrom fastapi import BackgroundTasks, FastAPI, HTTPException\nfrom google.cloud import storage\nfrom prometheus_client import Counter, Histogram, Summary, make_asgi_app\nfrom pydantic import BaseModel\nfrom transformers import BertModel, BertTokenizer\n\n# Define model and device configuration\nBUCKET_NAME = \"gcp_monitoring_exercise\"\nMODEL_NAME = \"bert-base-cased\"\nMODEL_FILE_NAME = \"bert_sentiment_model.pt\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass ReviewInput(BaseModel):\n    \"\"\"Define input data structure for the endpoint.\"\"\"\n\n    review: str\n\n\nclass PredictionOutput(BaseModel):\n    \"\"\"Define output data structure for the endpoint.\"\"\"\n\n    sentiment: str\n\n\nclass SentimentClassifier(nn.Module):\n    \"\"\"Sentiment Classifier class. Combines BERT model with a dropout and linear layer.\"\"\"\n\n    def __init__(self, n_classes, model_name=MODEL_NAME):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"Forward pass of the model.\"\"\"\n        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.drop(output[1])\n        return self.out(output)\n\n\n# Define Prometheus metrics\nerror_counter = Counter(\"prediction_error\", \"Number of prediction errors\")\nrequest_counter = Counter(\"prediction_requests\", \"Number of prediction requests\")\nrequest_latency = Histogram(\"prediction_latency_seconds\", \"Prediction latency in seconds\")\nreview_summary = Summary(\"review_length_summary\", \"Review length summary\")\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Load the model and tokenizer when the app starts and clean up when the app stops.\"\"\"\n    global model, tokenizer, class_names\n    if \"bert_sentiment_model.pt\" not in os.listdir():\n        download_model_from_gcp()  # Download the model from GCP\n    model = SentimentClassifier(n_classes=3)\n    model.load_state_dict(torch.load(\"bert_sentiment_model.pt\", map_location=device))\n    model = model.to(device)\n    model.eval()\n    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n    class_names = [\"negative\", \"neutral\", \"positive\"]\n    print(\"Model and tokenizer loaded successfully\")\n\n    yield\n\n    del model, tokenizer\n\n\n# Initialize FastAPI app\napp = FastAPI(lifespan=lifespan)\napp.mount(\"/metrics\", make_asgi_app())\n\n\ndef download_model_from_gcp():\n    \"\"\"Download the model from GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    blob = bucket.blob(MODEL_FILE_NAME)\n    blob.download_to_filename(MODEL_FILE_NAME)\n    print(f\"Model {MODEL_FILE_NAME} downloaded from GCP bucket {BUCKET_NAME}.\")\n\n\n# Save prediction results to GCP\ndef save_prediction_to_gcp(review: str, outputs: list[float], sentiment: str):\n    \"\"\"Save the prediction results to GCP bucket.\"\"\"\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    time = datetime.datetime.now(tz=datetime.UTC)\n    # Prepare prediction data\n    data = {\n        \"review\": review,\n        \"sentiment\": sentiment,\n        \"probability\": outputs,\n        \"timestamp\": datetime.datetime.now(tz=datetime.UTC).isoformat(),\n    }\n    blob = bucket.blob(f\"prediction_{time}.json\")\n    blob.upload_from_string(json.dumps(data))\n    print(\"Prediction saved to GCP bucket.\")\n\n\n# Prediction endpoint\n@app.post(\"/predict\", response_model=PredictionOutput)\nasync def predict_sentiment(review_input: ReviewInput, background_tasks: BackgroundTasks):\n    \"\"\"Predict sentiment of the input text.\"\"\"\n    request_counter.inc()\n    with request_latency.time():\n        try:\n            review_summary.observe(len(review_input.review))\n            # Encode input text\n            encoding = tokenizer.encode_plus(\n                review_input.review,\n                add_special_tokens=True,\n                max_length=160,\n                return_token_type_ids=False,\n                padding=\"max_length\",\n                return_attention_mask=True,\n                return_tensors=\"pt\",\n            )\n\n            input_ids = encoding[\"input_ids\"].to(device)\n            attention_mask = encoding[\"attention_mask\"].to(device)\n\n            # Model prediction\n            with torch.no_grad():\n                outputs: torch.Tensor = model(input_ids, attention_mask)\n                _, prediction = torch.max(outputs, dim=1)\n                sentiment = class_names[prediction]\n\n            background_tasks.add_task(\n                save_prediction_to_gcp, review_input.review, outputs.softmax(-1).tolist(), sentiment\n            )\n\n            return PredictionOutput(sentiment=sentiment)\n\n        except Exception as e:\n            error_counter.inc()\n            raise HTTPException(status_code=500, detail=str(e)) from e\n</code></pre> </li> <li> <p>Write a small dockerfile that containerizes the application. Check that you can build the container and run it.</p> Dockerfile for sentiment API <p>We here assume that you have implemented the code in a file called <code>sentiment_api_prometheus_advance.py</code>.</p> <pre><code>\n</code></pre> </li> </ol> </li> </ol> <p>Hopefully from the exercise you have now seen how easy it is to add Prometheus metrics to an application. The next step is to look at how we can collect these metrics when the application is running in the cloud.</p>"},{"location":"s8_monitoring/monitoring/#cloud-monitoring","title":"Cloud monitoring","text":"<p>Any cloud system with respect for itself will have some kind of monitoring system. GCP has a service called Monitoring that is designed to monitor all the different services. By default it will monitor a lot of metrics out-of-the-box. However, the question is if we want to monitor more than the default metrics. The complexity that comes with doing monitoring in the cloud is that we need more than one container. We at least need one container actually running the application that is also exposing the <code>/metrics</code> endpoint and then we need a another container that is collecting the metrics from the first container and storing them in a database. To implement such a system of containers that need to talk to each other we in general need to use a container orchestration system such as Kubernetes. This is out of scope for this course, but we can use a feature of <code>Cloud Run</code> called <code>sidecar containers</code> to achieve the same effect. A sidecar container is a container that is running alongside the main container and can be used to do things such as collect metrics.</p> <p></p>"},{"location":"s8_monitoring/monitoring/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Overall we recommend that you just become familiar with the monitoring tab for your cloud run service (see image)     above. Try to invoke your service a couple of times and see what happens to the metrics over time.</p> </li> <li> <p>Try creating a service level objective (SLO). In short an SLO     is a target for how well your application should be performing. Click the <code>Create SLO</code> button and fill it out with     what you consider to be a good SLO for your application.</p> <p> </p> </li> <li> <p>To expose our own metrics we need to set up a sidecar container. To do this follow the instructions     here. We have     created a simple example that uses FastAPI and Prometheus that you can find     here. After you have correctly created the sidecar container     you should be able to see your custom metrics in the monitoring tab.</p> </li> </ol>"},{"location":"s8_monitoring/monitoring/#alert-systems","title":"Alert systems","text":"<p>A core problem within monitoring is alert systems. The alert system is in charge of sending out alerts to relevant people when some telemetry or metric we are tracking is not behaving as it should. Alert systems are a subjective choice of when and how many should be sent out and in general should be proportional to the importance of the metric/telemetry. We commonly run into what is referred to the goldilocks problem where we want just the right amount of alerts, though it is more often the case that we either have</p> <ul> <li>Too many alerts, such that they become irrelevant and the really important ones are overseen, often referred to as     alert fatigue</li> <li>Or alternatively, we have too few alerts and problems that should have triggered an alert are not dealt with when     they happen which can have unforeseen consequences</li> </ul> <p>Therefore, setting up proper alert systems can be as challenging as setting up the systems for the actual metrics we want to trigger alerts.</p>"},{"location":"s8_monitoring/monitoring/#exercises_2","title":"\u2754 Exercises","text":"<p>In this exercise we are going to look at how we can setup automatic alerting such that we get a message every time one of our applications is not behaving as expected.</p> <ol> <li> <p>Go to the <code>Monitoring</code> service. Then go to the <code>Alerting</code> tab.      </p> </li> <li> <p>Start by setting up a notification channel. We recommend setting up with an email.</p> </li> <li> <p>Next let's create a policy. Clicking the <code>Add Condition</code> should bring up a window as below. You are free to set up the    condition however you want but the image is one way to setup an alert that will react to the number of times a cloud    function is invoked (actually it measures the number of log entries from cloud functions).</p> <p> </p> </li> <li> <p>After adding the condition, add the notification channel you created in one of the earlier steps. Remember to also     add some documentation that should be sent with the alert to better describe what the alert is actually doing.</p> </li> <li> <p>When the alert is set up you need to trigger it. If you set up the condition as in the image above you just need to     invoke the cloud function many times. Here is a small code snippet that you can execute on your laptop to call a     cloud function many times (you need to change the URL and payload depending on your function):</p> <pre><code>import time\nimport requests\nurl = 'https://us-central1-dtumlops-335110.cloudfunctions.net/function-2'\npayload = {'message': 'Hello, General Kenobi'}\n\nfor _ in range(1000):\n    r = requests.get(url, params=payload)\n</code></pre> </li> <li> <p>Make sure that you get the alert through the notification channel you set up.</p> </li> </ol>"},{"location":"s9_scalable_applications/","title":"Scaling applications","text":"<p>Slides</p> <ul> <li> <p></p> <p>Learn how to set up distributed data loading in your PyTorch application</p> <p> M29: Distributed Data Loading</p> </li> <li> <p></p> <p>Learn how to do distributed training in PyTorch using <code>pytorch-lightning</code></p> <p> M30: Distributed Training</p> </li> <li> <p></p> <p>Learn how to do scalable inference in PyTorch</p> <p> M31: Scalable Inference</p> </li> </ul> <p>This module is all about scaling the applications that we are building. Here we are going to use a very narrow definition of scaling, namely that we want our applications to run faster. However, one should note that in general scaling is a much broader term. There are many different ways to scale your applications and we are going to look at three of these related to different tasks in machine learning algorithms:</p> <ul> <li>Scaling data loading</li> <li>Scaling training</li> <li>Scaling inference</li> </ul> <p>We are going to approach the term scaling from two different angles and both should result in your application running faster. The first approach is levering multiple devices, such as using multiple CPU cores or parallelizing training across multiple GPUs. The second approach is more analytical, where we are going to look at how we can design smaller/faster model architectures that run faster.</p> <p>It should be noted that this module is specific to working with PyTorch applications. In particular, we are going to see how we can both improve base PyTorch code and how to utilize PyTorch Lightning which we introduced in module M14 on boilerplate to improve the scaling of our applications. If your application is written using another framework we can guarantee that the same techniques in these modules transfer to that framework, but may require you to seek out how to specifically do it.</p> <p>If you manage to complete all the modules in this session, feel free to check out the extra module on scalable hyperparameter optimization.</p> <p>Learning objectives</p> <p>The learning objectives of this session are:</p> <ul> <li>Understand how data loading during training can be parallelized and have experimented with it</li> <li>Understand the different paradigms for distributed training and can run multi-GPU experiments using the     framework <code>pytorch-lightning</code></li> <li>Knowledge of different ways, including quantization, pruning, architecture tuning, etc. to improve inference     speed</li> </ul>"},{"location":"s9_scalable_applications/data_loading/","title":"M29 - Distributed Data Loading","text":""},{"location":"s9_scalable_applications/data_loading/#distributed-data-loading","title":"Distributed Data Loading","text":"<p>Core Module</p> <p>One way that deep learning fundamentally changed the way we think about data in machine learning is that more data is always better. This was very much not the case with more traditional machine learning algorithms (random forest, support vector machines, etc.) where a plateau in performance was often reached for a certain amount of data and did not improve if more was added. However, as deep learning models have become deeper and deeper, more and more data-hungry performance seems to be ever-increasing or at least not reaching a plateau in the same way as for traditional machine learning.</p> <p></p>  Image credit  <p>As we are trying to feed more and more data into our models, the obvious first question to ask is how to do this efficiently. As a general rule of thumb, we want the performance bottleneck to be the forward/backward, e.g. the actual computation in our neural network and not the data loading. By bottleneck, we here refer to the part of our pipeline that is restricting how fast we can process data. If data loading is our bottleneck, then our compute device can sit idle while waiting for data to arrive, which is both inefficient and costly. For example, if you are using a cloud provider for training deep learning models, you are paying by the hour per device, and thus not using them fully can be costly in the long run.</p> <p>In the first set of exercises, we are therefore going to focus on distributed data loading, i.e., how to load data in parallel to make sure that we always have data ready for our compute devices. We are in the following going to look at what is going on behind the scenes when we use PyTorch to parallelize data loading.</p>"},{"location":"s9_scalable_applications/data_loading/#a-closer-look-at-data-loading","title":"A closer look at Data loading","text":"<p>Before we talk distributed applications it is important to understand the physical layout of a standard CPU (the brain of your computer).</p> <p></p> <p>Most modern CPUs are a single chip that consists of multiple cores. Each core can further be divided into threads. In most laptops, the core count is 4 and there are commonly 2 threads per core. This means that the common laptop has 8 threads. The number of threads a compute unit has is important because that directly corresponds to the number of parallel operations that can be executed, i.e., one per thread. In a Python terminal you should be able to get the number of cores in your machine by writing (try it):</p> <pre><code>import multiprocessing\ncores = multiprocessing.cpu_count()\nprint(f\"Number of cores: {cores}, Number of threads: {2*cores}\")\n</code></pre> <p>A distributed application is in general any kind of application that parallelizes some or all of its workload. In these exercises we focus only on distributed data loading, which happens primarily only on the CPU. In <code>PyTorch</code> it is easy to parallelize data loading if you are using their dataset/data loader interface:</p> <pre><code>from torch.utils.data import Dataset, DataLoader\nclass MyDataset(Dataset):\n    def __init__(self, ...):\n        # whatever logic is needed to init the data set\n        self.data = ...\n\n    def __getitem__(self, idx):\n        # return one item\n        return self.data[idx]\n\ndataset = MyDataset()\ndataloader = Dataloader(\n    dataset,\n    batch_size=8,\n    num_workers=4  # this is the number of threads we want to parallelize workload over\n)\n</code></pre> <p>Let's take a deep dive into what happens when we request a batch from our dataloader e.g. <code>next(dataloader)</code>. First, we must understand that we have a thread that plays the role of the main and the remaining threads (in the above example we request 4) are called workers. When the dataloader is created, we create this structure and make sure that all threads have a copy of our dataset definition so each can call the <code>__getitem__</code> method.</p> <p></p> <p>Then comes the actual part where we request a batch of data. Assume that we have a batch size of 8 and we do not do any shuffling. In this step, the master thread then distributes the list of requested data points (<code>[0,1,2,3,4,5,6,7]</code>) to the four worker threads. With 8 indices and 4 workers, each worker will receive 2 indices.</p> <p></p> <p>Each worker thread then calls the <code>__getitem__</code> method for all the indices it has received. When all processes are done, the loaded images get sent back to the master thread and collected into a single structure/tensor.</p> <p></p> <p>Each arrow corresponds to a communication between two threads, which is not a free operation. In total to get a single batch (not counting the initial startup cost) in this example we need to do 8 communication operations. This may seem like a small price to pay, but that may not be the case. If the processing time of <code>__getitem__</code> is very low ( data is stored in memory, we just need to index to get it) then it does not make sense to use multiprocessing. The computational savings by doing the look-up operations in parallel are smaller than the communication cost there is between the main thread and the workers. Multiprocessing makes sense when the processing time of <code>__getitem__</code> is high (data is probably stored on the hard drive).</p> <p>It is this trade-off that we are going to investigate in the exercises.</p>"},{"location":"s9_scalable_applications/data_loading/#exercises","title":"\u2754 Exercises","text":"<p>This exercise is intended to be done on the labeled faces in the wild (LFW) dataset. The dataset consists of images of famous people extracted from the internet. The dataset had been used to drive the field of facial verification, which you can read more about here. We are going to imagine that this dataset cannot fit in memory, and your job is therefore to construct a data pipeline that can be parallelized based on loading the raw data files (.jpg) at runtime.</p> <ol> <li> <p>Download the dataset and extract it to a folder. It does not matter if you choose the non-aligned or aligned version     of the dataset.</p> </li> <li> <p>We provide the <code>lfw_dataset.py</code> file where we have started the process of defining a data class. Fill out     <code>__init__</code>, <code>__len__</code> and <code>__getitem__</code>. Note that <code>__getitem__</code> expects that you return a single <code>img</code> which should     be a <code>torch.Tensor</code>. Loading should be done using PIL Image, as <code>PIL</code>     images are the default input format for torchvision for     transforms (for data augmentation).</p> </li> <li> <p>Make sure that the script runs without any additional arguments.</p> <pre><code>python lfw_dataset.py\n</code></pre> </li> <li> <p>Visualize a single batch by filling out the codeblock after the first TODO right after defining the dataloader.     The visualization should show when launching the script as</p> <pre><code>python lfw_dataset.py -visualize_batch\n</code></pre> <p>Hint: this tutorial.</p> </li> <li> <p>Explore how the number of workers influences the performance. We have already provided code that will pass over 100     batches from the dataset 5 times and calculate how long it took, which you can play around with by calling</p> <pre><code>python lfw_dataset.py -get_timing -num_workers 1\n</code></pre> <p>Make an errorbar plot with the number of workers along the x-axis and the timing along the y-axis. The errorbars should correspond to the standard deviation over the 5 runs. HINT: if it is taking too long to evaluate, measure the time over fewer batches (set the <code>-batches_to_check</code> flag). Also if you are not seeing any improvement, try increasing the batch size (since data loading is parallelized per batch).</p> <p>For certain machines like the Mac with M1 chipset it is necessary to set the <code>multiprocessing_context</code> flag in the dataloder to <code>\"fork\"</code>. This essentially tells the dataloader how the worker nodes should be created.</p> </li> <li> <p>Retry the experiment where you change the data augmentation to be more complex:</p> <pre><code>lfw_trans = transforms.Compose([\n    transforms.RandomAffine(5, (0.1, 0.1), (0.5, 2.0)),\n    # add more transforms here\n    transforms.ToTensor()\n])\n</code></pre> <p>By making the augmentation more computationally demanding, it should be easier to get a boost in performance when using multiple workers because the data augmentation is also executed in parallel.</p> </li> <li> <p>(Optional, requires access to GPU) If your dataset fits in GPU memory it is beneficial to set the <code>pin_memory</code> flag     to <code>True</code>. By setting this flag we are essentially telling PyTorch that it can lock the data in place in memory     which will make the transfer between the host (CPU) and the device (GPU) faster.</p> </li> </ol> <p>This ends the module on distributed data loading in PyTorch. If you want to go into more details we highly recommend that you read this paper that goes into great detail on analyzing how data loading in PyTorch works and performance benchmarks.</p>"},{"location":"s9_scalable_applications/distributed_training/","title":"M30 - Distributed Training","text":""},{"location":"s9_scalable_applications/distributed_training/#distributed-training","title":"Distributed Training","text":"<p>In this module we are going to look at distributed training. Distributed training is one of the key ingredients to all the awesome results that deep learning models are producing. For example: Alphafold the highly praised model from DeepMind that seems to have solved protein structure prediction, was trained in a distributed fashion for a few weeks. The training was done on 16 TPUv3s (specialized hardware), which is approximately equal to 100-200 modern GPUs. This means that training Alphafold without distributed training on a single GPU (probably not even possible) would take a couple of years to train! Therefore, it is simply impossible currently to train some of the state-of-the-art (SOTA) models within deep learning without taking advantage of distributed training.</p> <p>When we talk about distributed training, there are a number of different paradigms that we may use to parallelize our computations:</p> <ul> <li>Data parallel (DP) training</li> <li>Distributed data parallel (DDP) training</li> <li>Sharded training</li> </ul> <p>In this module we are going to look at data parallel training, which is the original way of doing parallel training and distributed data parallel training which is an improved version of data parallel. If you want to know more about sharded training which is the newest of the paradigms you can read more about it in this blog post, which describes how sharded can save over 60% of memory used during your training.</p> <p>Finally, we want to note that for all the exercises in the module you are going to need a multi-GPU setup. If you have not already gained access to multi GPU machines on GCP (see the quotas exercises in this module) you will need to find another way to do the exercises. For DTU Students I recommend checking out this optional module on using the high performance cluster (HPC) where you can get access to multi-GPU resources.</p>"},{"location":"s9_scalable_applications/distributed_training/#data-parallel","title":"Data parallel","text":"<p>While data parallel today in general is seen as obsolete compared to distributed data parallel, we are still going to investigate it a bit since it offers the most simple form of distributed computations in a deep learning pipeline.</p> <p>The figure below shows both the forward and backward steps in the data parallel paradigm.</p> <p></p> <p>The steps are the following:</p> <ul> <li> <p>Whenever we try to do a forward call e.g. <code>out=model(batch)</code> we take the batch and divide it equally among all     devices. If we have a batch size of <code>N</code> and <code>M</code> devices each device will be sent <code>N/M</code> datapoints.</p> </li> <li> <p>Afterwards each device receives a copy of the <code>model</code>, e.g., a copy of the weights that currently parametrize our     neural network.</p> </li> <li> <p>In this step we perform the actual forward pass in parallel. This is the actual step that can help us scale     our training.</p> </li> <li> <p>Finally we need to send back the output of each replicated model to the primary device.</p> </li> </ul> <p>Similar to the analysis we did of parallel data loading, we cannot always expect that this will actually take less time than doing the forward call on a single GPU. If we are parallelizing over <code>M</code> devices, we essentially need to do <code>3xM</code> communication calls to send batch, model and output between the devices. If the parallel forward call does not outweigh this, then it will take longer.</p> <p>In addition, we also have the backward path to focus on.</p> <ul> <li> <p>As the end of the forward collected the output on the primary device, this is also where the loss is accumulated.     Thus, loss gradients are first calculated on the primary device.</p> </li> <li> <p>Next we scatter the gradient to all the workers.</p> </li> <li> <p>The workers then perform a parallel backward pass through their individual model.</p> </li> <li> <p>Finally, we reduce (sum) the gradients from all the workers on the main process such that we can do gradient descent.</p> </li> </ul> <p>One of the big downsides of using data parallel is that all the replicas are destroyed after each backward call. This means that we over and over again need to replicate our model and send it to the devices that are part of the computations.</p> <p>Even though it seems like a lot of logic goes into implementing data parallel into your code, in PyTorch we can very simply enable data parallel training by wrapping our model in the nn.DataParallel class.</p> <pre><code>from torch import nn\nmodel = MyModelClass()\nmodel = nn.DataParallel(model, device_ids=[0, 1])  # data parallel on gpu 0 and 1\npreds = model(input)  # same as usual\n</code></pre>"},{"location":"s9_scalable_applications/distributed_training/#exercises","title":"\u2754 Exercises","text":"<p>Please note that the exercise only makes sense if you have access to multiple GPUs.</p> <ol> <li> <p>Create a new script (call it <code>data_parallel.py</code>) where you take a copy of model <code>FashionCNN</code>     from the <code>fashion_mnist.py</code> script. Instantiate the model and wrap <code>torch.nn.DataParallel</code>     around it such that it can be executed in data parallel.</p> </li> <li> <p>Try to run inference in parallel on multiple devices (pass a batch multiple times and time it):</p> <pre><code>import time\nstart = time.time()\nfor _ in range(n_reps):\n    out = model(batch)\nend = time.time()\n</code></pre> <p>Does data parallel decrease the inference time? If no, can you explain why that may be? Try playing around with the batch size, and see if data parallel is more beneficial for larger batch sizes.</p> </li> </ol>"},{"location":"s9_scalable_applications/distributed_training/#distributed-data-parallel","title":"Distributed data parallel","text":"<p>It should be clear that there is a huge disadvantage to using the data parallel paradigm to scale your applications: the model needs to replicated on each pass (because it is destroyed in the end), which requires a large transfer of data. This is the main problem that distributed data parallel tries to solve.</p> <p></p> <p>The key difference between distributed data parallel and data parallel is that we move the model update (the gradient step) to happen on each device in parallel instead of only on the main device. This has the consequence that we do not need to replicate the model in each step, instead we just keep a local version on each device that we keep updating. The full set of steps (as shown in the figure):</p> <ul> <li> <p>Initialize an exact copy of the model on each device.</p> </li> <li> <p>From disk (or memory) we start by loading data into a section of page-locked host memory per device. Page-locked     memory is essentially a way to reserve a piece of a computer's memory for a specific transfer that is going to     happen over and over again to speed it up. The page-locked regions are loaded with non-overlapping data.</p> </li> <li> <p>Transfer data from page-locked memory to each device in parallel.</p> </li> <li> <p>Perform a forward pass in parallel.</p> </li> <li> <p>Do an all-reduce operation on the gradients. An all-reduce operation is a so called all-to-all operation meaning     that all processes send their own gradient to all other processes and also receive them from all other processes.</p> </li> <li> <p>Reduce the combined gradient signal from all processes and update the individual model in parallel. Since all     processes received the same gradient information, all models will still be in sync.</p> </li> </ul> <p>Thus, in distributed data parallel we here end up only doing a single communication call between all processes, compared to all the communication going on in data parallel. While all-reduce is a more expensive operation that many of the other communication operations that we can do, because we only have to do one we gain a huge performance boost. Empirically distributed data parallel tends to be 2-3 times faster than data parallel.</p> <p>However, this performance increase does not come for free. Where we could implement data parallel in a single line in PyTorch, distributed data parallel is much more involved.</p>"},{"location":"s9_scalable_applications/distributed_training/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>We have provided an example of how to do distributed data parallel training in PyTorch in the two     files <code>distributed_example.py</code> and <code>distributed_example.sh</code>. Your objective is to get an understanding of the necessary     components in the script to get this kind of distributed training to work. Try to answer the following questions     (HINT: try to Google around):</p> <ol> <li> <p>What is the function of the <code>DDP</code> wrapper?</p> </li> <li> <p>What is the function of the <code>DistributedSampler</code>?</p> </li> <li> <p>Why is it necessary to call <code>dist.barrier()</code> before passing a batch into the model?</p> </li> <li> <p>What do the different environment variables do in the <code>.sh</code> file?</p> </li> </ol> </li> <li> <p>Try to benchmark the runs using 1 and 2 GPUs.</p> </li> <li> <p>The first exercise has hopefully convinced you that it can be quite the trouble writing distributed training     applications yourself. Luckily for us, <code>PyTorch-lightning</code> can take care of this for us such that we do not have to     care about the specific details. To get your model training on multiple GPUs you need to change two arguments in the     trainer: the <code>accelerator</code> flag and the <code>gpus</code> flag. In addition to this, you can read through this     guide about any additional steps you may     need to do (for many of you, it should just work). Try running your model on multiple GPUs.</p> </li> <li> <p>Try benchmarking your training using 1 and 2 gpus, e.g., try running a couple of epochs and measure how long it     takes. How much of a speedup can you actually get? Why can you not get a speedup of 2?</p> </li> </ol>"},{"location":"s9_scalable_applications/inference/","title":"M31 - Scalable Inference","text":""},{"location":"s9_scalable_applications/inference/#scalable-inference","title":"Scalable Inference","text":"<p>Inference is the task of applying our trained model to some new and unseen data, often called prediction. Thus, scaling inference is different from scaling data loading and training, mainly due to inference normally only using a single data point (or a few). As we can neither parallelize the data loading nor parallelize using multiple GPUs (at least not in any efficient way), this is of no use to us when we are doing inference. Additionally, performing inference is often not something we do on machines that can perform large computations, as most inference today is actually either done on edge devices, e.g. mobile phones or in low-cost-low-compute cloud environments. Thus, we need to be smarter about how we scale inference than just throwing more computing power at it.</p> <p>In this module, we are going to look at various ways that you can either reduce the size of your model or make your model faster. Both are important for running inference fast regardless of the setup you are running your model on. We want to note that this is still very much an active area of research and therefore best practices for what to do in a specific situation can change.</p>"},{"location":"s9_scalable_applications/inference/#choosing-the-right-architecture","title":"Choosing the right architecture","text":"<p>Assume you are starting a completely new project and have to come up with a model architecture for doing this. What is your strategy? The common way to do this is to look at prior work on similar problems that you are facing and either directly choose the same architecture or create some slight variation hereof. This is a great way to get started, but the architecture that you end up choosing may be optimal in terms of performance but not inference speed.</p> <p>The fact is that not all base architectures are created equal, and a 10K parameter model with one architecture can have a significantly different inference speed than another 10K parameter model with another architecture. For example, consider the figure below which compares a number of models from the timm package, colored based on their base architecture. The general trend is that the number of images that can be processed by a model per sec (y-axis) is inversely proportional to the number of parameters (x-axis). However, we in general see that convolutional base architectures (conv) are more efficient than transformer (vit) for the same parameter budget.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises","title":"\u2754 Exercises","text":"<p>As discussed in this blogpost the largest increase in inference speed you will see (given some specific hardware) is choosing an efficient model architecture. In the exercises below we are going to investigate the inference speed of different architectures.</p> <ol> <li> <p>Start by checking out this     table     which contains a list of pretrained weights in <code>torchvision</code>. Try finding</p> <ul> <li>Efficient net</li> <li>Resnet</li> <li>Transformer based</li> </ul> <p>models that have in the range of 20-30 million parameters.</p> </li> <li> <p>Write a small script that first initializes all models, creates a dummy input tensor of shape [100, 3, 256, 256] and     then measures the time it takes to do a forward pass on the input tensor. Make sure to do this multiple times to get     a good average time.</p> Solution <p>In this solution, we have chosen to use the efficientnet b5 (30.4M parameters), resnet50 (25.6M parameters) and the swin v2 transformer tiny (28.4M parameters) models.</p> <pre><code>import time\nimport torch\nfrom torchvision import models\n\nmodel_list = [\"efficientnet_b5\", \"resnet50\", \"swin_v2_t\"]\nimage = torch.randn(100, 3, 256, 256)\n\nn_reps = 10\nfor i, m in enumerate(model_list):\n    model = models.get_model(m)\n    tic = time.time()\n    for _ in range(n_reps):\n        _ = model(image)\n    toc = time.time()\n    print(f\"Model {i} took: {(toc - tic) / n_reps}\")\n</code></pre> </li> <li> <p>Do the results make sense? Based on the above figure we would expect that efficientnet is faster than resnet,     which is faster than the transformer based model. Is this also what you are seeing?</p> </li> <li> <p>To figure out why one net is more efficient than another we can try to count the operations each network needs to     perform for inference. An operation here we can define as a     FLOP (floating point operation) which is any mathematical operation (such as     +, -, *, /) or assignment that involves floating-point numbers. Luckily for us someone has already created a python     package for calculating this in pytorch: ptflops</p> <ol> <li> <p>Install the package.</p> <pre><code>pip install ptflops\n</code></pre> </li> <li> <p>Try calling the <code>get_model_complexity_info</code> function from the <code>ptflops</code> package on the networks from the     previous exercise. What are the results?</p> Solution <pre><code>from ptflops import get_model_complexity_info\nimport time\nimport torch\nfrom torchvision import models\n\nmodel_list = [\"efficientnet_b5\", \"resnet50\", \"swin_v2_t\"]\nfor model in model_list:\n    macs, params = get_model_complexity_info(\n        models.get_model(model_list[0]), (3, 256, 256), backend='pytorch', print_per_layer_stat=False\n    )\n    print(f\"Model {model} have {params} parameters and uses {macs}\")\n</code></pre> </li> </ol> </li> <li> <p>In the table from the initial exercise, you could also see the overall performance of each network on the     Imagenet-1K dataset. Given this performance, the inference speed, the flops count, what network would you choose     to use in a production setting? Discuss when choosing one over another should be considered.</p> </li> </ol>"},{"location":"s9_scalable_applications/inference/#quantization","title":"Quantization","text":"<p>Quantization is a technique where all computations are performed with integers instead of floats. We are essentially taking all continuous signals and converting them into discretized signals.</p> <p></p>  Image credit  <p>As discussed in this blogpost series, while <code>float</code> (32-bit) is the primarily used precision in machine learning because it strikes a good balance between memory consumption, precision and computational requirements, it does not mean that during inference we can't take advantage of quantization to improve the speed of our model. For instance:</p> <ul> <li> <p>Floating-point computations are slower than integer operations.</p> </li> <li> <p>Recent hardware have specialized hardware for doing integer operations.</p> </li> <li> <p>Many neural networks are actually not bottlenecked by how many computations they need to do but by how fast we can     transfer data, e.g. the memory bandwidth and cache of your system is the limiting factor. Therefore working with 8-bit     integers vs. 32-bit floats means that we can approximately move data around 4 times as fast.</p> </li> <li> <p>Storing models in integers instead of floats saves us approximately 75% of the ram/harddisk space whenever we save     a checkpoint. This is especially useful in relation to deploying models using docker (as you hopefully remember) as     it will decrease the size of our docker images.</p> </li> </ul> <p>But how do we convert between floats and integers in quantization? In most cases we often use a linear affine quantization:</p> <p>$$ x_{int} = \\text{round}\\left( \\frac{x_{float}}{s} + z \\right) $$</p> <p>where $s$ is a scale and $z$ is the so-called zero point. But how does that relate to doing inference in a neural network? The figure below shows all the conversations that we need to make to our standard inference pipeline to actually do computations in quantized format.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises_1","title":"\u2754 Exercises","text":"<ol> <li> <p>Let's look at how quantized tensors look in PyTorch.</p> <ol> <li> <p>Start by creating a tensor that contains both random numbers.</p> </li> <li> <p>Next call the <code>torch.quantize_per_tensor</code> function on the tensor. What does the quantized tensor     look like? How do the values relate to the <code>scale</code> and <code>zero_point</code> arguments?</p> </li> <li> <p>Finally, try to call the <code>.dequantize()</code> method on the tensor. Do you get a tensor back that is     close to what you initially started out with?</p> </li> </ol> </li> <li> <p>As you hopefully saw in the first exercise we are going to perform a number of rounding errors when     doing quantization and naively we would expect that these would accumulate and lead to a much worse model.     However, in practice we observe that quantization still works, and we actually have a mathematically     sound reason for this. Can you figure out why quantization still works with all the small rounding     errors? HINT: it has to do with the central limit theorem</p> </li> <li> <p>Let's move on to quantization of our model. Follow this     tutorial from PyTorch on how to do quantization. The goal is     to construct a model <code>model_fc32</code> that works on normal floats and a quantized version <code>model_int8</code>. For simplicity     you can just use one of the models from the tutorial.</p> </li> <li> <p>Let's try to benchmark our quantized model and see if all the trouble that we went through actually paid off. Also     try to perform the benchmark on the non-quantized model and see if you get a difference. If you do not get an     improvement, explain why that may be.</p> </li> </ol>"},{"location":"s9_scalable_applications/inference/#pruning","title":"Pruning","text":"<p>Pruning is another way to reduce the model size and maybe improve the performance of our network. As the figure below illustrates, in pruning we are simply removing weights in our network that we do not consider important for the task at hand. By removing, we here mean that the weight gets set to 0. There are many ways to determine if a weight is important but the general rule is that the importance of a weight is proportional to the magnitude of a given weight. This intuitively makes sense, since weights in all linear operations (fully connected or convolutional) are always multiplied by the incoming value, thus a small weight means a small outgoing activation.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises_2","title":"\u2754 Exercises","text":"<ol> <li> <p>We provide a start script that implements the famous     LeNet in this     file.     Open and run it just to make sure that you know the network.</p> </li> <li> <p>PyTorch already has some pruning methods implemented in its package.     Import the <code>prune</code> module from <code>torch.nn.utils</code> in the script.</p> </li> <li> <p>Try to prune the weights of the first convolutional layer by calling.</p> <pre><code>prune.random_unstructured(module_1, name=\"weight\", amount=0.3)  # (1)!\n</code></pre> <ol> <li> You can read about the prune method     here.</li> </ol> <p>Try printing <code>named_parameters</code> and <code>named_buffers</code> before and after the module is pruned. Can you explain the difference and what the connection is to the <code>module_1.weight</code> attribute?</p> </li> <li> <p>Try pruning the bias of the same module this time using the <code>l1_unstructured</code> function from the pruning module. Again     check the <code>named_parameters</code> and <code>named_buffers</code> arguments to make sure you understand the difference between L1 pruning     and unstructured pruning.</p> </li> <li> <p>Instead of pruning only a single module in the model let's try pruning the whole model. To do this we just need to     iterate over all <code>named_modules</code> in the model like this:</p> <pre><code>for name, module in new_model.named_modules():\n    prune.l1_unstructured(module, name='weight', amount=0.2)\n</code></pre> <p>But what if we wanted to apply different pruning to different layers? Implement a pruning scheme where</p> <ul> <li>The weights of convolutional layers are L1 pruned with <code>amount=0.2</code></li> <li>The weights of linear layers are unstructured pruned with <code>amount=0.4</code></li> </ul> <p>Print <code>print(dict(new_model.named_buffers()).keys())</code> after the pruning to confirm that all weights have been correctly pruned.</p> </li> <li> <p>The pruning we have looked at until now has only been local in nature, i.e., we have applied the pruning     independently for each layer, not accounting globally for how much we should actually prune. As you may realize this     can quickly lead to a network that is pruned too much. Instead, the more common approach is to prune globally     where we remove the smallest <code>X</code> amount of connections:</p> <ol> <li> <p>Start by creating a tuple over all the weights with the following format:</p> <pre><code>parameters_to_prune = (\n    (model.conv1, 'weight'),\n    # fill in the rest of the modules yourself\n    (model.fc3, 'weight'),\n)\n</code></pre> <p>The tuple needs to have length 5. Challenge: Can you construct the tuple using <code>for</code> loops, such that the code works for arbitrary-size networks?</p> </li> <li> <p>Next prune using the <code>global_unstructured</code> function to globally prune the tuple of parameters.</p> <pre><code>prune.global_unstructured(\n    parameters_to_prune,\n    pruning_method=prune.L1Unstructured,\n    amount=0.2,\n)\n</code></pre> </li> <li> <p>Check that the amount that has been pruned is actually equal to the 20% specified in the pruning. We provide     the following function that for a given submodule (for example <code>model.conv1</code>) computes the number of pruned     weights.</p> <pre><code>def check_prune_level(module: nn.Module):\n    sparsity_level = 100 * float(torch.sum(module.weight == 0) / module.weight.numel())\n    print(f\"Sparsity level of module {sparsity_level}\")\n</code></pre> </li> </ol> </li> <li> <p>With a pruned network we really want to see if all our effort actually resulted in a network that is faster and/or     smaller in memory. Do the following to the globally pruned network from the previous exercises:</p> <ol> <li> <p>First we need to make the pruning of our network permanent. Right now it is only semi-permanent as we are still     keeping a copy of the original weights in memory. Make the change permanent by calling <code>prune.remove</code> on every     pruned module in the model. Hint: iterate over the <code>parameters_to_prune</code> tuple.</p> </li> <li> <p>Next try to measure the time of a single inference (repeated 100 times) for both the pruned and non-pruned networks.</p> <pre><code>import time\ntic = time.time()\nfor _ in range(100):\n    _ = network(torch.randn(100, 1, 28, 28))\ntoc = time.time()\nprint(toc - tic)\n</code></pre> <p>Is the pruned network actually faster? If not, can you explain why?</p> </li> <li> <p>Next let's measure the size of our network (called <code>pruned_network</code>) and a freshly initialized network (called     <code>network</code>):</p> <pre><code>torch.save(pruned_network.state_dict(), 'pruned_network.pt')\ntorch.save(network.state_dict(), 'network.pt')\n</code></pre> <p>Look up the size of each file. Is the pruned network actually smaller? If not, can you explain why?</p> </li> <li> <p>Repeat the last exercise, but this time start by converting all pruned weights to sparse format first by calling     the <code>.to_sparse()</code> method on each pruned weight. Is the saved model smaller now?</p> </li> </ol> </li> </ol> <p>This ends the exercises on pruning. As you probably realized in the last couple of exercises, pruning does not guarantee speedups out of the box. This is because linear operations in PyTorch do not handle sparse structures out of the box. To actually get speedups we would need to deep dive into the sparse tensor operations, which again does not even guarantee a speedup because the performance of these operations depends on the sparsity structure of the pruned weights. Investigating this is out of scope for these exercises, but we highly recommend checking it out if you are interested in sparse networks.</p>"},{"location":"s9_scalable_applications/inference/#knowledge-distillation","title":"Knowledge distillation","text":"<p>Knowledge distillation is somewhat similar to pruning in the sense that it tries to find a smaller model that can perform equally well as a large model, however it does so in a completely different way. Knowledge distillation is a model compression technique that builds on the work of Bucila et al. in which we try to distill/compress the knowledge of a large complex model (also called the teacher model) into a simpler model (also called the student model).</p> <p>The best known example of this is the DistilBERT model. The DistilBERT model is a smaller version of the large natural-language procession model Bert, which achieves 97% of the performance of Bert while only containing 40% of the weights and being 60% faster. You can see in the figure below how it is much smaller in size compared to other models developed at the same time.</p> <p></p>  Image credit  <p>Knowledge distillation works by assuming we have a big teacher that is already performing well that we want to compress. By running our training set through our large model we get a softmax distribution for each and every training sample. The goal of the student is to both match the original labels of the training data but also match the softmax distribution of the teacher model. The intuition behind doing this is that the teacher model needs to be more complex to learn the complex inter-class relationship from just (one-hot) labels. The student on the other hand gets directly fed with softmax distributions from the teacher that explicitly encodes this inter-class relasionship and thus does not need the same capacity to learn as the teacher.</p> <p></p>  Image credit"},{"location":"s9_scalable_applications/inference/#exercises_3","title":"\u2754 Exercises","text":"<p>Let's try implementing model distillation ourselves. We are going to see if we can accomplish this on the cifar10 dataset. Do note that the exercises below can take quite a long time to finish because they involve training multiple networks and therefore involve some waiting.</p> <ol> <li> <p>Start by install the <code>transformers</code> and <code>datasets</code> packages from Huggingface</p> <pre><code>pip install transformers\npip install datasets\n</code></pre> <p>from which we are going to download the cifar10 dataset and a teacher model.</p> </li> <li> <p>Next download the cifar10 dataset</p> <pre><code>from datasets import load_dataset\ndataset = load_dataset(\"cifar10\")\n</code></pre> </li> <li> <p>Next let's initialize our teacher model. For this we consider a large transformer-based model:</p> <pre><code>from transformers import AutoFeatureExtractor, AutoModelForImageClassification\nextractor = AutoFeatureExtractor.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\nmodel = AutoModelForImageClassification.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\n</code></pre> </li> <li> <p>To get the logits (un-normalized softmax scores) from our teacher model for a single datapoint from the training     dataset you would extract it like this:</p> <pre><code>sample_img = dataset['train'][0]['img']\npreprocessed_img = extractor(dataset['train'][0]['img'], return_tensors='pt')\noutput =  model(**preprocessed_img)\nprint(output.logits)\n# tensor([[ 3.3682, -0.3160, -0.2798, -0.5006, -0.5529, -0.5625, -0.6144, -0.4671, 0.2807, -0.3066]])\n</code></pre> <p>Repeat this process for the whole training dataset and store the result somewhere.</p> </li> <li> <p>Implement a simple convolutional model. You can create a custom one yourself or use a small one from <code>torchvision</code>.</p> </li> <li> <p>Train the model on cifar10 to convergence, so you have a base result on how the model is performing.</p> </li> <li> <p>Redo the training, but this time add knowledge distillation to your training objective. It should look like this:</p> <pre><code>for batch in dataset:\n    # ...\n    img, target, teacher_logits = batch\n    preds = model(img)\n    loss = torch.nn.functional.cross_entropy(preds, target)\n    loss_teacher = torch.nn.functional.cross_entropy(preds, teacher_logits)\n    loss = loss + loss_teacher\n    loss.backward()\n    # ...\n</code></pre> </li> <li> <p>Compare the final performance obtained with and without knowledge distillation. Did the performance improve or not?</p> </li> </ol> <p>This ends the module on scaling inference in machine learning models.</p>"}]}